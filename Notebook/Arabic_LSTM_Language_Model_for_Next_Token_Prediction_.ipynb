{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c10dba7d3284f0ca74a75f4ae661aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f3be0f759f4536b456eb8960801bfd",
              "IPY_MODEL_d3100bd534f94c64abe4b151bd0d0c97",
              "IPY_MODEL_09fb8b3f7b054e41b4aabec488ca93c3"
            ],
            "layout": "IPY_MODEL_19e298d4feb047849c7e68fbebda0709"
          }
        },
        "e0f3be0f759f4536b456eb8960801bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9508edc8a5c6432cae49ad32ce74514d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_69f25441dd714a4a8f93725bef5d287c",
            "value": "Downloading‚Äáreadme:‚Äá100%"
          }
        },
        "d3100bd534f94c64abe4b151bd0d0c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2ca7fc0c1740a8a72c122878e0d6f6",
            "max": 303334,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ab6653bb3244ea9a9ef725df1160f05",
            "value": 303334
          }
        },
        "09fb8b3f7b054e41b4aabec488ca93c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a4e9d0979d4cf387efad339b8d7e8e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_620fd27935574c4388640aebf60ff560",
            "value": "‚Äá303k/303k‚Äá[00:00&lt;00:00,‚Äá1.10MB/s]"
          }
        },
        "19e298d4feb047849c7e68fbebda0709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9508edc8a5c6432cae49ad32ce74514d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69f25441dd714a4a8f93725bef5d287c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff2ca7fc0c1740a8a72c122878e0d6f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab6653bb3244ea9a9ef725df1160f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81a4e9d0979d4cf387efad339b8d7e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620fd27935574c4388640aebf60ff560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4311f718b56c42db90db595642f5d3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_97fa739ec0eb4bd6b2e750694e62f7a7"
          }
        },
        "909ad79b29844469a1fcb7339d084228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adfda549e79942758cc893d48c34b5e5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_744b31d7cba049db93df5737a8e70a2d",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3e270f3b52b0413fa7564ed44d664d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_839cbc4086f5464a842d3dbb2586bce4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bb11143c69ef46088c321cfc510d8a83",
            "value": ""
          }
        },
        "8e942e320b9b461a87f706fdef7e2b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b5515f6dfe9b417cbdfc3946f66efb4e",
            "style": "IPY_MODEL_3a094b2bdc5d4421b9e3ad61b3e4b6fd",
            "value": true
          }
        },
        "83a6270fa5234e149e6a9aec925e39bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_568e93b27af145db8f02911c5ec6edb0",
            "style": "IPY_MODEL_1a10352c40904612a34adfde317831f6",
            "tooltip": ""
          }
        },
        "15b06350267c46c5b1de4d91e14ffe75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0ef66f2b0f9429ab85afa0f4f04448e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2d7a7a80320347c993fa7933efaad557",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "97fa739ec0eb4bd6b2e750694e62f7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "adfda549e79942758cc893d48c34b5e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "744b31d7cba049db93df5737a8e70a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "839cbc4086f5464a842d3dbb2586bce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb11143c69ef46088c321cfc510d8a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5515f6dfe9b417cbdfc3946f66efb4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a094b2bdc5d4421b9e3ad61b3e4b6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "568e93b27af145db8f02911c5ec6edb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a10352c40904612a34adfde317831f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f0ef66f2b0f9429ab85afa0f4f04448e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7a7a80320347c993fa7933efaad557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8926e39034af4aaa82612c7cfc817438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_420fd728c23e4b8ab48b0c17522161df",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bb210ceaba8749088ab3ef9a18cdca0c",
            "value": "Connecting..."
          }
        },
        "420fd728c23e4b8ab48b0c17522161df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb210ceaba8749088ab3ef9a18cdca0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "# Arabic LSTM Language Model\n",
        "\n",
        "This notebook demonstrates how to build an LSTM-based language model for Arabic text\n",
        "that can predict the next token in a sequence.\n",
        "\n",
        "## What the Steps are:\n",
        "1. **Text Preprocessing**: Clean and tokenize Arabic text\n",
        "2. **Vocabulary Building**: Create word-to-index mappings\n",
        "3. **LSTM Architecture**: Build neural network for language modeling\n",
        "4. **Training Process**: Train the model on Arabic data\n",
        "5. **Text Generation**: Generate new Arabic text\n",
        "6. **Next Token Prediction**: Predict most likely next words\n",
        "\n",
        "## Requirements:\n",
        "```bash\n",
        "pip install torch datasets matplotlib arabic-reshaper python-bidi\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JW_eFFGIDk-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Required Libraries\n"
      ],
      "metadata": {
        "id": "LGlhkpGwEB8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8L8ZwyTz9gLa"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Data handling\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"The device is using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6H_Ov4v-0si",
        "outputId": "21d9cb0a-8726-48bc-e180-798f7427f3c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The device is using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 2: Load Arabic Dataset from HuggingFace\n",
        "\n",
        "**First, I will load our data!**\n",
        "\n",
        "This section loads Arabic text data from HuggingFace datasets. We provide multiple options:\n",
        "\n",
        "### Arabic Datasets:\n",
        "  **- OSCAR** - Large multilingual corpus\n",
        "\n",
        "\n",
        "\n",
        "### Dataset Loading:\n"
      ],
      "metadata": {
        "id": "3jEapWN2Esn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U datasets\n",
        "!pip install fsspec==2023.9.2"
      ],
      "metadata": {
        "id": "d6RYzD-GADCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "a0a18f11-3754-4c85-9133-94e68fe64265"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec==2023.9.2\n",
            "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/173.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2023.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fsspec"
                ]
              },
              "id": "98f80e805aa94c419f42e543ee139d87"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_arabic_data(dataset_choice, config_name=None, num_samples=10):\n",
        "    \"\"\"\n",
        "    Load a small Arabic dataset sample from Hugging Face.\n",
        "\n",
        "    Args:\n",
        "        dataset_choice (str): Dataset name.\n",
        "        config_name (str or None): Configuration (e.g., for OSCAR).\n",
        "        num_samples (int): Number of samples to load.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Arabic text samples.\n",
        "    \"\"\"\n",
        "    print(f\"üì• Loading {num_samples} Arabic texts from '{dataset_choice}'...\")\n",
        "\n",
        "    try:\n",
        "        # Load full train split\n",
        "        dataset = load_dataset(dataset_choice, config_name, split=\"train\", streaming=True)\n",
        "\n",
        "        # Take only a few\n",
        "        texts = []\n",
        "        for sample in dataset:\n",
        "          if sample.get(\"text\", \"\").strip():\n",
        "              texts.append(sample[\"text\"].strip())\n",
        "          if len(texts) >= num_samples:\n",
        "              break\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(texts)} samples from '{dataset_choice}'\")\n",
        "        return texts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "fK-ZpFEX28kB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET LOADING CONFIGURATION\n",
        "dataset_choice = 'oscar'\n",
        "config_name = \"unshuffled_deduplicated_ar\"\n",
        "num_samples = 5000\n",
        "\n",
        "texts = load_arabic_data(dataset_choice, config_name, num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "1c10dba7d3284f0ca74a75f4ae661aaa",
            "e0f3be0f759f4536b456eb8960801bfd",
            "d3100bd534f94c64abe4b151bd0d0c97",
            "09fb8b3f7b054e41b4aabec488ca93c3",
            "19e298d4feb047849c7e68fbebda0709",
            "9508edc8a5c6432cae49ad32ce74514d",
            "69f25441dd714a4a8f93725bef5d287c",
            "ff2ca7fc0c1740a8a72c122878e0d6f6",
            "8ab6653bb3244ea9a9ef725df1160f05",
            "81a4e9d0979d4cf387efad339b8d7e8e",
            "620fd27935574c4388640aebf60ff560"
          ]
        },
        "id": "CUilfzOhJsJR",
        "outputId": "d19e6d66-062c-44d6-878e-89b857eccc8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Loading 5000 Arabic texts from 'oscar'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/303k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c10dba7d3284f0ca74a75f4ae661aaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 5000 samples from 'oscar'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Total texts loaded: {len(texts)}\")\n",
        "print(f\"   Average text length: {np.mean([len(text.split()) for text in texts]):.1f} words\")\n",
        "print(f\"   Shortest text: {min([len(text.split()) for text in texts])} words\")\n",
        "print(f\"   Longest text: {max([len(text.split()) for text in texts])} words\")\n",
        "\n",
        "print(f\"\\nüìù Sample text preview:\")\n",
        "print(f\"   Text 1: {texts[0][:100]}...\")\n",
        "if len(texts) > 1:\n",
        "    print(f\"   Text 2: {texts[1][:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_tfHokQJ4Fz",
        "outputId": "955b7e41-af5a-4b84-f5cc-a470a78228ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Dataset Statistics:\n",
            "   Total texts loaded: 5000\n",
            "   Average text length: 436.2 words\n",
            "   Shortest text: 7 words\n",
            "   Longest text: 41782 words\n",
            "\n",
            "üìù Sample text preview:\n",
            "   Text 1: ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉ ÿπÿ≤Ÿäÿ≤ ÿßŸÑÿ≤ÿßÿ¶ÿ± ŸÜÿ™ŸÖŸÜŸâ ŸÑŸÉ ÿ£ŸàŸÇÿßÿ™ÿßŸã ÿ≥ÿπŸäÿØÿ© ŸÖÿπŸÜÿß Ÿàÿ£ŸÜ ŸÜÿ≤ÿØÿßÿØ ÿ¥ÿ±ŸÅÿß ÿ®ÿÆÿØŸÖÿ™ŸÉ ŸàŸÑÿß ÿ™ŸÜÿ≥Ÿâ ÿßŸÑÿ™ÿ≥ÿ¨ŸäŸÑ ŸÖÿπŸÜÿß ŸÑÿ™ÿ≥ÿ™ŸÅŸäÿØ...\n",
            "   Text 2: ÿ¥ ÿ≥Ÿàÿ±ÿ© ÿßŸÑŸÅÿßÿ™ÿ≠ÿ© ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿ®ŸÇÿ±ÿ© ÿ≥Ÿàÿ±ÿ© ÿ¢ŸÑ ÿπŸÖÿ±ÿßŸÜ ÿ≥Ÿàÿ±ÿ© ÿßŸÑŸÜÿ≥ÿßÿ° ÿ≥Ÿàÿ±ÿ©ÿßŸÑŸÖÿßÿ¶ÿØÿ© ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿßŸíŸÜÿπÿßŸÖ ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿßŸíÿπÿ±ÿßŸÅ ÿ≥Ÿàÿ±ÿ© ÿßŸÑ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Arabic Text Tokenizer\n",
        "\n",
        "Now that we have our data loaded, let's build the tokenizer that handles:\n",
        "- **Cleaning**: Remove diacritics, normalize text\n",
        "- **Tokenization**: Split text into words\n",
        "- **Vocabulary**: Build word-to-index mappings\n",
        "- **Encoding/Decoding**: Convert between text and numbers"
      ],
      "metadata": {
        "id": "V8qmtCxfLHlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicTokenizer:\n",
        "  \"\"\"Custom tokenizer for Arabic text processing\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.word_to_idx = {}\n",
        "    self.idx_to_word = {}\n",
        "    self.vocab_built = False\n",
        "\n",
        "    self.PAD_TOKEN = '<PAD>'\n",
        "    self.UNK_TOKEN = '<UNK>'\n",
        "    self.START_TOKEN = '<SOS>'\n",
        "    self.END_TOKEN = '<EOS>'\n",
        "\n",
        "    print(f\"Tokenizer initialized with vocabulary size: {self.vocab_size}\")\n",
        "\n",
        "  def clean_arabic_text(self, text):\n",
        "    \"\"\"Clean and normalize Arabic text\"\"\"\n",
        "    # Remove diacritics (tashkeel)\n",
        "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove English characters and numbers\n",
        "    text = re.sub(r'[a-zA-Z0-9]', '', text)\n",
        "\n",
        "    # Keep only Arabic characters and punctuation\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s\\u060C\\u061B\\u061F\\u0640]', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "  def tokenize(self, texts):\n",
        "    \"\"\"Tokenize a list of Arabic texts\"\"\"\n",
        "    cleaned_text = self.clean_arabic_text(texts)\n",
        "    pattern = r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF]+'\n",
        "    tokens = re.findall(pattern , cleaned_text)\n",
        "    return [token for token in tokens if token.strip()]\n",
        "\n",
        "\n",
        "  def build_vocab(self, texts):\n",
        "    \"\"\"Build the vocabulary from a list of Arabic texts\"\"\"\n",
        "    print(\"Building vocabulary...\")\n",
        "\n",
        "    # Collect all tokens\n",
        "    tokens = []\n",
        "    for text in texts:\n",
        "      tokens.extend(self.tokenize(text))\n",
        "\n",
        "    # Count frequencies\n",
        "    token_counts = Counter(tokens)\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocab = [self.PAD_TOKEN, self.UNK_TOKEN, self.START_TOKEN, self.END_TOKEN]\n",
        "    most_common = token_counts.most_common(self.vocab_size - len(vocab))\n",
        "    vocab.extend([token for token, _ in most_common])\n",
        "\n",
        "    # Create mappings\n",
        "    self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    self.idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    self.vocab_built = True\n",
        "\n",
        "    print(f\"Vocabulary built with {len(self.word_to_idx)} tokens\")\n",
        "    print(f\"Most common words: {[token for token, _ in token_counts.most_common(10)]}\")\n",
        "\n",
        "  def encoder(self, text):\n",
        "    \"\"\"Convert text to indices\"\"\"\n",
        "    if not self.vocab_built:\n",
        "      raise ValueError(\"Vocabulary not built. Call build_vocab() first.\")\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.word_to_idx.get(token, self.word_to_idx[self.UNK_TOKEN]) for token in tokens]\n",
        "\n",
        "  def decoder(self, indices):\n",
        "    \"\"\"Convert indices to text\"\"\"\n",
        "    tokens = [self.idx_to_word.get(idx, self.UNK_TOKEN) for idx in indices]\n",
        "    return ' '.join(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "wVmZOykcLIBj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer\n",
        "print(\"=== Testing Arabic Tokenizer ===\")\n",
        "sample_text = \"Ÿáÿ∞ÿß ŸÜÿµ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ¨ŸÖŸäŸÑÿ©\"\n",
        "tokenizer = ArabicTokenizer(vocab_size=2)\n",
        "\n",
        "print(f\"Original: {sample_text}\")\n",
        "print(f\"Cleaned: {tokenizer.clean_arabic_text(sample_text)}\")\n",
        "print(f\"Tokens: {tokenizer.tokenize(sample_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae6EDDdsVH3d",
        "outputId": "8814de6d-0ec2-4db1-b380-116425145116"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Arabic Tokenizer ===\n",
            "Tokenizer initialized with vocabulary size: 2\n",
            "Original: Ÿáÿ∞ÿß ŸÜÿµ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ¨ŸÖŸäŸÑÿ©\n",
            "Cleaned: Ÿáÿ∞ÿß ŸÜÿµ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ¨ŸÖŸäŸÑÿ©\n",
            "Tokens: ['Ÿáÿ∞ÿß', 'ŸÜÿµ', 'ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä', 'ÿ®ÿßŸÑŸÑÿ∫ÿ©', 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©', 'ÿßŸÑÿ¨ŸÖŸäŸÑÿ©']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Dataset Class\n",
        "\n",
        "This class prepares data for LSTM training by:\n",
        "- Converting texts to sequences of indices\n",
        "- Creating input-target pairs for next token prediction\n",
        "- Handling padding for variable length sequences"
      ],
      "metadata": {
        "id": "xV-jdUDqVSY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicLSTMDataset(Dataset):\n",
        "  \"\"\" Dataset for LSTM lnaguage modeling \"\"\"\n",
        "\n",
        "  def __init__(self, texts, tokenizer, sequence_length = 50):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.sequence_length = sequence_length\n",
        "    self.sequences = [] # Corrected variable name\n",
        "\n",
        "    print(f\"Creating sequences of length {self.sequence_length}...\")\n",
        "\n",
        "    for text in texts:\n",
        "      encoded  = tokenizer.encoder(text)\n",
        "      # Only process sequences long enough to create at least one input/target pair\n",
        "      if len(encoded) > 1:\n",
        "        # Sliding window sequences\n",
        "        for i in range(len(encoded) - 1): # Corrected range\n",
        "          end_idx = min(i + sequence_length, len(encoded))\n",
        "          # Ensure there is at least one target token\n",
        "          if end_idx - i > 0:\n",
        "              input_seq = encoded[i:end_idx]\n",
        "              target_seq = encoded[i+1:end_idx+1]\n",
        "\n",
        "              # Pad if needed\n",
        "              pad_id = tokenizer.word_to_idx[tokenizer.PAD_TOKEN]\n",
        "              # Pad input sequence\n",
        "              while len(input_seq) < sequence_length:\n",
        "                  input_seq.append(pad_id)\n",
        "              # Pad target sequence to match input sequence length\n",
        "              while len(target_seq) < sequence_length:\n",
        "                  target_seq.append(pad_id)\n",
        "\n",
        "\n",
        "              self.sequences.append((input_seq, target_seq))\n",
        "\n",
        "    print(f\"Created {len(self.sequences)} training sequences\") # Corrected indentation\n",
        "\n",
        "  def __len__(self): # Added __len__ method\n",
        "      return len(self.sequences)\n",
        "\n",
        "  def __getitem__(self, idx): # Corrected indentation\n",
        "      input_seq, target_seq = self.sequences[idx]\n",
        "      return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)"
      ],
      "metadata": {
        "id": "7BO9XNE3YGDF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: LSTM Model Architecture\n",
        "\n",
        "The LSTM model consists of:\n",
        "1. **Embedding Layer**: Converts word indices to dense vectors\n",
        "2. **LSTM Layers**: Process sequences and maintain memory\n",
        "3. **Dropout**: Prevents overfitting\n",
        "4. **Output Layer**: Predicts next token probabilities"
      ],
      "metadata": {
        "id": "IH5L4MIAVqpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicLSTM(nn.Module):\n",
        "  \"\"\" LSTM model for language modeling \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "    super(ArabicLSTM, self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    #Model layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "    # Model info\n",
        "    total_params = sum(p.numel() for p in self.parameters())\n",
        "    print(f\"LSTM Model created:\")\n",
        "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
        "    print(f\"  - Embedding dimension: {embedding_dim}\")\n",
        "    print(f\"  - Hidden dimension: {hidden_dim}\")\n",
        "    print(f\"  - Number of layers: {num_layers}\")\n",
        "    print(f\"  - Total parameters: {total_params:,}\")\n",
        "\n",
        "\n",
        "  def init_weights(self):\n",
        "    \"\"\"Initialize weights\"\"\"\n",
        "    initrange = 0.1\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    # Corrected from output_layer to fc\n",
        "    self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.bias.data.zero_()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    \"\"\"Forward pass\"\"\"\n",
        "    embedded = self.embedding(input)\n",
        "    output, hidden = self.lstm(embedded, hidden)\n",
        "    output = self.dropout(output)\n",
        "    # Corrected from output_layer to fc\n",
        "    output = self.fc(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    \"\"\"Initialize hidden state\"\"\"\n",
        "    return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
        "            torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))"
      ],
      "metadata": {
        "id": "KEYIWTNkE1ou"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Prepare Training Data\n",
        "\n",
        "Build vocabulary and create datasets for training and validation using the loaded texts."
      ],
      "metadata": {
        "id": "9NT0gukfVvf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and build vocabulary from our loaded texts\n",
        "tokenizer = ArabicTokenizer(vocab_size=2000)\n",
        "tokenizer.build_vocab(texts)\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(texts))\n",
        "train_texts = texts[:train_size]\n",
        "val_texts = texts[train_size:]\n",
        "\n",
        "print(f\"Training texts: {len(train_texts)}\")\n",
        "print(f\"Validation texts: {len(val_texts)}\")\n",
        "\n",
        "# Create datasets\n",
        "sequence_length = 25\n",
        "train_dataset = ArabicLSTMDataset(train_texts, tokenizer, sequence_length)\n",
        "val_dataset = ArabicLSTMDataset(val_texts, tokenizer, sequence_length)\n",
        "\n",
        "# Show vocabulary statistics\n",
        "vocab_size = len(tokenizer.word_to_idx)\n",
        "print(f\"\\nVocabulary statistics:\")\n",
        "print(f\"Total vocabulary size: {vocab_size}\")\n",
        "print(f\"Training sequences: {len(train_dataset)}\")\n",
        "print(f\"Validation sequences: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPKnIl6QUMcc",
        "outputId": "77ca3171-8175-40a6-ff8c-65e60072bad6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer initialized with vocabulary size: 2000\n",
            "Building vocabulary...\n",
            "Vocabulary built with 2000 tokens\n",
            "Most common words: ['ŸÅŸä', 'ŸÖŸÜ', 'ÿπŸÑŸâ', 'Ÿà', 'ÿ£ŸÜ', 'ÿå', 'ÿ•ŸÑŸâ', 'ÿπŸÜ', 'ÿßŸÑÿ™Ÿä', 'ŸÖŸÜÿ™ÿØŸâ']\n",
            "Training texts: 4000\n",
            "Validation texts: 1000\n",
            "Creating sequences of length 25...\n",
            "Created 1568932 training sequences\n",
            "Creating sequences of length 25...\n",
            "Created 458781 training sequences\n",
            "\n",
            "Vocabulary statistics:\n",
            "Total vocabulary size: 2000\n",
            "Training sequences: 1568932\n",
            "Validation sequences: 458781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Create and Configure Model\n",
        "\n",
        "Initialize the LSTM model with our vocabulary."
      ],
      "metadata": {
        "id": "6nsx6jMQVy4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "model_config = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'embedding_dim': 256,\n",
        "    'hidden_dim': 512,\n",
        "    'num_layers': 5,\n",
        "    'dropout': 0.3\n",
        "}\n",
        "\n",
        "# Create model\n",
        "model = ArabicLSTM(**model_config).to(device)\n",
        "\n",
        "print(f\"Model moved to device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIi2wuM8UUZg",
        "outputId": "ff8d19ba-d489-49fc-a561-1d57ea506ea9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Model created:\n",
            "  - Vocabulary size: 2000\n",
            "  - Embedding dimension: 256\n",
            "  - Hidden dimension: 512\n",
            "  - Number of layers: 5\n",
            "  - Total parameters: 11,519,952\n",
            "Model moved to device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Training Setup\n",
        "\n",
        "Configure training parameters and data loaders."
      ],
      "metadata": {
        "id": "JzQFm5ucV0ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_idx[tokenizer.PAD_TOKEN])\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  - Epochs: {EPOCHS}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Training batches: {len(train_loader)}\")\n",
        "print(f\"  - Validation batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zeBl0BGUY8P",
        "outputId": "919c152f-5763-4b5c-9b19-f0fd5a20fc5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration:\n",
            "  - Epochs: 1\n",
            "  - Batch size: 16\n",
            "  - Learning rate: 0.001\n",
            "  - Training batches: 98059\n",
            "  - Validation batches: 28674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Training Loop\n",
        "\n",
        "Train the model and track performance."
      ],
      "metadata": {
        "id": "NoefQKBtV3NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_batches = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        hidden = model.init_hidden(inputs.size(0))\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_batches += 1\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(f'Epoch {epoch+1}/{EPOCHS}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            hidden = model.init_hidden(inputs.size(0))\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
        "            val_loss += loss.item()\n",
        "            val_batches += 1\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_train_loss = train_loss / train_batches\n",
        "    avg_val_loss = val_loss / val_batches\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} Summary:')\n",
        "    print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "    print(f'  Val Loss: {avg_val_loss:.4f}')\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVItMX_SUfmg",
        "outputId": "b5e9b5a9-0e1d-41ea-93e4-40d28146c5f4",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "==================================================\n",
            "Epoch 1/1, Batch 0/98059, Loss: 7.5982\n",
            "Epoch 1/1, Batch 20/98059, Loss: 3.7672\n",
            "Epoch 1/1, Batch 40/98059, Loss: 3.6582\n",
            "Epoch 1/1, Batch 60/98059, Loss: 4.0866\n",
            "Epoch 1/1, Batch 80/98059, Loss: 3.8294\n",
            "Epoch 1/1, Batch 100/98059, Loss: 3.8062\n",
            "Epoch 1/1, Batch 120/98059, Loss: 4.5751\n",
            "Epoch 1/1, Batch 140/98059, Loss: 3.7768\n",
            "Epoch 1/1, Batch 160/98059, Loss: 4.1460\n",
            "Epoch 1/1, Batch 180/98059, Loss: 4.1136\n",
            "Epoch 1/1, Batch 200/98059, Loss: 4.0580\n",
            "Epoch 1/1, Batch 220/98059, Loss: 3.8131\n",
            "Epoch 1/1, Batch 240/98059, Loss: 4.0850\n",
            "Epoch 1/1, Batch 260/98059, Loss: 3.9824\n",
            "Epoch 1/1, Batch 280/98059, Loss: 3.8871\n",
            "Epoch 1/1, Batch 300/98059, Loss: 4.1706\n",
            "Epoch 1/1, Batch 320/98059, Loss: 4.0486\n",
            "Epoch 1/1, Batch 340/98059, Loss: 4.0817\n",
            "Epoch 1/1, Batch 360/98059, Loss: 3.8054\n",
            "Epoch 1/1, Batch 380/98059, Loss: 4.2200\n",
            "Epoch 1/1, Batch 400/98059, Loss: 3.6170\n",
            "Epoch 1/1, Batch 420/98059, Loss: 3.6085\n",
            "Epoch 1/1, Batch 440/98059, Loss: 4.0074\n",
            "Epoch 1/1, Batch 460/98059, Loss: 3.9658\n",
            "Epoch 1/1, Batch 480/98059, Loss: 3.6969\n",
            "Epoch 1/1, Batch 500/98059, Loss: 4.3441\n",
            "Epoch 1/1, Batch 520/98059, Loss: 4.4922\n",
            "Epoch 1/1, Batch 540/98059, Loss: 3.3244\n",
            "Epoch 1/1, Batch 560/98059, Loss: 4.0053\n",
            "Epoch 1/1, Batch 580/98059, Loss: 3.7896\n",
            "Epoch 1/1, Batch 600/98059, Loss: 3.5737\n",
            "Epoch 1/1, Batch 620/98059, Loss: 4.2527\n",
            "Epoch 1/1, Batch 640/98059, Loss: 3.6875\n",
            "Epoch 1/1, Batch 660/98059, Loss: 4.0503\n",
            "Epoch 1/1, Batch 680/98059, Loss: 3.1656\n",
            "Epoch 1/1, Batch 700/98059, Loss: 3.9149\n",
            "Epoch 1/1, Batch 720/98059, Loss: 3.9125\n",
            "Epoch 1/1, Batch 740/98059, Loss: 2.9627\n",
            "Epoch 1/1, Batch 760/98059, Loss: 3.9750\n",
            "Epoch 1/1, Batch 780/98059, Loss: 4.0364\n",
            "Epoch 1/1, Batch 800/98059, Loss: 4.1367\n",
            "Epoch 1/1, Batch 820/98059, Loss: 4.2163\n",
            "Epoch 1/1, Batch 840/98059, Loss: 3.6822\n",
            "Epoch 1/1, Batch 860/98059, Loss: 3.8476\n",
            "Epoch 1/1, Batch 880/98059, Loss: 3.8787\n",
            "Epoch 1/1, Batch 900/98059, Loss: 4.0910\n",
            "Epoch 1/1, Batch 920/98059, Loss: 3.9434\n",
            "Epoch 1/1, Batch 940/98059, Loss: 3.6034\n",
            "Epoch 1/1, Batch 960/98059, Loss: 4.1424\n",
            "Epoch 1/1, Batch 980/98059, Loss: 3.9528\n",
            "Epoch 1/1, Batch 1000/98059, Loss: 3.8648\n",
            "Epoch 1/1, Batch 1020/98059, Loss: 3.7524\n",
            "Epoch 1/1, Batch 1040/98059, Loss: 3.8353\n",
            "Epoch 1/1, Batch 1060/98059, Loss: 3.4702\n",
            "Epoch 1/1, Batch 1080/98059, Loss: 4.0579\n",
            "Epoch 1/1, Batch 1100/98059, Loss: 3.5627\n",
            "Epoch 1/1, Batch 1120/98059, Loss: 4.0398\n",
            "Epoch 1/1, Batch 1140/98059, Loss: 3.7817\n",
            "Epoch 1/1, Batch 1160/98059, Loss: 3.4877\n",
            "Epoch 1/1, Batch 1180/98059, Loss: 3.8660\n",
            "Epoch 1/1, Batch 1200/98059, Loss: 4.0599\n",
            "Epoch 1/1, Batch 1220/98059, Loss: 3.9482\n",
            "Epoch 1/1, Batch 1240/98059, Loss: 3.6952\n",
            "Epoch 1/1, Batch 1260/98059, Loss: 4.2141\n",
            "Epoch 1/1, Batch 1280/98059, Loss: 3.7951\n",
            "Epoch 1/1, Batch 1300/98059, Loss: 3.4649\n",
            "Epoch 1/1, Batch 1320/98059, Loss: 3.7486\n",
            "Epoch 1/1, Batch 1340/98059, Loss: 4.2468\n",
            "Epoch 1/1, Batch 1360/98059, Loss: 3.9276\n",
            "Epoch 1/1, Batch 1380/98059, Loss: 3.8962\n",
            "Epoch 1/1, Batch 1400/98059, Loss: 3.9485\n",
            "Epoch 1/1, Batch 1420/98059, Loss: 3.6704\n",
            "Epoch 1/1, Batch 1440/98059, Loss: 3.8269\n",
            "Epoch 1/1, Batch 1460/98059, Loss: 3.7990\n",
            "Epoch 1/1, Batch 1480/98059, Loss: 4.0020\n",
            "Epoch 1/1, Batch 1500/98059, Loss: 3.3326\n",
            "Epoch 1/1, Batch 1520/98059, Loss: 3.5268\n",
            "Epoch 1/1, Batch 1540/98059, Loss: 3.5959\n",
            "Epoch 1/1, Batch 1560/98059, Loss: 3.9655\n",
            "Epoch 1/1, Batch 1580/98059, Loss: 3.6622\n",
            "Epoch 1/1, Batch 1600/98059, Loss: 3.6551\n",
            "Epoch 1/1, Batch 1620/98059, Loss: 3.7719\n",
            "Epoch 1/1, Batch 1640/98059, Loss: 3.7951\n",
            "Epoch 1/1, Batch 1660/98059, Loss: 3.5449\n",
            "Epoch 1/1, Batch 1680/98059, Loss: 3.6170\n",
            "Epoch 1/1, Batch 1700/98059, Loss: 3.4445\n",
            "Epoch 1/1, Batch 1720/98059, Loss: 3.6437\n",
            "Epoch 1/1, Batch 1740/98059, Loss: 4.0141\n",
            "Epoch 1/1, Batch 1760/98059, Loss: 3.7494\n",
            "Epoch 1/1, Batch 1780/98059, Loss: 3.6950\n",
            "Epoch 1/1, Batch 1800/98059, Loss: 3.9002\n",
            "Epoch 1/1, Batch 1820/98059, Loss: 3.7261\n",
            "Epoch 1/1, Batch 1840/98059, Loss: 3.8731\n",
            "Epoch 1/1, Batch 1860/98059, Loss: 3.6360\n",
            "Epoch 1/1, Batch 1880/98059, Loss: 3.5741\n",
            "Epoch 1/1, Batch 1900/98059, Loss: 3.4904\n",
            "Epoch 1/1, Batch 1920/98059, Loss: 3.6825\n",
            "Epoch 1/1, Batch 1940/98059, Loss: 3.0860\n",
            "Epoch 1/1, Batch 1960/98059, Loss: 3.5389\n",
            "Epoch 1/1, Batch 1980/98059, Loss: 3.8516\n",
            "Epoch 1/1, Batch 2000/98059, Loss: 3.6768\n",
            "Epoch 1/1, Batch 2020/98059, Loss: 3.6113\n",
            "Epoch 1/1, Batch 2040/98059, Loss: 3.9634\n",
            "Epoch 1/1, Batch 2060/98059, Loss: 3.8764\n",
            "Epoch 1/1, Batch 2080/98059, Loss: 3.3320\n",
            "Epoch 1/1, Batch 2100/98059, Loss: 3.5198\n",
            "Epoch 1/1, Batch 2120/98059, Loss: 3.3900\n",
            "Epoch 1/1, Batch 2140/98059, Loss: 3.9025\n",
            "Epoch 1/1, Batch 2160/98059, Loss: 3.8802\n",
            "Epoch 1/1, Batch 2180/98059, Loss: 3.8117\n",
            "Epoch 1/1, Batch 2200/98059, Loss: 3.7195\n",
            "Epoch 1/1, Batch 2220/98059, Loss: 3.5706\n",
            "Epoch 1/1, Batch 2240/98059, Loss: 3.7354\n",
            "Epoch 1/1, Batch 2260/98059, Loss: 3.6630\n",
            "Epoch 1/1, Batch 2280/98059, Loss: 3.7859\n",
            "Epoch 1/1, Batch 2300/98059, Loss: 3.3238\n",
            "Epoch 1/1, Batch 2320/98059, Loss: 3.6347\n",
            "Epoch 1/1, Batch 2340/98059, Loss: 3.8930\n",
            "Epoch 1/1, Batch 2360/98059, Loss: 3.6181\n",
            "Epoch 1/1, Batch 2380/98059, Loss: 3.4563\n",
            "Epoch 1/1, Batch 2400/98059, Loss: 3.8205\n",
            "Epoch 1/1, Batch 2420/98059, Loss: 3.5238\n",
            "Epoch 1/1, Batch 2440/98059, Loss: 3.6597\n",
            "Epoch 1/1, Batch 2460/98059, Loss: 3.4054\n",
            "Epoch 1/1, Batch 2480/98059, Loss: 4.0132\n",
            "Epoch 1/1, Batch 2500/98059, Loss: 3.6354\n",
            "Epoch 1/1, Batch 2520/98059, Loss: 4.0138\n",
            "Epoch 1/1, Batch 2540/98059, Loss: 3.5679\n",
            "Epoch 1/1, Batch 2560/98059, Loss: 3.2538\n",
            "Epoch 1/1, Batch 2580/98059, Loss: 3.4366\n",
            "Epoch 1/1, Batch 2600/98059, Loss: 3.3086\n",
            "Epoch 1/1, Batch 2620/98059, Loss: 3.3108\n",
            "Epoch 1/1, Batch 2640/98059, Loss: 3.6500\n",
            "Epoch 1/1, Batch 2660/98059, Loss: 3.9898\n",
            "Epoch 1/1, Batch 2680/98059, Loss: 3.5907\n",
            "Epoch 1/1, Batch 2700/98059, Loss: 3.5535\n",
            "Epoch 1/1, Batch 2720/98059, Loss: 3.2837\n",
            "Epoch 1/1, Batch 2740/98059, Loss: 3.2918\n",
            "Epoch 1/1, Batch 2760/98059, Loss: 3.8142\n",
            "Epoch 1/1, Batch 2780/98059, Loss: 3.6007\n",
            "Epoch 1/1, Batch 2800/98059, Loss: 3.7500\n",
            "Epoch 1/1, Batch 2820/98059, Loss: 3.5786\n",
            "Epoch 1/1, Batch 2840/98059, Loss: 3.5745\n",
            "Epoch 1/1, Batch 2860/98059, Loss: 3.8440\n",
            "Epoch 1/1, Batch 2880/98059, Loss: 3.6598\n",
            "Epoch 1/1, Batch 2900/98059, Loss: 3.4333\n",
            "Epoch 1/1, Batch 2920/98059, Loss: 4.1566\n",
            "Epoch 1/1, Batch 2940/98059, Loss: 3.2331\n",
            "Epoch 1/1, Batch 2960/98059, Loss: 3.9175\n",
            "Epoch 1/1, Batch 2980/98059, Loss: 3.3362\n",
            "Epoch 1/1, Batch 3000/98059, Loss: 3.8737\n",
            "Epoch 1/1, Batch 3020/98059, Loss: 3.7889\n",
            "Epoch 1/1, Batch 3040/98059, Loss: 3.3308\n",
            "Epoch 1/1, Batch 3060/98059, Loss: 3.7350\n",
            "Epoch 1/1, Batch 3080/98059, Loss: 3.9786\n",
            "Epoch 1/1, Batch 3100/98059, Loss: 4.2143\n",
            "Epoch 1/1, Batch 3120/98059, Loss: 3.6675\n",
            "Epoch 1/1, Batch 3140/98059, Loss: 3.5139\n",
            "Epoch 1/1, Batch 3160/98059, Loss: 3.5618\n",
            "Epoch 1/1, Batch 3180/98059, Loss: 3.7105\n",
            "Epoch 1/1, Batch 3200/98059, Loss: 3.5638\n",
            "Epoch 1/1, Batch 3220/98059, Loss: 3.7827\n",
            "Epoch 1/1, Batch 3240/98059, Loss: 3.8760\n",
            "Epoch 1/1, Batch 3260/98059, Loss: 3.1276\n",
            "Epoch 1/1, Batch 3280/98059, Loss: 3.3464\n",
            "Epoch 1/1, Batch 3300/98059, Loss: 4.0260\n",
            "Epoch 1/1, Batch 3320/98059, Loss: 3.5529\n",
            "Epoch 1/1, Batch 3340/98059, Loss: 3.7362\n",
            "Epoch 1/1, Batch 3360/98059, Loss: 3.8503\n",
            "Epoch 1/1, Batch 3380/98059, Loss: 3.6514\n",
            "Epoch 1/1, Batch 3400/98059, Loss: 3.5941\n",
            "Epoch 1/1, Batch 3420/98059, Loss: 3.1772\n",
            "Epoch 1/1, Batch 3440/98059, Loss: 3.4313\n",
            "Epoch 1/1, Batch 3460/98059, Loss: 3.4946\n",
            "Epoch 1/1, Batch 3480/98059, Loss: 3.1848\n",
            "Epoch 1/1, Batch 3500/98059, Loss: 3.6312\n",
            "Epoch 1/1, Batch 3520/98059, Loss: 3.5174\n",
            "Epoch 1/1, Batch 3540/98059, Loss: 3.3972\n",
            "Epoch 1/1, Batch 3560/98059, Loss: 3.4389\n",
            "Epoch 1/1, Batch 3580/98059, Loss: 3.5027\n",
            "Epoch 1/1, Batch 3600/98059, Loss: 3.3200\n",
            "Epoch 1/1, Batch 3620/98059, Loss: 3.8258\n",
            "Epoch 1/1, Batch 3640/98059, Loss: 3.5308\n",
            "Epoch 1/1, Batch 3660/98059, Loss: 4.2894\n",
            "Epoch 1/1, Batch 3680/98059, Loss: 3.6024\n",
            "Epoch 1/1, Batch 3700/98059, Loss: 3.5299\n",
            "Epoch 1/1, Batch 3720/98059, Loss: 3.5908\n",
            "Epoch 1/1, Batch 3740/98059, Loss: 3.3093\n",
            "Epoch 1/1, Batch 3760/98059, Loss: 3.4767\n",
            "Epoch 1/1, Batch 3780/98059, Loss: 3.5508\n",
            "Epoch 1/1, Batch 3800/98059, Loss: 3.5855\n",
            "Epoch 1/1, Batch 3820/98059, Loss: 3.7774\n",
            "Epoch 1/1, Batch 3840/98059, Loss: 3.5735\n",
            "Epoch 1/1, Batch 3860/98059, Loss: 3.3237\n",
            "Epoch 1/1, Batch 3880/98059, Loss: 4.2112\n",
            "Epoch 1/1, Batch 3900/98059, Loss: 3.6739\n",
            "Epoch 1/1, Batch 3920/98059, Loss: 3.5736\n",
            "Epoch 1/1, Batch 3940/98059, Loss: 3.8808\n",
            "Epoch 1/1, Batch 3960/98059, Loss: 3.7829\n",
            "Epoch 1/1, Batch 3980/98059, Loss: 3.8127\n",
            "Epoch 1/1, Batch 4000/98059, Loss: 3.8261\n",
            "Epoch 1/1, Batch 4020/98059, Loss: 3.5875\n",
            "Epoch 1/1, Batch 4040/98059, Loss: 2.9953\n",
            "Epoch 1/1, Batch 4060/98059, Loss: 3.8276\n",
            "Epoch 1/1, Batch 4080/98059, Loss: 3.4771\n",
            "Epoch 1/1, Batch 4100/98059, Loss: 3.9086\n",
            "Epoch 1/1, Batch 4120/98059, Loss: 3.4651\n",
            "Epoch 1/1, Batch 4140/98059, Loss: 3.5184\n",
            "Epoch 1/1, Batch 4160/98059, Loss: 3.8509\n",
            "Epoch 1/1, Batch 4180/98059, Loss: 3.7677\n",
            "Epoch 1/1, Batch 4200/98059, Loss: 3.4395\n",
            "Epoch 1/1, Batch 4220/98059, Loss: 3.3933\n",
            "Epoch 1/1, Batch 4240/98059, Loss: 3.4508\n",
            "Epoch 1/1, Batch 4260/98059, Loss: 3.3748\n",
            "Epoch 1/1, Batch 4280/98059, Loss: 3.7316\n",
            "Epoch 1/1, Batch 4300/98059, Loss: 3.4564\n",
            "Epoch 1/1, Batch 4320/98059, Loss: 3.4114\n",
            "Epoch 1/1, Batch 4340/98059, Loss: 3.6010\n",
            "Epoch 1/1, Batch 4360/98059, Loss: 3.2034\n",
            "Epoch 1/1, Batch 4380/98059, Loss: 3.5738\n",
            "Epoch 1/1, Batch 4400/98059, Loss: 3.4875\n",
            "Epoch 1/1, Batch 4420/98059, Loss: 3.3436\n",
            "Epoch 1/1, Batch 4440/98059, Loss: 3.3586\n",
            "Epoch 1/1, Batch 4460/98059, Loss: 3.3317\n",
            "Epoch 1/1, Batch 4480/98059, Loss: 3.5888\n",
            "Epoch 1/1, Batch 4500/98059, Loss: 3.1894\n",
            "Epoch 1/1, Batch 4520/98059, Loss: 3.5749\n",
            "Epoch 1/1, Batch 4540/98059, Loss: 3.4472\n",
            "Epoch 1/1, Batch 4560/98059, Loss: 3.4110\n",
            "Epoch 1/1, Batch 4580/98059, Loss: 3.5919\n",
            "Epoch 1/1, Batch 4600/98059, Loss: 3.0670\n",
            "Epoch 1/1, Batch 4620/98059, Loss: 3.5480\n",
            "Epoch 1/1, Batch 4640/98059, Loss: 3.7790\n",
            "Epoch 1/1, Batch 4660/98059, Loss: 3.5183\n",
            "Epoch 1/1, Batch 4680/98059, Loss: 4.0777\n",
            "Epoch 1/1, Batch 4700/98059, Loss: 3.1142\n",
            "Epoch 1/1, Batch 4720/98059, Loss: 3.3048\n",
            "Epoch 1/1, Batch 4740/98059, Loss: 3.4877\n",
            "Epoch 1/1, Batch 4760/98059, Loss: 3.4114\n",
            "Epoch 1/1, Batch 4780/98059, Loss: 3.5343\n",
            "Epoch 1/1, Batch 4800/98059, Loss: 3.7229\n",
            "Epoch 1/1, Batch 4820/98059, Loss: 3.6204\n",
            "Epoch 1/1, Batch 4840/98059, Loss: 3.5787\n",
            "Epoch 1/1, Batch 4860/98059, Loss: 3.4261\n",
            "Epoch 1/1, Batch 4880/98059, Loss: 3.6190\n",
            "Epoch 1/1, Batch 4900/98059, Loss: 3.3183\n",
            "Epoch 1/1, Batch 4920/98059, Loss: 3.5756\n",
            "Epoch 1/1, Batch 4940/98059, Loss: 3.7124\n",
            "Epoch 1/1, Batch 4960/98059, Loss: 3.5453\n",
            "Epoch 1/1, Batch 4980/98059, Loss: 3.5115\n",
            "Epoch 1/1, Batch 5000/98059, Loss: 3.7157\n",
            "Epoch 1/1, Batch 5020/98059, Loss: 3.7307\n",
            "Epoch 1/1, Batch 5040/98059, Loss: 3.4627\n",
            "Epoch 1/1, Batch 5060/98059, Loss: 3.3974\n",
            "Epoch 1/1, Batch 5080/98059, Loss: 3.3666\n",
            "Epoch 1/1, Batch 5100/98059, Loss: 3.6787\n",
            "Epoch 1/1, Batch 5120/98059, Loss: 3.1593\n",
            "Epoch 1/1, Batch 5140/98059, Loss: 3.5204\n",
            "Epoch 1/1, Batch 5160/98059, Loss: 3.5848\n",
            "Epoch 1/1, Batch 5180/98059, Loss: 3.6689\n",
            "Epoch 1/1, Batch 5200/98059, Loss: 3.6792\n",
            "Epoch 1/1, Batch 5220/98059, Loss: 3.5544\n",
            "Epoch 1/1, Batch 5240/98059, Loss: 3.5722\n",
            "Epoch 1/1, Batch 5260/98059, Loss: 3.6210\n",
            "Epoch 1/1, Batch 5280/98059, Loss: 3.6030\n",
            "Epoch 1/1, Batch 5300/98059, Loss: 3.4990\n",
            "Epoch 1/1, Batch 5320/98059, Loss: 3.5108\n",
            "Epoch 1/1, Batch 5340/98059, Loss: 3.6599\n",
            "Epoch 1/1, Batch 5360/98059, Loss: 3.3576\n",
            "Epoch 1/1, Batch 5380/98059, Loss: 3.4030\n",
            "Epoch 1/1, Batch 5400/98059, Loss: 3.2681\n",
            "Epoch 1/1, Batch 5420/98059, Loss: 3.4984\n",
            "Epoch 1/1, Batch 5440/98059, Loss: 3.5129\n",
            "Epoch 1/1, Batch 5460/98059, Loss: 3.7342\n",
            "Epoch 1/1, Batch 5480/98059, Loss: 3.4499\n",
            "Epoch 1/1, Batch 5500/98059, Loss: 3.4962\n",
            "Epoch 1/1, Batch 5520/98059, Loss: 3.4671\n",
            "Epoch 1/1, Batch 5540/98059, Loss: 3.6325\n",
            "Epoch 1/1, Batch 5560/98059, Loss: 3.4313\n",
            "Epoch 1/1, Batch 5580/98059, Loss: 3.6775\n",
            "Epoch 1/1, Batch 5600/98059, Loss: 3.3910\n",
            "Epoch 1/1, Batch 5620/98059, Loss: 3.3451\n",
            "Epoch 1/1, Batch 5640/98059, Loss: 3.3321\n",
            "Epoch 1/1, Batch 5660/98059, Loss: 3.6444\n",
            "Epoch 1/1, Batch 5680/98059, Loss: 3.5727\n",
            "Epoch 1/1, Batch 5700/98059, Loss: 3.6671\n",
            "Epoch 1/1, Batch 5720/98059, Loss: 3.6766\n",
            "Epoch 1/1, Batch 5740/98059, Loss: 3.4171\n",
            "Epoch 1/1, Batch 5760/98059, Loss: 3.9024\n",
            "Epoch 1/1, Batch 5780/98059, Loss: 3.2499\n",
            "Epoch 1/1, Batch 5800/98059, Loss: 3.6857\n",
            "Epoch 1/1, Batch 5820/98059, Loss: 3.4806\n",
            "Epoch 1/1, Batch 5840/98059, Loss: 3.7530\n",
            "Epoch 1/1, Batch 5860/98059, Loss: 3.3531\n",
            "Epoch 1/1, Batch 5880/98059, Loss: 2.9264\n",
            "Epoch 1/1, Batch 5900/98059, Loss: 4.0932\n",
            "Epoch 1/1, Batch 5920/98059, Loss: 3.6788\n",
            "Epoch 1/1, Batch 5940/98059, Loss: 3.5864\n",
            "Epoch 1/1, Batch 5960/98059, Loss: 3.5742\n",
            "Epoch 1/1, Batch 5980/98059, Loss: 3.4562\n",
            "Epoch 1/1, Batch 6000/98059, Loss: 3.2921\n",
            "Epoch 1/1, Batch 6020/98059, Loss: 3.3183\n",
            "Epoch 1/1, Batch 6040/98059, Loss: 3.3567\n",
            "Epoch 1/1, Batch 6060/98059, Loss: 3.1438\n",
            "Epoch 1/1, Batch 6080/98059, Loss: 3.4647\n",
            "Epoch 1/1, Batch 6100/98059, Loss: 3.4561\n",
            "Epoch 1/1, Batch 6120/98059, Loss: 3.4836\n",
            "Epoch 1/1, Batch 6140/98059, Loss: 3.5327\n",
            "Epoch 1/1, Batch 6160/98059, Loss: 3.5046\n",
            "Epoch 1/1, Batch 6180/98059, Loss: 3.4236\n",
            "Epoch 1/1, Batch 6200/98059, Loss: 3.0427\n",
            "Epoch 1/1, Batch 6220/98059, Loss: 3.4232\n",
            "Epoch 1/1, Batch 6240/98059, Loss: 3.1486\n",
            "Epoch 1/1, Batch 6260/98059, Loss: 3.3832\n",
            "Epoch 1/1, Batch 6280/98059, Loss: 3.6026\n",
            "Epoch 1/1, Batch 6300/98059, Loss: 3.7950\n",
            "Epoch 1/1, Batch 6320/98059, Loss: 3.7090\n",
            "Epoch 1/1, Batch 6340/98059, Loss: 3.7600\n",
            "Epoch 1/1, Batch 6360/98059, Loss: 3.3092\n",
            "Epoch 1/1, Batch 6380/98059, Loss: 3.6314\n",
            "Epoch 1/1, Batch 6400/98059, Loss: 3.2139\n",
            "Epoch 1/1, Batch 6420/98059, Loss: 3.1475\n",
            "Epoch 1/1, Batch 6440/98059, Loss: 3.9475\n",
            "Epoch 1/1, Batch 6460/98059, Loss: 4.0017\n",
            "Epoch 1/1, Batch 6480/98059, Loss: 3.8834\n",
            "Epoch 1/1, Batch 6500/98059, Loss: 3.5046\n",
            "Epoch 1/1, Batch 6520/98059, Loss: 3.0193\n",
            "Epoch 1/1, Batch 6540/98059, Loss: 3.7435\n",
            "Epoch 1/1, Batch 6560/98059, Loss: 3.8718\n",
            "Epoch 1/1, Batch 6580/98059, Loss: 3.3847\n",
            "Epoch 1/1, Batch 6600/98059, Loss: 3.1303\n",
            "Epoch 1/1, Batch 6620/98059, Loss: 3.4827\n",
            "Epoch 1/1, Batch 6640/98059, Loss: 3.6659\n",
            "Epoch 1/1, Batch 6660/98059, Loss: 3.3320\n",
            "Epoch 1/1, Batch 6680/98059, Loss: 3.7369\n",
            "Epoch 1/1, Batch 6700/98059, Loss: 3.6903\n",
            "Epoch 1/1, Batch 6720/98059, Loss: 3.3653\n",
            "Epoch 1/1, Batch 6740/98059, Loss: 2.9759\n",
            "Epoch 1/1, Batch 6760/98059, Loss: 3.3023\n",
            "Epoch 1/1, Batch 6780/98059, Loss: 3.3108\n",
            "Epoch 1/1, Batch 6800/98059, Loss: 3.2678\n",
            "Epoch 1/1, Batch 6820/98059, Loss: 3.2572\n",
            "Epoch 1/1, Batch 6840/98059, Loss: 3.6482\n",
            "Epoch 1/1, Batch 6860/98059, Loss: 3.6924\n",
            "Epoch 1/1, Batch 6880/98059, Loss: 3.5322\n",
            "Epoch 1/1, Batch 6900/98059, Loss: 3.1337\n",
            "Epoch 1/1, Batch 6920/98059, Loss: 3.3737\n",
            "Epoch 1/1, Batch 6940/98059, Loss: 3.3127\n",
            "Epoch 1/1, Batch 6960/98059, Loss: 3.7249\n",
            "Epoch 1/1, Batch 6980/98059, Loss: 3.4809\n",
            "Epoch 1/1, Batch 7000/98059, Loss: 3.4889\n",
            "Epoch 1/1, Batch 7020/98059, Loss: 3.5215\n",
            "Epoch 1/1, Batch 7040/98059, Loss: 3.0763\n",
            "Epoch 1/1, Batch 7060/98059, Loss: 3.1041\n",
            "Epoch 1/1, Batch 7080/98059, Loss: 3.1227\n",
            "Epoch 1/1, Batch 7100/98059, Loss: 3.5805\n",
            "Epoch 1/1, Batch 7120/98059, Loss: 3.4387\n",
            "Epoch 1/1, Batch 7140/98059, Loss: 3.4563\n",
            "Epoch 1/1, Batch 7160/98059, Loss: 3.5371\n",
            "Epoch 1/1, Batch 7180/98059, Loss: 3.7298\n",
            "Epoch 1/1, Batch 7200/98059, Loss: 3.8087\n",
            "Epoch 1/1, Batch 7220/98059, Loss: 3.6832\n",
            "Epoch 1/1, Batch 7240/98059, Loss: 3.4150\n",
            "Epoch 1/1, Batch 7260/98059, Loss: 3.5134\n",
            "Epoch 1/1, Batch 7280/98059, Loss: 3.6251\n",
            "Epoch 1/1, Batch 7300/98059, Loss: 3.3772\n",
            "Epoch 1/1, Batch 7320/98059, Loss: 3.0973\n",
            "Epoch 1/1, Batch 7340/98059, Loss: 3.1721\n",
            "Epoch 1/1, Batch 7360/98059, Loss: 3.3914\n",
            "Epoch 1/1, Batch 7380/98059, Loss: 3.8034\n",
            "Epoch 1/1, Batch 7400/98059, Loss: 3.1488\n",
            "Epoch 1/1, Batch 7420/98059, Loss: 3.7595\n",
            "Epoch 1/1, Batch 7440/98059, Loss: 3.5616\n",
            "Epoch 1/1, Batch 7460/98059, Loss: 3.5821\n",
            "Epoch 1/1, Batch 7480/98059, Loss: 3.5388\n",
            "Epoch 1/1, Batch 7500/98059, Loss: 2.9577\n",
            "Epoch 1/1, Batch 7520/98059, Loss: 3.3630\n",
            "Epoch 1/1, Batch 7540/98059, Loss: 3.0420\n",
            "Epoch 1/1, Batch 7560/98059, Loss: 3.5363\n",
            "Epoch 1/1, Batch 7580/98059, Loss: 3.3010\n",
            "Epoch 1/1, Batch 7600/98059, Loss: 3.5795\n",
            "Epoch 1/1, Batch 7620/98059, Loss: 3.4605\n",
            "Epoch 1/1, Batch 7640/98059, Loss: 3.3217\n",
            "Epoch 1/1, Batch 7660/98059, Loss: 3.2529\n",
            "Epoch 1/1, Batch 7680/98059, Loss: 3.5699\n",
            "Epoch 1/1, Batch 7700/98059, Loss: 3.3894\n",
            "Epoch 1/1, Batch 7720/98059, Loss: 3.3076\n",
            "Epoch 1/1, Batch 7740/98059, Loss: 2.9285\n",
            "Epoch 1/1, Batch 7760/98059, Loss: 3.2345\n",
            "Epoch 1/1, Batch 7780/98059, Loss: 3.2471\n",
            "Epoch 1/1, Batch 7800/98059, Loss: 3.4111\n",
            "Epoch 1/1, Batch 7820/98059, Loss: 3.2240\n",
            "Epoch 1/1, Batch 7840/98059, Loss: 3.3607\n",
            "Epoch 1/1, Batch 7860/98059, Loss: 3.3679\n",
            "Epoch 1/1, Batch 7880/98059, Loss: 3.8094\n",
            "Epoch 1/1, Batch 7900/98059, Loss: 3.4731\n",
            "Epoch 1/1, Batch 7920/98059, Loss: 3.7120\n",
            "Epoch 1/1, Batch 7940/98059, Loss: 3.5996\n",
            "Epoch 1/1, Batch 7960/98059, Loss: 3.9039\n",
            "Epoch 1/1, Batch 7980/98059, Loss: 3.6350\n",
            "Epoch 1/1, Batch 8000/98059, Loss: 3.4683\n",
            "Epoch 1/1, Batch 8020/98059, Loss: 3.3375\n",
            "Epoch 1/1, Batch 8040/98059, Loss: 3.5672\n",
            "Epoch 1/1, Batch 8060/98059, Loss: 3.5750\n",
            "Epoch 1/1, Batch 8080/98059, Loss: 4.0834\n",
            "Epoch 1/1, Batch 8100/98059, Loss: 3.4242\n",
            "Epoch 1/1, Batch 8120/98059, Loss: 3.3837\n",
            "Epoch 1/1, Batch 8140/98059, Loss: 3.4201\n",
            "Epoch 1/1, Batch 8160/98059, Loss: 3.4536\n",
            "Epoch 1/1, Batch 8180/98059, Loss: 3.8654\n",
            "Epoch 1/1, Batch 8200/98059, Loss: 3.3224\n",
            "Epoch 1/1, Batch 8220/98059, Loss: 3.2797\n",
            "Epoch 1/1, Batch 8240/98059, Loss: 3.6350\n",
            "Epoch 1/1, Batch 8260/98059, Loss: 3.4293\n",
            "Epoch 1/1, Batch 8280/98059, Loss: 3.6786\n",
            "Epoch 1/1, Batch 8300/98059, Loss: 3.3338\n",
            "Epoch 1/1, Batch 8320/98059, Loss: 3.5721\n",
            "Epoch 1/1, Batch 8340/98059, Loss: 3.6422\n",
            "Epoch 1/1, Batch 8360/98059, Loss: 3.0785\n",
            "Epoch 1/1, Batch 8380/98059, Loss: 3.2668\n",
            "Epoch 1/1, Batch 8400/98059, Loss: 3.0856\n",
            "Epoch 1/1, Batch 8420/98059, Loss: 3.2592\n",
            "Epoch 1/1, Batch 8440/98059, Loss: 3.3681\n",
            "Epoch 1/1, Batch 8460/98059, Loss: 3.6121\n",
            "Epoch 1/1, Batch 8480/98059, Loss: 3.5050\n",
            "Epoch 1/1, Batch 8500/98059, Loss: 3.3727\n",
            "Epoch 1/1, Batch 8520/98059, Loss: 3.3270\n",
            "Epoch 1/1, Batch 8540/98059, Loss: 3.6682\n",
            "Epoch 1/1, Batch 8560/98059, Loss: 3.0609\n",
            "Epoch 1/1, Batch 8580/98059, Loss: 3.1376\n",
            "Epoch 1/1, Batch 8600/98059, Loss: 3.5814\n",
            "Epoch 1/1, Batch 8620/98059, Loss: 3.5530\n",
            "Epoch 1/1, Batch 8640/98059, Loss: 3.5586\n",
            "Epoch 1/1, Batch 8660/98059, Loss: 3.5034\n",
            "Epoch 1/1, Batch 8680/98059, Loss: 3.5906\n",
            "Epoch 1/1, Batch 8700/98059, Loss: 3.5057\n",
            "Epoch 1/1, Batch 8720/98059, Loss: 3.0271\n",
            "Epoch 1/1, Batch 8740/98059, Loss: 3.1836\n",
            "Epoch 1/1, Batch 8760/98059, Loss: 3.3186\n",
            "Epoch 1/1, Batch 8780/98059, Loss: 3.1516\n",
            "Epoch 1/1, Batch 8800/98059, Loss: 3.2737\n",
            "Epoch 1/1, Batch 8820/98059, Loss: 3.5563\n",
            "Epoch 1/1, Batch 8840/98059, Loss: 2.9110\n",
            "Epoch 1/1, Batch 8860/98059, Loss: 3.4776\n",
            "Epoch 1/1, Batch 8880/98059, Loss: 3.2476\n",
            "Epoch 1/1, Batch 8900/98059, Loss: 3.6918\n",
            "Epoch 1/1, Batch 8920/98059, Loss: 3.1848\n",
            "Epoch 1/1, Batch 8940/98059, Loss: 3.5184\n",
            "Epoch 1/1, Batch 8960/98059, Loss: 3.1988\n",
            "Epoch 1/1, Batch 8980/98059, Loss: 3.2606\n",
            "Epoch 1/1, Batch 9000/98059, Loss: 3.5116\n",
            "Epoch 1/1, Batch 9020/98059, Loss: 3.5184\n",
            "Epoch 1/1, Batch 9040/98059, Loss: 3.6720\n",
            "Epoch 1/1, Batch 9060/98059, Loss: 2.8976\n",
            "Epoch 1/1, Batch 9080/98059, Loss: 3.9425\n",
            "Epoch 1/1, Batch 9100/98059, Loss: 3.3853\n",
            "Epoch 1/1, Batch 9120/98059, Loss: 3.2365\n",
            "Epoch 1/1, Batch 9140/98059, Loss: 3.3516\n",
            "Epoch 1/1, Batch 9160/98059, Loss: 3.3568\n",
            "Epoch 1/1, Batch 9180/98059, Loss: 3.4905\n",
            "Epoch 1/1, Batch 9200/98059, Loss: 3.4774\n",
            "Epoch 1/1, Batch 9220/98059, Loss: 3.5103\n",
            "Epoch 1/1, Batch 9240/98059, Loss: 3.7108\n",
            "Epoch 1/1, Batch 9260/98059, Loss: 3.4230\n",
            "Epoch 1/1, Batch 9280/98059, Loss: 3.0535\n",
            "Epoch 1/1, Batch 9300/98059, Loss: 3.2710\n",
            "Epoch 1/1, Batch 9320/98059, Loss: 3.1841\n",
            "Epoch 1/1, Batch 9340/98059, Loss: 3.5533\n",
            "Epoch 1/1, Batch 9360/98059, Loss: 3.1439\n",
            "Epoch 1/1, Batch 9380/98059, Loss: 3.6562\n",
            "Epoch 1/1, Batch 9400/98059, Loss: 3.8899\n",
            "Epoch 1/1, Batch 9420/98059, Loss: 3.2962\n",
            "Epoch 1/1, Batch 9440/98059, Loss: 3.0349\n",
            "Epoch 1/1, Batch 9460/98059, Loss: 3.7542\n",
            "Epoch 1/1, Batch 9480/98059, Loss: 3.4252\n",
            "Epoch 1/1, Batch 9500/98059, Loss: 3.3858\n",
            "Epoch 1/1, Batch 9520/98059, Loss: 3.2753\n",
            "Epoch 1/1, Batch 9540/98059, Loss: 3.5075\n",
            "Epoch 1/1, Batch 9560/98059, Loss: 3.0007\n",
            "Epoch 1/1, Batch 9580/98059, Loss: 3.1201\n",
            "Epoch 1/1, Batch 9600/98059, Loss: 3.7883\n",
            "Epoch 1/1, Batch 9620/98059, Loss: 3.4219\n",
            "Epoch 1/1, Batch 9640/98059, Loss: 3.3515\n",
            "Epoch 1/1, Batch 9660/98059, Loss: 3.3585\n",
            "Epoch 1/1, Batch 9680/98059, Loss: 3.5859\n",
            "Epoch 1/1, Batch 9700/98059, Loss: 3.2562\n",
            "Epoch 1/1, Batch 9720/98059, Loss: 3.1132\n",
            "Epoch 1/1, Batch 9740/98059, Loss: 3.7153\n",
            "Epoch 1/1, Batch 9760/98059, Loss: 3.5270\n",
            "Epoch 1/1, Batch 9780/98059, Loss: 2.9901\n",
            "Epoch 1/1, Batch 9800/98059, Loss: 3.6548\n",
            "Epoch 1/1, Batch 9820/98059, Loss: 3.4625\n",
            "Epoch 1/1, Batch 9840/98059, Loss: 3.0487\n",
            "Epoch 1/1, Batch 9860/98059, Loss: 3.4481\n",
            "Epoch 1/1, Batch 9880/98059, Loss: 3.0662\n",
            "Epoch 1/1, Batch 9900/98059, Loss: 3.5065\n",
            "Epoch 1/1, Batch 9920/98059, Loss: 3.1837\n",
            "Epoch 1/1, Batch 9940/98059, Loss: 3.3367\n",
            "Epoch 1/1, Batch 9960/98059, Loss: 3.2687\n",
            "Epoch 1/1, Batch 9980/98059, Loss: 3.6681\n",
            "Epoch 1/1, Batch 10000/98059, Loss: 2.9892\n",
            "Epoch 1/1, Batch 10020/98059, Loss: 3.4583\n",
            "Epoch 1/1, Batch 10040/98059, Loss: 3.1896\n",
            "Epoch 1/1, Batch 10060/98059, Loss: 2.9165\n",
            "Epoch 1/1, Batch 10080/98059, Loss: 3.3855\n",
            "Epoch 1/1, Batch 10100/98059, Loss: 3.6551\n",
            "Epoch 1/1, Batch 10120/98059, Loss: 3.1890\n",
            "Epoch 1/1, Batch 10140/98059, Loss: 3.7319\n",
            "Epoch 1/1, Batch 10160/98059, Loss: 3.4938\n",
            "Epoch 1/1, Batch 10180/98059, Loss: 3.4882\n",
            "Epoch 1/1, Batch 10200/98059, Loss: 3.2051\n",
            "Epoch 1/1, Batch 10220/98059, Loss: 3.3629\n",
            "Epoch 1/1, Batch 10240/98059, Loss: 3.5322\n",
            "Epoch 1/1, Batch 10260/98059, Loss: 3.4028\n",
            "Epoch 1/1, Batch 10280/98059, Loss: 3.0499\n",
            "Epoch 1/1, Batch 10300/98059, Loss: 3.4139\n",
            "Epoch 1/1, Batch 10320/98059, Loss: 3.5449\n",
            "Epoch 1/1, Batch 10340/98059, Loss: 3.5975\n",
            "Epoch 1/1, Batch 10360/98059, Loss: 3.3746\n",
            "Epoch 1/1, Batch 10380/98059, Loss: 3.6877\n",
            "Epoch 1/1, Batch 10400/98059, Loss: 3.5094\n",
            "Epoch 1/1, Batch 10420/98059, Loss: 3.3176\n",
            "Epoch 1/1, Batch 10440/98059, Loss: 3.2532\n",
            "Epoch 1/1, Batch 10460/98059, Loss: 3.1470\n",
            "Epoch 1/1, Batch 10480/98059, Loss: 3.7164\n",
            "Epoch 1/1, Batch 10500/98059, Loss: 3.2769\n",
            "Epoch 1/1, Batch 10520/98059, Loss: 3.4078\n",
            "Epoch 1/1, Batch 10540/98059, Loss: 3.3024\n",
            "Epoch 1/1, Batch 10560/98059, Loss: 3.3080\n",
            "Epoch 1/1, Batch 10580/98059, Loss: 3.4571\n",
            "Epoch 1/1, Batch 10600/98059, Loss: 3.4571\n",
            "Epoch 1/1, Batch 10620/98059, Loss: 3.4765\n",
            "Epoch 1/1, Batch 10640/98059, Loss: 2.9598\n",
            "Epoch 1/1, Batch 10660/98059, Loss: 3.1406\n",
            "Epoch 1/1, Batch 10680/98059, Loss: 3.5231\n",
            "Epoch 1/1, Batch 10700/98059, Loss: 3.3833\n",
            "Epoch 1/1, Batch 10720/98059, Loss: 3.3332\n",
            "Epoch 1/1, Batch 10740/98059, Loss: 3.3603\n",
            "Epoch 1/1, Batch 10760/98059, Loss: 3.4261\n",
            "Epoch 1/1, Batch 10780/98059, Loss: 3.7247\n",
            "Epoch 1/1, Batch 10800/98059, Loss: 3.8275\n",
            "Epoch 1/1, Batch 10820/98059, Loss: 3.5376\n",
            "Epoch 1/1, Batch 10840/98059, Loss: 3.5023\n",
            "Epoch 1/1, Batch 10860/98059, Loss: 3.3930\n",
            "Epoch 1/1, Batch 10880/98059, Loss: 3.4170\n",
            "Epoch 1/1, Batch 10900/98059, Loss: 3.3529\n",
            "Epoch 1/1, Batch 10920/98059, Loss: 3.5872\n",
            "Epoch 1/1, Batch 10940/98059, Loss: 3.9050\n",
            "Epoch 1/1, Batch 10960/98059, Loss: 3.4805\n",
            "Epoch 1/1, Batch 10980/98059, Loss: 3.2350\n",
            "Epoch 1/1, Batch 11000/98059, Loss: 2.9462\n",
            "Epoch 1/1, Batch 11020/98059, Loss: 3.3358\n",
            "Epoch 1/1, Batch 11040/98059, Loss: 3.2865\n",
            "Epoch 1/1, Batch 11060/98059, Loss: 3.6980\n",
            "Epoch 1/1, Batch 11080/98059, Loss: 3.4622\n",
            "Epoch 1/1, Batch 11100/98059, Loss: 3.5446\n",
            "Epoch 1/1, Batch 11120/98059, Loss: 3.2921\n",
            "Epoch 1/1, Batch 11140/98059, Loss: 3.4552\n",
            "Epoch 1/1, Batch 11160/98059, Loss: 3.8654\n",
            "Epoch 1/1, Batch 11180/98059, Loss: 3.4910\n",
            "Epoch 1/1, Batch 11200/98059, Loss: 3.0675\n",
            "Epoch 1/1, Batch 11220/98059, Loss: 3.3333\n",
            "Epoch 1/1, Batch 11240/98059, Loss: 3.4081\n",
            "Epoch 1/1, Batch 11260/98059, Loss: 3.2418\n",
            "Epoch 1/1, Batch 11280/98059, Loss: 3.3732\n",
            "Epoch 1/1, Batch 11300/98059, Loss: 3.4902\n",
            "Epoch 1/1, Batch 11320/98059, Loss: 3.4773\n",
            "Epoch 1/1, Batch 11340/98059, Loss: 3.5260\n",
            "Epoch 1/1, Batch 11360/98059, Loss: 3.4275\n",
            "Epoch 1/1, Batch 11380/98059, Loss: 3.4233\n",
            "Epoch 1/1, Batch 11400/98059, Loss: 3.4011\n",
            "Epoch 1/1, Batch 11420/98059, Loss: 3.6287\n",
            "Epoch 1/1, Batch 11440/98059, Loss: 3.3721\n",
            "Epoch 1/1, Batch 11460/98059, Loss: 3.3707\n",
            "Epoch 1/1, Batch 11480/98059, Loss: 3.1494\n",
            "Epoch 1/1, Batch 11500/98059, Loss: 3.4291\n",
            "Epoch 1/1, Batch 11520/98059, Loss: 2.9350\n",
            "Epoch 1/1, Batch 11540/98059, Loss: 3.3198\n",
            "Epoch 1/1, Batch 11560/98059, Loss: 3.0469\n",
            "Epoch 1/1, Batch 11580/98059, Loss: 3.6468\n",
            "Epoch 1/1, Batch 11600/98059, Loss: 3.5640\n",
            "Epoch 1/1, Batch 11620/98059, Loss: 3.4555\n",
            "Epoch 1/1, Batch 11640/98059, Loss: 3.0927\n",
            "Epoch 1/1, Batch 11660/98059, Loss: 3.0020\n",
            "Epoch 1/1, Batch 11680/98059, Loss: 3.2475\n",
            "Epoch 1/1, Batch 11700/98059, Loss: 3.2903\n",
            "Epoch 1/1, Batch 11720/98059, Loss: 3.1855\n",
            "Epoch 1/1, Batch 11740/98059, Loss: 3.4587\n",
            "Epoch 1/1, Batch 11760/98059, Loss: 3.5177\n",
            "Epoch 1/1, Batch 11780/98059, Loss: 3.3642\n",
            "Epoch 1/1, Batch 11800/98059, Loss: 3.2353\n",
            "Epoch 1/1, Batch 11820/98059, Loss: 3.0157\n",
            "Epoch 1/1, Batch 11840/98059, Loss: 3.4496\n",
            "Epoch 1/1, Batch 11860/98059, Loss: 3.1818\n",
            "Epoch 1/1, Batch 11880/98059, Loss: 3.6689\n",
            "Epoch 1/1, Batch 11900/98059, Loss: 3.4093\n",
            "Epoch 1/1, Batch 11920/98059, Loss: 3.3015\n",
            "Epoch 1/1, Batch 11940/98059, Loss: 3.6868\n",
            "Epoch 1/1, Batch 11960/98059, Loss: 3.5164\n",
            "Epoch 1/1, Batch 11980/98059, Loss: 3.0972\n",
            "Epoch 1/1, Batch 12000/98059, Loss: 3.3309\n",
            "Epoch 1/1, Batch 12020/98059, Loss: 3.1131\n",
            "Epoch 1/1, Batch 12040/98059, Loss: 3.4040\n",
            "Epoch 1/1, Batch 12060/98059, Loss: 3.0807\n",
            "Epoch 1/1, Batch 12080/98059, Loss: 3.4604\n",
            "Epoch 1/1, Batch 12100/98059, Loss: 3.4380\n",
            "Epoch 1/1, Batch 12120/98059, Loss: 3.3978\n",
            "Epoch 1/1, Batch 12140/98059, Loss: 3.3339\n",
            "Epoch 1/1, Batch 12160/98059, Loss: 3.5766\n",
            "Epoch 1/1, Batch 12180/98059, Loss: 3.3956\n",
            "Epoch 1/1, Batch 12200/98059, Loss: 3.6036\n",
            "Epoch 1/1, Batch 12220/98059, Loss: 3.4834\n",
            "Epoch 1/1, Batch 12240/98059, Loss: 3.3058\n",
            "Epoch 1/1, Batch 12260/98059, Loss: 2.9115\n",
            "Epoch 1/1, Batch 12280/98059, Loss: 3.4251\n",
            "Epoch 1/1, Batch 12300/98059, Loss: 3.3931\n",
            "Epoch 1/1, Batch 12320/98059, Loss: 3.2437\n",
            "Epoch 1/1, Batch 12340/98059, Loss: 3.3492\n",
            "Epoch 1/1, Batch 12360/98059, Loss: 3.5680\n",
            "Epoch 1/1, Batch 12380/98059, Loss: 3.8157\n",
            "Epoch 1/1, Batch 12400/98059, Loss: 3.2213\n",
            "Epoch 1/1, Batch 12420/98059, Loss: 3.5590\n",
            "Epoch 1/1, Batch 12440/98059, Loss: 3.3571\n",
            "Epoch 1/1, Batch 12460/98059, Loss: 3.2553\n",
            "Epoch 1/1, Batch 12480/98059, Loss: 3.5052\n",
            "Epoch 1/1, Batch 12500/98059, Loss: 3.1574\n",
            "Epoch 1/1, Batch 12520/98059, Loss: 3.2481\n",
            "Epoch 1/1, Batch 12540/98059, Loss: 3.3184\n",
            "Epoch 1/1, Batch 12560/98059, Loss: 3.4973\n",
            "Epoch 1/1, Batch 12580/98059, Loss: 2.8366\n",
            "Epoch 1/1, Batch 12600/98059, Loss: 3.4815\n",
            "Epoch 1/1, Batch 12620/98059, Loss: 3.1908\n",
            "Epoch 1/1, Batch 12640/98059, Loss: 3.2576\n",
            "Epoch 1/1, Batch 12660/98059, Loss: 3.2242\n",
            "Epoch 1/1, Batch 12680/98059, Loss: 3.1353\n",
            "Epoch 1/1, Batch 12700/98059, Loss: 3.1835\n",
            "Epoch 1/1, Batch 12720/98059, Loss: 3.8634\n",
            "Epoch 1/1, Batch 12740/98059, Loss: 3.6164\n",
            "Epoch 1/1, Batch 12760/98059, Loss: 2.8626\n",
            "Epoch 1/1, Batch 12780/98059, Loss: 3.6092\n",
            "Epoch 1/1, Batch 12800/98059, Loss: 3.3833\n",
            "Epoch 1/1, Batch 12820/98059, Loss: 3.2954\n",
            "Epoch 1/1, Batch 12840/98059, Loss: 2.9213\n",
            "Epoch 1/1, Batch 12860/98059, Loss: 3.5395\n",
            "Epoch 1/1, Batch 12880/98059, Loss: 3.6228\n",
            "Epoch 1/1, Batch 12900/98059, Loss: 3.4401\n",
            "Epoch 1/1, Batch 12920/98059, Loss: 3.2934\n",
            "Epoch 1/1, Batch 12940/98059, Loss: 2.8279\n",
            "Epoch 1/1, Batch 12960/98059, Loss: 3.6545\n",
            "Epoch 1/1, Batch 12980/98059, Loss: 3.5779\n",
            "Epoch 1/1, Batch 13000/98059, Loss: 3.3493\n",
            "Epoch 1/1, Batch 13020/98059, Loss: 3.5151\n",
            "Epoch 1/1, Batch 13040/98059, Loss: 3.2748\n",
            "Epoch 1/1, Batch 13060/98059, Loss: 3.2691\n",
            "Epoch 1/1, Batch 13080/98059, Loss: 3.4800\n",
            "Epoch 1/1, Batch 13100/98059, Loss: 2.9943\n",
            "Epoch 1/1, Batch 13120/98059, Loss: 3.1280\n",
            "Epoch 1/1, Batch 13140/98059, Loss: 3.1797\n",
            "Epoch 1/1, Batch 13160/98059, Loss: 3.1475\n",
            "Epoch 1/1, Batch 13180/98059, Loss: 3.1271\n",
            "Epoch 1/1, Batch 13200/98059, Loss: 3.0830\n",
            "Epoch 1/1, Batch 13220/98059, Loss: 3.3505\n",
            "Epoch 1/1, Batch 13240/98059, Loss: 3.6181\n",
            "Epoch 1/1, Batch 13260/98059, Loss: 3.1647\n",
            "Epoch 1/1, Batch 13280/98059, Loss: 3.0387\n",
            "Epoch 1/1, Batch 13300/98059, Loss: 3.4894\n",
            "Epoch 1/1, Batch 13320/98059, Loss: 3.2529\n",
            "Epoch 1/1, Batch 13340/98059, Loss: 3.0893\n",
            "Epoch 1/1, Batch 13360/98059, Loss: 3.4577\n",
            "Epoch 1/1, Batch 13380/98059, Loss: 3.3044\n",
            "Epoch 1/1, Batch 13400/98059, Loss: 2.9815\n",
            "Epoch 1/1, Batch 13420/98059, Loss: 3.6395\n",
            "Epoch 1/1, Batch 13440/98059, Loss: 3.2136\n",
            "Epoch 1/1, Batch 13460/98059, Loss: 3.3579\n",
            "Epoch 1/1, Batch 13480/98059, Loss: 3.1169\n",
            "Epoch 1/1, Batch 13500/98059, Loss: 3.3105\n",
            "Epoch 1/1, Batch 13520/98059, Loss: 3.5037\n",
            "Epoch 1/1, Batch 13540/98059, Loss: 3.5740\n",
            "Epoch 1/1, Batch 13560/98059, Loss: 3.2547\n",
            "Epoch 1/1, Batch 13580/98059, Loss: 3.1446\n",
            "Epoch 1/1, Batch 13600/98059, Loss: 2.8975\n",
            "Epoch 1/1, Batch 13620/98059, Loss: 3.5532\n",
            "Epoch 1/1, Batch 13640/98059, Loss: 3.0708\n",
            "Epoch 1/1, Batch 13660/98059, Loss: 3.1809\n",
            "Epoch 1/1, Batch 13680/98059, Loss: 3.3682\n",
            "Epoch 1/1, Batch 13700/98059, Loss: 3.4427\n",
            "Epoch 1/1, Batch 13720/98059, Loss: 3.5172\n",
            "Epoch 1/1, Batch 13740/98059, Loss: 2.9940\n",
            "Epoch 1/1, Batch 13760/98059, Loss: 3.1665\n",
            "Epoch 1/1, Batch 13780/98059, Loss: 3.0200\n",
            "Epoch 1/1, Batch 13800/98059, Loss: 3.4903\n",
            "Epoch 1/1, Batch 13820/98059, Loss: 3.2318\n",
            "Epoch 1/1, Batch 13840/98059, Loss: 3.3004\n",
            "Epoch 1/1, Batch 13860/98059, Loss: 2.9494\n",
            "Epoch 1/1, Batch 13880/98059, Loss: 3.5749\n",
            "Epoch 1/1, Batch 13900/98059, Loss: 3.6016\n",
            "Epoch 1/1, Batch 13920/98059, Loss: 3.6331\n",
            "Epoch 1/1, Batch 13940/98059, Loss: 3.4298\n",
            "Epoch 1/1, Batch 13960/98059, Loss: 3.4020\n",
            "Epoch 1/1, Batch 13980/98059, Loss: 3.6131\n",
            "Epoch 1/1, Batch 14000/98059, Loss: 3.3542\n",
            "Epoch 1/1, Batch 14020/98059, Loss: 3.3190\n",
            "Epoch 1/1, Batch 14040/98059, Loss: 3.5921\n",
            "Epoch 1/1, Batch 14060/98059, Loss: 3.4292\n",
            "Epoch 1/1, Batch 14080/98059, Loss: 3.8180\n",
            "Epoch 1/1, Batch 14100/98059, Loss: 3.5675\n",
            "Epoch 1/1, Batch 14120/98059, Loss: 3.0794\n",
            "Epoch 1/1, Batch 14140/98059, Loss: 3.0244\n",
            "Epoch 1/1, Batch 14160/98059, Loss: 3.1462\n",
            "Epoch 1/1, Batch 14180/98059, Loss: 2.9480\n",
            "Epoch 1/1, Batch 14200/98059, Loss: 2.9046\n",
            "Epoch 1/1, Batch 14220/98059, Loss: 3.2154\n",
            "Epoch 1/1, Batch 14240/98059, Loss: 3.1409\n",
            "Epoch 1/1, Batch 14260/98059, Loss: 3.3455\n",
            "Epoch 1/1, Batch 14280/98059, Loss: 3.2557\n",
            "Epoch 1/1, Batch 14300/98059, Loss: 3.5584\n",
            "Epoch 1/1, Batch 14320/98059, Loss: 3.0552\n",
            "Epoch 1/1, Batch 14340/98059, Loss: 3.3978\n",
            "Epoch 1/1, Batch 14360/98059, Loss: 3.2826\n",
            "Epoch 1/1, Batch 14380/98059, Loss: 3.3442\n",
            "Epoch 1/1, Batch 14400/98059, Loss: 3.2546\n",
            "Epoch 1/1, Batch 14420/98059, Loss: 3.2503\n",
            "Epoch 1/1, Batch 14440/98059, Loss: 3.4015\n",
            "Epoch 1/1, Batch 14460/98059, Loss: 3.1617\n",
            "Epoch 1/1, Batch 14480/98059, Loss: 2.8614\n",
            "Epoch 1/1, Batch 14500/98059, Loss: 3.4962\n",
            "Epoch 1/1, Batch 14520/98059, Loss: 3.6971\n",
            "Epoch 1/1, Batch 14540/98059, Loss: 2.6437\n",
            "Epoch 1/1, Batch 14560/98059, Loss: 3.5421\n",
            "Epoch 1/1, Batch 14580/98059, Loss: 3.2465\n",
            "Epoch 1/1, Batch 14600/98059, Loss: 3.4105\n",
            "Epoch 1/1, Batch 14620/98059, Loss: 3.1022\n",
            "Epoch 1/1, Batch 14640/98059, Loss: 3.4558\n",
            "Epoch 1/1, Batch 14660/98059, Loss: 3.3496\n",
            "Epoch 1/1, Batch 14680/98059, Loss: 3.0413\n",
            "Epoch 1/1, Batch 14700/98059, Loss: 3.3754\n",
            "Epoch 1/1, Batch 14720/98059, Loss: 3.5029\n",
            "Epoch 1/1, Batch 14740/98059, Loss: 3.5989\n",
            "Epoch 1/1, Batch 14760/98059, Loss: 3.2286\n",
            "Epoch 1/1, Batch 14780/98059, Loss: 3.1656\n",
            "Epoch 1/1, Batch 14800/98059, Loss: 3.2630\n",
            "Epoch 1/1, Batch 14820/98059, Loss: 3.1408\n",
            "Epoch 1/1, Batch 14840/98059, Loss: 3.1269\n",
            "Epoch 1/1, Batch 14860/98059, Loss: 3.1776\n",
            "Epoch 1/1, Batch 14880/98059, Loss: 3.4541\n",
            "Epoch 1/1, Batch 14900/98059, Loss: 3.0942\n",
            "Epoch 1/1, Batch 14920/98059, Loss: 3.1761\n",
            "Epoch 1/1, Batch 14940/98059, Loss: 3.5302\n",
            "Epoch 1/1, Batch 14960/98059, Loss: 3.3079\n",
            "Epoch 1/1, Batch 14980/98059, Loss: 3.1659\n",
            "Epoch 1/1, Batch 15000/98059, Loss: 3.2150\n",
            "Epoch 1/1, Batch 15020/98059, Loss: 3.3930\n",
            "Epoch 1/1, Batch 15040/98059, Loss: 3.4165\n",
            "Epoch 1/1, Batch 15060/98059, Loss: 3.2728\n",
            "Epoch 1/1, Batch 15080/98059, Loss: 3.6171\n",
            "Epoch 1/1, Batch 15100/98059, Loss: 3.3467\n",
            "Epoch 1/1, Batch 15120/98059, Loss: 2.9810\n",
            "Epoch 1/1, Batch 15140/98059, Loss: 3.2835\n",
            "Epoch 1/1, Batch 15160/98059, Loss: 3.1793\n",
            "Epoch 1/1, Batch 15180/98059, Loss: 3.2817\n",
            "Epoch 1/1, Batch 15200/98059, Loss: 2.8100\n",
            "Epoch 1/1, Batch 15220/98059, Loss: 3.6210\n",
            "Epoch 1/1, Batch 15240/98059, Loss: 3.6003\n",
            "Epoch 1/1, Batch 15260/98059, Loss: 3.2468\n",
            "Epoch 1/1, Batch 15280/98059, Loss: 3.4109\n",
            "Epoch 1/1, Batch 15300/98059, Loss: 2.9404\n",
            "Epoch 1/1, Batch 15320/98059, Loss: 3.2458\n",
            "Epoch 1/1, Batch 15340/98059, Loss: 3.4424\n",
            "Epoch 1/1, Batch 15360/98059, Loss: 3.5350\n",
            "Epoch 1/1, Batch 15380/98059, Loss: 3.3346\n",
            "Epoch 1/1, Batch 15400/98059, Loss: 3.3933\n",
            "Epoch 1/1, Batch 15420/98059, Loss: 3.4292\n",
            "Epoch 1/1, Batch 15440/98059, Loss: 3.3610\n",
            "Epoch 1/1, Batch 15460/98059, Loss: 3.2369\n",
            "Epoch 1/1, Batch 15480/98059, Loss: 3.4313\n",
            "Epoch 1/1, Batch 15500/98059, Loss: 3.2611\n",
            "Epoch 1/1, Batch 15520/98059, Loss: 2.8518\n",
            "Epoch 1/1, Batch 15540/98059, Loss: 3.1157\n",
            "Epoch 1/1, Batch 15560/98059, Loss: 3.6693\n",
            "Epoch 1/1, Batch 15580/98059, Loss: 3.1157\n",
            "Epoch 1/1, Batch 15600/98059, Loss: 3.0884\n",
            "Epoch 1/1, Batch 15620/98059, Loss: 3.2220\n",
            "Epoch 1/1, Batch 15640/98059, Loss: 3.3204\n",
            "Epoch 1/1, Batch 15660/98059, Loss: 3.4682\n",
            "Epoch 1/1, Batch 15680/98059, Loss: 3.0053\n",
            "Epoch 1/1, Batch 15700/98059, Loss: 3.2632\n",
            "Epoch 1/1, Batch 15720/98059, Loss: 3.4189\n",
            "Epoch 1/1, Batch 15740/98059, Loss: 3.2409\n",
            "Epoch 1/1, Batch 15760/98059, Loss: 3.2469\n",
            "Epoch 1/1, Batch 15780/98059, Loss: 3.3854\n",
            "Epoch 1/1, Batch 15800/98059, Loss: 2.6097\n",
            "Epoch 1/1, Batch 15820/98059, Loss: 3.4652\n",
            "Epoch 1/1, Batch 15840/98059, Loss: 3.3294\n",
            "Epoch 1/1, Batch 15860/98059, Loss: 3.2457\n",
            "Epoch 1/1, Batch 15880/98059, Loss: 3.0544\n",
            "Epoch 1/1, Batch 15900/98059, Loss: 3.1619\n",
            "Epoch 1/1, Batch 15920/98059, Loss: 3.5737\n",
            "Epoch 1/1, Batch 15940/98059, Loss: 3.1171\n",
            "Epoch 1/1, Batch 15960/98059, Loss: 3.4245\n",
            "Epoch 1/1, Batch 15980/98059, Loss: 3.4672\n",
            "Epoch 1/1, Batch 16000/98059, Loss: 3.0699\n",
            "Epoch 1/1, Batch 16020/98059, Loss: 3.3089\n",
            "Epoch 1/1, Batch 16040/98059, Loss: 3.5888\n",
            "Epoch 1/1, Batch 16060/98059, Loss: 3.1874\n",
            "Epoch 1/1, Batch 16080/98059, Loss: 3.6079\n",
            "Epoch 1/1, Batch 16100/98059, Loss: 3.0123\n",
            "Epoch 1/1, Batch 16120/98059, Loss: 3.3246\n",
            "Epoch 1/1, Batch 16140/98059, Loss: 3.4788\n",
            "Epoch 1/1, Batch 16160/98059, Loss: 3.2440\n",
            "Epoch 1/1, Batch 16180/98059, Loss: 3.2516\n",
            "Epoch 1/1, Batch 16200/98059, Loss: 3.0671\n",
            "Epoch 1/1, Batch 16220/98059, Loss: 2.7503\n",
            "Epoch 1/1, Batch 16240/98059, Loss: 3.4228\n",
            "Epoch 1/1, Batch 16260/98059, Loss: 3.2275\n",
            "Epoch 1/1, Batch 16280/98059, Loss: 3.4507\n",
            "Epoch 1/1, Batch 16300/98059, Loss: 3.1439\n",
            "Epoch 1/1, Batch 16320/98059, Loss: 3.3739\n",
            "Epoch 1/1, Batch 16340/98059, Loss: 3.4161\n",
            "Epoch 1/1, Batch 16360/98059, Loss: 3.4281\n",
            "Epoch 1/1, Batch 16380/98059, Loss: 2.9306\n",
            "Epoch 1/1, Batch 16400/98059, Loss: 3.2502\n",
            "Epoch 1/1, Batch 16420/98059, Loss: 3.3533\n",
            "Epoch 1/1, Batch 16440/98059, Loss: 3.3624\n",
            "Epoch 1/1, Batch 16460/98059, Loss: 3.5453\n",
            "Epoch 1/1, Batch 16480/98059, Loss: 3.0718\n",
            "Epoch 1/1, Batch 16500/98059, Loss: 3.6478\n",
            "Epoch 1/1, Batch 16520/98059, Loss: 3.3354\n",
            "Epoch 1/1, Batch 16540/98059, Loss: 2.9830\n",
            "Epoch 1/1, Batch 16560/98059, Loss: 2.8135\n",
            "Epoch 1/1, Batch 16580/98059, Loss: 3.2580\n",
            "Epoch 1/1, Batch 16600/98059, Loss: 3.4385\n",
            "Epoch 1/1, Batch 16620/98059, Loss: 3.1797\n",
            "Epoch 1/1, Batch 16640/98059, Loss: 3.4646\n",
            "Epoch 1/1, Batch 16660/98059, Loss: 3.5234\n",
            "Epoch 1/1, Batch 16680/98059, Loss: 3.1919\n",
            "Epoch 1/1, Batch 16700/98059, Loss: 3.2789\n",
            "Epoch 1/1, Batch 16720/98059, Loss: 3.4816\n",
            "Epoch 1/1, Batch 16740/98059, Loss: 3.4216\n",
            "Epoch 1/1, Batch 16760/98059, Loss: 3.2017\n",
            "Epoch 1/1, Batch 16780/98059, Loss: 3.2831\n",
            "Epoch 1/1, Batch 16800/98059, Loss: 3.7527\n",
            "Epoch 1/1, Batch 16820/98059, Loss: 3.4523\n",
            "Epoch 1/1, Batch 16840/98059, Loss: 3.3280\n",
            "Epoch 1/1, Batch 16860/98059, Loss: 3.1703\n",
            "Epoch 1/1, Batch 16880/98059, Loss: 3.5483\n",
            "Epoch 1/1, Batch 16900/98059, Loss: 3.2122\n",
            "Epoch 1/1, Batch 16920/98059, Loss: 3.0443\n",
            "Epoch 1/1, Batch 16940/98059, Loss: 3.4551\n",
            "Epoch 1/1, Batch 16960/98059, Loss: 3.1143\n",
            "Epoch 1/1, Batch 16980/98059, Loss: 3.7085\n",
            "Epoch 1/1, Batch 17000/98059, Loss: 3.4647\n",
            "Epoch 1/1, Batch 17020/98059, Loss: 3.0314\n",
            "Epoch 1/1, Batch 17040/98059, Loss: 2.9967\n",
            "Epoch 1/1, Batch 17060/98059, Loss: 3.4343\n",
            "Epoch 1/1, Batch 17080/98059, Loss: 3.1218\n",
            "Epoch 1/1, Batch 17100/98059, Loss: 2.8083\n",
            "Epoch 1/1, Batch 17120/98059, Loss: 3.4669\n",
            "Epoch 1/1, Batch 17140/98059, Loss: 3.5264\n",
            "Epoch 1/1, Batch 17160/98059, Loss: 3.2580\n",
            "Epoch 1/1, Batch 17180/98059, Loss: 3.6941\n",
            "Epoch 1/1, Batch 17200/98059, Loss: 3.3502\n",
            "Epoch 1/1, Batch 17220/98059, Loss: 2.9303\n",
            "Epoch 1/1, Batch 17240/98059, Loss: 3.4561\n",
            "Epoch 1/1, Batch 17260/98059, Loss: 3.3199\n",
            "Epoch 1/1, Batch 17280/98059, Loss: 3.3675\n",
            "Epoch 1/1, Batch 17300/98059, Loss: 3.4818\n",
            "Epoch 1/1, Batch 17320/98059, Loss: 3.2750\n",
            "Epoch 1/1, Batch 17340/98059, Loss: 3.3390\n",
            "Epoch 1/1, Batch 17360/98059, Loss: 2.8074\n",
            "Epoch 1/1, Batch 17380/98059, Loss: 3.2823\n",
            "Epoch 1/1, Batch 17400/98059, Loss: 3.1579\n",
            "Epoch 1/1, Batch 17420/98059, Loss: 3.0732\n",
            "Epoch 1/1, Batch 17440/98059, Loss: 3.1458\n",
            "Epoch 1/1, Batch 17460/98059, Loss: 3.3346\n",
            "Epoch 1/1, Batch 17480/98059, Loss: 2.8633\n",
            "Epoch 1/1, Batch 17500/98059, Loss: 3.4025\n",
            "Epoch 1/1, Batch 17520/98059, Loss: 3.4813\n",
            "Epoch 1/1, Batch 17540/98059, Loss: 3.1346\n",
            "Epoch 1/1, Batch 17560/98059, Loss: 3.4065\n",
            "Epoch 1/1, Batch 17580/98059, Loss: 3.1464\n",
            "Epoch 1/1, Batch 17600/98059, Loss: 3.1842\n",
            "Epoch 1/1, Batch 17620/98059, Loss: 3.3222\n",
            "Epoch 1/1, Batch 17640/98059, Loss: 3.3693\n",
            "Epoch 1/1, Batch 17660/98059, Loss: 3.1351\n",
            "Epoch 1/1, Batch 17680/98059, Loss: 3.1657\n",
            "Epoch 1/1, Batch 17700/98059, Loss: 3.2391\n",
            "Epoch 1/1, Batch 17720/98059, Loss: 3.3967\n",
            "Epoch 1/1, Batch 17740/98059, Loss: 3.2189\n",
            "Epoch 1/1, Batch 17760/98059, Loss: 3.3982\n",
            "Epoch 1/1, Batch 17780/98059, Loss: 3.2578\n",
            "Epoch 1/1, Batch 17800/98059, Loss: 3.3736\n",
            "Epoch 1/1, Batch 17820/98059, Loss: 2.7106\n",
            "Epoch 1/1, Batch 17840/98059, Loss: 2.9983\n",
            "Epoch 1/1, Batch 17860/98059, Loss: 2.8556\n",
            "Epoch 1/1, Batch 17880/98059, Loss: 2.9465\n",
            "Epoch 1/1, Batch 17900/98059, Loss: 3.1965\n",
            "Epoch 1/1, Batch 17920/98059, Loss: 3.0868\n",
            "Epoch 1/1, Batch 17940/98059, Loss: 3.1115\n",
            "Epoch 1/1, Batch 17960/98059, Loss: 3.5403\n",
            "Epoch 1/1, Batch 17980/98059, Loss: 3.0778\n",
            "Epoch 1/1, Batch 18000/98059, Loss: 3.6557\n",
            "Epoch 1/1, Batch 18020/98059, Loss: 3.5051\n",
            "Epoch 1/1, Batch 18040/98059, Loss: 2.9630\n",
            "Epoch 1/1, Batch 18060/98059, Loss: 2.9195\n",
            "Epoch 1/1, Batch 18080/98059, Loss: 3.5638\n",
            "Epoch 1/1, Batch 18100/98059, Loss: 3.5500\n",
            "Epoch 1/1, Batch 18120/98059, Loss: 3.0236\n",
            "Epoch 1/1, Batch 18140/98059, Loss: 3.2954\n",
            "Epoch 1/1, Batch 18160/98059, Loss: 3.5290\n",
            "Epoch 1/1, Batch 18180/98059, Loss: 3.1992\n",
            "Epoch 1/1, Batch 18200/98059, Loss: 2.8447\n",
            "Epoch 1/1, Batch 18220/98059, Loss: 3.0765\n",
            "Epoch 1/1, Batch 18240/98059, Loss: 3.2085\n",
            "Epoch 1/1, Batch 18260/98059, Loss: 3.2772\n",
            "Epoch 1/1, Batch 18280/98059, Loss: 3.4279\n",
            "Epoch 1/1, Batch 18300/98059, Loss: 3.1138\n",
            "Epoch 1/1, Batch 18320/98059, Loss: 3.1500\n",
            "Epoch 1/1, Batch 18340/98059, Loss: 3.4181\n",
            "Epoch 1/1, Batch 18360/98059, Loss: 3.1671\n",
            "Epoch 1/1, Batch 18380/98059, Loss: 3.5094\n",
            "Epoch 1/1, Batch 18400/98059, Loss: 3.5033\n",
            "Epoch 1/1, Batch 18420/98059, Loss: 3.3356\n",
            "Epoch 1/1, Batch 18440/98059, Loss: 3.1458\n",
            "Epoch 1/1, Batch 18460/98059, Loss: 3.4891\n",
            "Epoch 1/1, Batch 18480/98059, Loss: 3.0332\n",
            "Epoch 1/1, Batch 18500/98059, Loss: 3.4215\n",
            "Epoch 1/1, Batch 18520/98059, Loss: 3.3015\n",
            "Epoch 1/1, Batch 18540/98059, Loss: 3.2319\n",
            "Epoch 1/1, Batch 18560/98059, Loss: 3.4869\n",
            "Epoch 1/1, Batch 18580/98059, Loss: 2.9557\n",
            "Epoch 1/1, Batch 18600/98059, Loss: 3.3116\n",
            "Epoch 1/1, Batch 18620/98059, Loss: 3.3117\n",
            "Epoch 1/1, Batch 18640/98059, Loss: 3.3306\n",
            "Epoch 1/1, Batch 18660/98059, Loss: 3.0446\n",
            "Epoch 1/1, Batch 18680/98059, Loss: 3.2913\n",
            "Epoch 1/1, Batch 18700/98059, Loss: 3.2798\n",
            "Epoch 1/1, Batch 18720/98059, Loss: 3.0382\n",
            "Epoch 1/1, Batch 18740/98059, Loss: 3.4082\n",
            "Epoch 1/1, Batch 18760/98059, Loss: 3.0738\n",
            "Epoch 1/1, Batch 18780/98059, Loss: 3.2400\n",
            "Epoch 1/1, Batch 18800/98059, Loss: 3.2821\n",
            "Epoch 1/1, Batch 18820/98059, Loss: 3.0965\n",
            "Epoch 1/1, Batch 18840/98059, Loss: 3.4517\n",
            "Epoch 1/1, Batch 18860/98059, Loss: 3.5411\n",
            "Epoch 1/1, Batch 18880/98059, Loss: 3.1285\n",
            "Epoch 1/1, Batch 18900/98059, Loss: 3.1534\n",
            "Epoch 1/1, Batch 18920/98059, Loss: 3.0551\n",
            "Epoch 1/1, Batch 18940/98059, Loss: 3.1884\n",
            "Epoch 1/1, Batch 18960/98059, Loss: 3.1359\n",
            "Epoch 1/1, Batch 18980/98059, Loss: 3.0969\n",
            "Epoch 1/1, Batch 19000/98059, Loss: 3.1711\n",
            "Epoch 1/1, Batch 19020/98059, Loss: 3.0438\n",
            "Epoch 1/1, Batch 19040/98059, Loss: 2.9973\n",
            "Epoch 1/1, Batch 19060/98059, Loss: 3.4980\n",
            "Epoch 1/1, Batch 19080/98059, Loss: 3.3626\n",
            "Epoch 1/1, Batch 19100/98059, Loss: 3.1502\n",
            "Epoch 1/1, Batch 19120/98059, Loss: 3.2373\n",
            "Epoch 1/1, Batch 19140/98059, Loss: 3.2914\n",
            "Epoch 1/1, Batch 19160/98059, Loss: 3.2261\n",
            "Epoch 1/1, Batch 19180/98059, Loss: 3.3071\n",
            "Epoch 1/1, Batch 19200/98059, Loss: 3.1852\n",
            "Epoch 1/1, Batch 19220/98059, Loss: 3.3551\n",
            "Epoch 1/1, Batch 19240/98059, Loss: 3.4301\n",
            "Epoch 1/1, Batch 19260/98059, Loss: 3.3345\n",
            "Epoch 1/1, Batch 19280/98059, Loss: 3.0218\n",
            "Epoch 1/1, Batch 19300/98059, Loss: 3.0653\n",
            "Epoch 1/1, Batch 19320/98059, Loss: 3.2593\n",
            "Epoch 1/1, Batch 19340/98059, Loss: 3.2202\n",
            "Epoch 1/1, Batch 19360/98059, Loss: 3.4539\n",
            "Epoch 1/1, Batch 19380/98059, Loss: 3.2541\n",
            "Epoch 1/1, Batch 19400/98059, Loss: 3.2217\n",
            "Epoch 1/1, Batch 19420/98059, Loss: 3.3285\n",
            "Epoch 1/1, Batch 19440/98059, Loss: 3.1397\n",
            "Epoch 1/1, Batch 19460/98059, Loss: 2.8066\n",
            "Epoch 1/1, Batch 19480/98059, Loss: 3.0359\n",
            "Epoch 1/1, Batch 19500/98059, Loss: 3.5986\n",
            "Epoch 1/1, Batch 19520/98059, Loss: 3.3088\n",
            "Epoch 1/1, Batch 19540/98059, Loss: 2.9482\n",
            "Epoch 1/1, Batch 19560/98059, Loss: 2.7499\n",
            "Epoch 1/1, Batch 19580/98059, Loss: 3.7199\n",
            "Epoch 1/1, Batch 19600/98059, Loss: 3.3357\n",
            "Epoch 1/1, Batch 19620/98059, Loss: 3.2334\n",
            "Epoch 1/1, Batch 19640/98059, Loss: 3.5865\n",
            "Epoch 1/1, Batch 19660/98059, Loss: 3.3816\n",
            "Epoch 1/1, Batch 19680/98059, Loss: 3.1977\n",
            "Epoch 1/1, Batch 19700/98059, Loss: 3.1413\n",
            "Epoch 1/1, Batch 19720/98059, Loss: 3.4216\n",
            "Epoch 1/1, Batch 19740/98059, Loss: 3.2118\n",
            "Epoch 1/1, Batch 19760/98059, Loss: 3.1626\n",
            "Epoch 1/1, Batch 19780/98059, Loss: 3.0272\n",
            "Epoch 1/1, Batch 19800/98059, Loss: 3.0589\n",
            "Epoch 1/1, Batch 19820/98059, Loss: 3.1655\n",
            "Epoch 1/1, Batch 19840/98059, Loss: 3.1012\n",
            "Epoch 1/1, Batch 19860/98059, Loss: 3.3170\n",
            "Epoch 1/1, Batch 19880/98059, Loss: 3.1188\n",
            "Epoch 1/1, Batch 19900/98059, Loss: 2.8653\n",
            "Epoch 1/1, Batch 19920/98059, Loss: 3.2885\n",
            "Epoch 1/1, Batch 19940/98059, Loss: 3.2033\n",
            "Epoch 1/1, Batch 19960/98059, Loss: 3.2160\n",
            "Epoch 1/1, Batch 19980/98059, Loss: 3.0592\n",
            "Epoch 1/1, Batch 20000/98059, Loss: 3.4536\n",
            "Epoch 1/1, Batch 20020/98059, Loss: 3.0105\n",
            "Epoch 1/1, Batch 20040/98059, Loss: 3.1082\n",
            "Epoch 1/1, Batch 20060/98059, Loss: 2.9949\n",
            "Epoch 1/1, Batch 20080/98059, Loss: 3.2009\n",
            "Epoch 1/1, Batch 20100/98059, Loss: 3.3020\n",
            "Epoch 1/1, Batch 20120/98059, Loss: 3.3030\n",
            "Epoch 1/1, Batch 20140/98059, Loss: 2.8433\n",
            "Epoch 1/1, Batch 20160/98059, Loss: 3.2054\n",
            "Epoch 1/1, Batch 20180/98059, Loss: 3.3538\n",
            "Epoch 1/1, Batch 20200/98059, Loss: 3.2245\n",
            "Epoch 1/1, Batch 20220/98059, Loss: 3.4115\n",
            "Epoch 1/1, Batch 20240/98059, Loss: 3.1314\n",
            "Epoch 1/1, Batch 20260/98059, Loss: 3.3061\n",
            "Epoch 1/1, Batch 20280/98059, Loss: 3.6112\n",
            "Epoch 1/1, Batch 20300/98059, Loss: 3.2118\n",
            "Epoch 1/1, Batch 20320/98059, Loss: 3.4623\n",
            "Epoch 1/1, Batch 20340/98059, Loss: 3.3073\n",
            "Epoch 1/1, Batch 20360/98059, Loss: 3.1699\n",
            "Epoch 1/1, Batch 20380/98059, Loss: 3.5402\n",
            "Epoch 1/1, Batch 20400/98059, Loss: 3.3419\n",
            "Epoch 1/1, Batch 20420/98059, Loss: 3.3787\n",
            "Epoch 1/1, Batch 20440/98059, Loss: 3.4246\n",
            "Epoch 1/1, Batch 20460/98059, Loss: 3.1383\n",
            "Epoch 1/1, Batch 20480/98059, Loss: 3.1528\n",
            "Epoch 1/1, Batch 20500/98059, Loss: 3.1285\n",
            "Epoch 1/1, Batch 20520/98059, Loss: 2.9602\n",
            "Epoch 1/1, Batch 20540/98059, Loss: 3.4776\n",
            "Epoch 1/1, Batch 20560/98059, Loss: 3.1642\n",
            "Epoch 1/1, Batch 20580/98059, Loss: 3.1933\n",
            "Epoch 1/1, Batch 20600/98059, Loss: 2.9919\n",
            "Epoch 1/1, Batch 20620/98059, Loss: 3.0063\n",
            "Epoch 1/1, Batch 20640/98059, Loss: 3.1937\n",
            "Epoch 1/1, Batch 20660/98059, Loss: 3.3118\n",
            "Epoch 1/1, Batch 20680/98059, Loss: 3.4068\n",
            "Epoch 1/1, Batch 20700/98059, Loss: 3.5145\n",
            "Epoch 1/1, Batch 20720/98059, Loss: 3.2909\n",
            "Epoch 1/1, Batch 20740/98059, Loss: 3.4108\n",
            "Epoch 1/1, Batch 20760/98059, Loss: 2.8511\n",
            "Epoch 1/1, Batch 20780/98059, Loss: 3.2382\n",
            "Epoch 1/1, Batch 20800/98059, Loss: 3.4008\n",
            "Epoch 1/1, Batch 20820/98059, Loss: 3.3851\n",
            "Epoch 1/1, Batch 20840/98059, Loss: 3.3736\n",
            "Epoch 1/1, Batch 20860/98059, Loss: 3.2537\n",
            "Epoch 1/1, Batch 20880/98059, Loss: 3.2650\n",
            "Epoch 1/1, Batch 20900/98059, Loss: 2.7359\n",
            "Epoch 1/1, Batch 20920/98059, Loss: 2.9983\n",
            "Epoch 1/1, Batch 20940/98059, Loss: 3.2325\n",
            "Epoch 1/1, Batch 20960/98059, Loss: 2.9147\n",
            "Epoch 1/1, Batch 20980/98059, Loss: 2.9577\n",
            "Epoch 1/1, Batch 21000/98059, Loss: 2.9834\n",
            "Epoch 1/1, Batch 21020/98059, Loss: 3.1393\n",
            "Epoch 1/1, Batch 21040/98059, Loss: 3.3446\n",
            "Epoch 1/1, Batch 21060/98059, Loss: 3.4100\n",
            "Epoch 1/1, Batch 21080/98059, Loss: 2.8919\n",
            "Epoch 1/1, Batch 21100/98059, Loss: 3.4145\n",
            "Epoch 1/1, Batch 21120/98059, Loss: 3.5930\n",
            "Epoch 1/1, Batch 21140/98059, Loss: 3.5010\n",
            "Epoch 1/1, Batch 21160/98059, Loss: 3.2463\n",
            "Epoch 1/1, Batch 21180/98059, Loss: 3.2101\n",
            "Epoch 1/1, Batch 21200/98059, Loss: 3.5031\n",
            "Epoch 1/1, Batch 21220/98059, Loss: 3.3093\n",
            "Epoch 1/1, Batch 21240/98059, Loss: 3.2190\n",
            "Epoch 1/1, Batch 21260/98059, Loss: 3.2462\n",
            "Epoch 1/1, Batch 21280/98059, Loss: 3.5869\n",
            "Epoch 1/1, Batch 21300/98059, Loss: 2.9282\n",
            "Epoch 1/1, Batch 21320/98059, Loss: 3.2153\n",
            "Epoch 1/1, Batch 21340/98059, Loss: 3.1529\n",
            "Epoch 1/1, Batch 21360/98059, Loss: 3.1507\n",
            "Epoch 1/1, Batch 21380/98059, Loss: 3.3975\n",
            "Epoch 1/1, Batch 21400/98059, Loss: 3.3949\n",
            "Epoch 1/1, Batch 21420/98059, Loss: 3.3737\n",
            "Epoch 1/1, Batch 21440/98059, Loss: 2.8525\n",
            "Epoch 1/1, Batch 21460/98059, Loss: 3.3151\n",
            "Epoch 1/1, Batch 21480/98059, Loss: 3.4561\n",
            "Epoch 1/1, Batch 21500/98059, Loss: 3.0223\n",
            "Epoch 1/1, Batch 21520/98059, Loss: 3.2644\n",
            "Epoch 1/1, Batch 21540/98059, Loss: 3.1752\n",
            "Epoch 1/1, Batch 21560/98059, Loss: 3.2804\n",
            "Epoch 1/1, Batch 21580/98059, Loss: 2.9754\n",
            "Epoch 1/1, Batch 21600/98059, Loss: 3.0983\n",
            "Epoch 1/1, Batch 21620/98059, Loss: 3.3300\n",
            "Epoch 1/1, Batch 21640/98059, Loss: 3.3522\n",
            "Epoch 1/1, Batch 21660/98059, Loss: 3.2256\n",
            "Epoch 1/1, Batch 21680/98059, Loss: 3.2760\n",
            "Epoch 1/1, Batch 21700/98059, Loss: 3.5042\n",
            "Epoch 1/1, Batch 21720/98059, Loss: 3.5421\n",
            "Epoch 1/1, Batch 21740/98059, Loss: 3.5350\n",
            "Epoch 1/1, Batch 21760/98059, Loss: 3.0152\n",
            "Epoch 1/1, Batch 21780/98059, Loss: 3.5326\n",
            "Epoch 1/1, Batch 21800/98059, Loss: 3.4852\n",
            "Epoch 1/1, Batch 21820/98059, Loss: 3.3539\n",
            "Epoch 1/1, Batch 21840/98059, Loss: 3.2551\n",
            "Epoch 1/1, Batch 21860/98059, Loss: 3.3682\n",
            "Epoch 1/1, Batch 21880/98059, Loss: 3.6852\n",
            "Epoch 1/1, Batch 21900/98059, Loss: 3.2046\n",
            "Epoch 1/1, Batch 21920/98059, Loss: 3.1504\n",
            "Epoch 1/1, Batch 21940/98059, Loss: 3.2300\n",
            "Epoch 1/1, Batch 21960/98059, Loss: 3.3774\n",
            "Epoch 1/1, Batch 21980/98059, Loss: 3.0451\n",
            "Epoch 1/1, Batch 22000/98059, Loss: 2.9324\n",
            "Epoch 1/1, Batch 22020/98059, Loss: 3.0823\n",
            "Epoch 1/1, Batch 22040/98059, Loss: 3.4673\n",
            "Epoch 1/1, Batch 22060/98059, Loss: 3.5584\n",
            "Epoch 1/1, Batch 22080/98059, Loss: 3.3120\n",
            "Epoch 1/1, Batch 22100/98059, Loss: 3.3132\n",
            "Epoch 1/1, Batch 22120/98059, Loss: 3.0694\n",
            "Epoch 1/1, Batch 22140/98059, Loss: 2.9589\n",
            "Epoch 1/1, Batch 22160/98059, Loss: 3.0504\n",
            "Epoch 1/1, Batch 22180/98059, Loss: 3.2848\n",
            "Epoch 1/1, Batch 22200/98059, Loss: 3.0432\n",
            "Epoch 1/1, Batch 22220/98059, Loss: 3.2789\n",
            "Epoch 1/1, Batch 22240/98059, Loss: 3.5315\n",
            "Epoch 1/1, Batch 22260/98059, Loss: 3.0729\n",
            "Epoch 1/1, Batch 22280/98059, Loss: 3.0685\n",
            "Epoch 1/1, Batch 22300/98059, Loss: 3.3837\n",
            "Epoch 1/1, Batch 22320/98059, Loss: 3.2736\n",
            "Epoch 1/1, Batch 22340/98059, Loss: 3.4284\n",
            "Epoch 1/1, Batch 22360/98059, Loss: 3.4047\n",
            "Epoch 1/1, Batch 22380/98059, Loss: 3.3573\n",
            "Epoch 1/1, Batch 22400/98059, Loss: 3.0603\n",
            "Epoch 1/1, Batch 22420/98059, Loss: 3.3702\n",
            "Epoch 1/1, Batch 22440/98059, Loss: 3.4295\n",
            "Epoch 1/1, Batch 22460/98059, Loss: 3.4690\n",
            "Epoch 1/1, Batch 22480/98059, Loss: 2.7131\n",
            "Epoch 1/1, Batch 22500/98059, Loss: 3.4404\n",
            "Epoch 1/1, Batch 22520/98059, Loss: 2.8759\n",
            "Epoch 1/1, Batch 22540/98059, Loss: 3.4298\n",
            "Epoch 1/1, Batch 22560/98059, Loss: 3.2268\n",
            "Epoch 1/1, Batch 22580/98059, Loss: 3.0604\n",
            "Epoch 1/1, Batch 22600/98059, Loss: 3.3471\n",
            "Epoch 1/1, Batch 22620/98059, Loss: 3.2863\n",
            "Epoch 1/1, Batch 22640/98059, Loss: 2.9416\n",
            "Epoch 1/1, Batch 22660/98059, Loss: 3.1027\n",
            "Epoch 1/1, Batch 22680/98059, Loss: 2.9739\n",
            "Epoch 1/1, Batch 22700/98059, Loss: 3.1369\n",
            "Epoch 1/1, Batch 22720/98059, Loss: 3.0982\n",
            "Epoch 1/1, Batch 22740/98059, Loss: 3.2453\n",
            "Epoch 1/1, Batch 22760/98059, Loss: 3.0829\n",
            "Epoch 1/1, Batch 22780/98059, Loss: 3.7065\n",
            "Epoch 1/1, Batch 22800/98059, Loss: 2.8247\n",
            "Epoch 1/1, Batch 22820/98059, Loss: 3.0756\n",
            "Epoch 1/1, Batch 22840/98059, Loss: 3.3224\n",
            "Epoch 1/1, Batch 22860/98059, Loss: 3.0865\n",
            "Epoch 1/1, Batch 22880/98059, Loss: 3.3226\n",
            "Epoch 1/1, Batch 22900/98059, Loss: 3.1928\n",
            "Epoch 1/1, Batch 22920/98059, Loss: 3.0097\n",
            "Epoch 1/1, Batch 22940/98059, Loss: 2.9951\n",
            "Epoch 1/1, Batch 22960/98059, Loss: 2.9991\n",
            "Epoch 1/1, Batch 22980/98059, Loss: 3.0276\n",
            "Epoch 1/1, Batch 23000/98059, Loss: 3.0857\n",
            "Epoch 1/1, Batch 23020/98059, Loss: 3.3033\n",
            "Epoch 1/1, Batch 23040/98059, Loss: 3.1026\n",
            "Epoch 1/1, Batch 23060/98059, Loss: 2.9670\n",
            "Epoch 1/1, Batch 23080/98059, Loss: 3.0747\n",
            "Epoch 1/1, Batch 23100/98059, Loss: 3.0807\n",
            "Epoch 1/1, Batch 23120/98059, Loss: 3.0229\n",
            "Epoch 1/1, Batch 23140/98059, Loss: 3.2366\n",
            "Epoch 1/1, Batch 23160/98059, Loss: 3.2426\n",
            "Epoch 1/1, Batch 23180/98059, Loss: 3.3607\n",
            "Epoch 1/1, Batch 23200/98059, Loss: 2.9589\n",
            "Epoch 1/1, Batch 23220/98059, Loss: 3.1816\n",
            "Epoch 1/1, Batch 23240/98059, Loss: 3.1308\n",
            "Epoch 1/1, Batch 23260/98059, Loss: 3.4221\n",
            "Epoch 1/1, Batch 23280/98059, Loss: 3.3372\n",
            "Epoch 1/1, Batch 23300/98059, Loss: 3.2395\n",
            "Epoch 1/1, Batch 23320/98059, Loss: 3.2318\n",
            "Epoch 1/1, Batch 23340/98059, Loss: 2.8903\n",
            "Epoch 1/1, Batch 23360/98059, Loss: 3.3013\n",
            "Epoch 1/1, Batch 23380/98059, Loss: 3.6028\n",
            "Epoch 1/1, Batch 23400/98059, Loss: 3.0438\n",
            "Epoch 1/1, Batch 23420/98059, Loss: 3.5808\n",
            "Epoch 1/1, Batch 23440/98059, Loss: 3.1837\n",
            "Epoch 1/1, Batch 23460/98059, Loss: 3.4072\n",
            "Epoch 1/1, Batch 23480/98059, Loss: 3.1535\n",
            "Epoch 1/1, Batch 23500/98059, Loss: 3.4850\n",
            "Epoch 1/1, Batch 23520/98059, Loss: 3.0410\n",
            "Epoch 1/1, Batch 23540/98059, Loss: 3.0533\n",
            "Epoch 1/1, Batch 23560/98059, Loss: 3.4748\n",
            "Epoch 1/1, Batch 23580/98059, Loss: 3.0519\n",
            "Epoch 1/1, Batch 23600/98059, Loss: 3.3056\n",
            "Epoch 1/1, Batch 23620/98059, Loss: 3.2775\n",
            "Epoch 1/1, Batch 23640/98059, Loss: 3.3985\n",
            "Epoch 1/1, Batch 23660/98059, Loss: 3.2724\n",
            "Epoch 1/1, Batch 23680/98059, Loss: 3.3543\n",
            "Epoch 1/1, Batch 23700/98059, Loss: 3.0412\n",
            "Epoch 1/1, Batch 23720/98059, Loss: 3.2634\n",
            "Epoch 1/1, Batch 23740/98059, Loss: 3.0291\n",
            "Epoch 1/1, Batch 23760/98059, Loss: 3.1781\n",
            "Epoch 1/1, Batch 23780/98059, Loss: 3.3279\n",
            "Epoch 1/1, Batch 23800/98059, Loss: 3.2824\n",
            "Epoch 1/1, Batch 23820/98059, Loss: 2.6512\n",
            "Epoch 1/1, Batch 23840/98059, Loss: 3.0081\n",
            "Epoch 1/1, Batch 23860/98059, Loss: 2.9741\n",
            "Epoch 1/1, Batch 23880/98059, Loss: 3.3091\n",
            "Epoch 1/1, Batch 23900/98059, Loss: 3.1725\n",
            "Epoch 1/1, Batch 23920/98059, Loss: 3.1115\n",
            "Epoch 1/1, Batch 23940/98059, Loss: 3.4033\n",
            "Epoch 1/1, Batch 23960/98059, Loss: 3.1478\n",
            "Epoch 1/1, Batch 23980/98059, Loss: 3.4766\n",
            "Epoch 1/1, Batch 24000/98059, Loss: 3.3275\n",
            "Epoch 1/1, Batch 24020/98059, Loss: 3.1603\n",
            "Epoch 1/1, Batch 24040/98059, Loss: 3.0840\n",
            "Epoch 1/1, Batch 24060/98059, Loss: 2.8926\n",
            "Epoch 1/1, Batch 24080/98059, Loss: 2.9919\n",
            "Epoch 1/1, Batch 24100/98059, Loss: 3.2050\n",
            "Epoch 1/1, Batch 24120/98059, Loss: 3.5990\n",
            "Epoch 1/1, Batch 24140/98059, Loss: 3.1986\n",
            "Epoch 1/1, Batch 24160/98059, Loss: 3.1867\n",
            "Epoch 1/1, Batch 24180/98059, Loss: 3.0470\n",
            "Epoch 1/1, Batch 24200/98059, Loss: 2.9523\n",
            "Epoch 1/1, Batch 24220/98059, Loss: 2.9413\n",
            "Epoch 1/1, Batch 24240/98059, Loss: 3.0791\n",
            "Epoch 1/1, Batch 24260/98059, Loss: 3.0345\n",
            "Epoch 1/1, Batch 24280/98059, Loss: 3.3476\n",
            "Epoch 1/1, Batch 24300/98059, Loss: 3.1924\n",
            "Epoch 1/1, Batch 24320/98059, Loss: 3.1237\n",
            "Epoch 1/1, Batch 24340/98059, Loss: 3.3023\n",
            "Epoch 1/1, Batch 24360/98059, Loss: 3.2498\n",
            "Epoch 1/1, Batch 24380/98059, Loss: 3.0826\n",
            "Epoch 1/1, Batch 24400/98059, Loss: 2.8631\n",
            "Epoch 1/1, Batch 24420/98059, Loss: 3.1458\n",
            "Epoch 1/1, Batch 24440/98059, Loss: 3.0211\n",
            "Epoch 1/1, Batch 24460/98059, Loss: 3.0854\n",
            "Epoch 1/1, Batch 24480/98059, Loss: 3.0464\n",
            "Epoch 1/1, Batch 24500/98059, Loss: 3.2098\n",
            "Epoch 1/1, Batch 24520/98059, Loss: 3.0222\n",
            "Epoch 1/1, Batch 24540/98059, Loss: 3.3885\n",
            "Epoch 1/1, Batch 24560/98059, Loss: 3.1296\n",
            "Epoch 1/1, Batch 24580/98059, Loss: 3.0416\n",
            "Epoch 1/1, Batch 24600/98059, Loss: 3.5686\n",
            "Epoch 1/1, Batch 24620/98059, Loss: 3.1537\n",
            "Epoch 1/1, Batch 24640/98059, Loss: 3.0037\n",
            "Epoch 1/1, Batch 24660/98059, Loss: 3.3421\n",
            "Epoch 1/1, Batch 24680/98059, Loss: 3.1879\n",
            "Epoch 1/1, Batch 24700/98059, Loss: 3.0707\n",
            "Epoch 1/1, Batch 24720/98059, Loss: 3.3221\n",
            "Epoch 1/1, Batch 24740/98059, Loss: 3.2725\n",
            "Epoch 1/1, Batch 24760/98059, Loss: 3.0296\n",
            "Epoch 1/1, Batch 24780/98059, Loss: 3.1872\n",
            "Epoch 1/1, Batch 24800/98059, Loss: 3.1262\n",
            "Epoch 1/1, Batch 24820/98059, Loss: 2.9995\n",
            "Epoch 1/1, Batch 24840/98059, Loss: 2.9954\n",
            "Epoch 1/1, Batch 24860/98059, Loss: 2.9854\n",
            "Epoch 1/1, Batch 24880/98059, Loss: 3.1549\n",
            "Epoch 1/1, Batch 24900/98059, Loss: 2.9774\n",
            "Epoch 1/1, Batch 24920/98059, Loss: 3.0919\n",
            "Epoch 1/1, Batch 24940/98059, Loss: 3.2370\n",
            "Epoch 1/1, Batch 24960/98059, Loss: 3.3753\n",
            "Epoch 1/1, Batch 24980/98059, Loss: 3.0232\n",
            "Epoch 1/1, Batch 25000/98059, Loss: 3.0060\n",
            "Epoch 1/1, Batch 25020/98059, Loss: 3.1436\n",
            "Epoch 1/1, Batch 25040/98059, Loss: 3.5153\n",
            "Epoch 1/1, Batch 25060/98059, Loss: 3.1577\n",
            "Epoch 1/1, Batch 25080/98059, Loss: 3.7285\n",
            "Epoch 1/1, Batch 25100/98059, Loss: 3.3048\n",
            "Epoch 1/1, Batch 25120/98059, Loss: 3.2161\n",
            "Epoch 1/1, Batch 25140/98059, Loss: 3.2349\n",
            "Epoch 1/1, Batch 25160/98059, Loss: 3.1099\n",
            "Epoch 1/1, Batch 25180/98059, Loss: 3.3868\n",
            "Epoch 1/1, Batch 25200/98059, Loss: 2.8556\n",
            "Epoch 1/1, Batch 25220/98059, Loss: 2.7855\n",
            "Epoch 1/1, Batch 25240/98059, Loss: 2.9393\n",
            "Epoch 1/1, Batch 25260/98059, Loss: 3.0088\n",
            "Epoch 1/1, Batch 25280/98059, Loss: 3.2230\n",
            "Epoch 1/1, Batch 25300/98059, Loss: 2.7367\n",
            "Epoch 1/1, Batch 25320/98059, Loss: 3.0484\n",
            "Epoch 1/1, Batch 25340/98059, Loss: 2.9795\n",
            "Epoch 1/1, Batch 25360/98059, Loss: 2.7638\n",
            "Epoch 1/1, Batch 25380/98059, Loss: 3.2492\n",
            "Epoch 1/1, Batch 25400/98059, Loss: 2.9985\n",
            "Epoch 1/1, Batch 25420/98059, Loss: 3.1792\n",
            "Epoch 1/1, Batch 25440/98059, Loss: 2.8683\n",
            "Epoch 1/1, Batch 25460/98059, Loss: 3.3687\n",
            "Epoch 1/1, Batch 25480/98059, Loss: 2.7139\n",
            "Epoch 1/1, Batch 25500/98059, Loss: 3.3183\n",
            "Epoch 1/1, Batch 25520/98059, Loss: 3.2148\n",
            "Epoch 1/1, Batch 25540/98059, Loss: 3.1666\n",
            "Epoch 1/1, Batch 25560/98059, Loss: 3.1609\n",
            "Epoch 1/1, Batch 25580/98059, Loss: 3.1120\n",
            "Epoch 1/1, Batch 25600/98059, Loss: 2.9354\n",
            "Epoch 1/1, Batch 25620/98059, Loss: 3.0194\n",
            "Epoch 1/1, Batch 25640/98059, Loss: 3.1289\n",
            "Epoch 1/1, Batch 25660/98059, Loss: 3.1466\n",
            "Epoch 1/1, Batch 25680/98059, Loss: 3.2489\n",
            "Epoch 1/1, Batch 25700/98059, Loss: 3.1350\n",
            "Epoch 1/1, Batch 25720/98059, Loss: 3.1919\n",
            "Epoch 1/1, Batch 25740/98059, Loss: 3.2636\n",
            "Epoch 1/1, Batch 25760/98059, Loss: 3.4125\n",
            "Epoch 1/1, Batch 25780/98059, Loss: 2.9563\n",
            "Epoch 1/1, Batch 25800/98059, Loss: 3.6671\n",
            "Epoch 1/1, Batch 25820/98059, Loss: 3.4650\n",
            "Epoch 1/1, Batch 25840/98059, Loss: 3.3033\n",
            "Epoch 1/1, Batch 25860/98059, Loss: 2.8089\n",
            "Epoch 1/1, Batch 25880/98059, Loss: 3.5249\n",
            "Epoch 1/1, Batch 25900/98059, Loss: 3.5871\n",
            "Epoch 1/1, Batch 25920/98059, Loss: 3.4352\n",
            "Epoch 1/1, Batch 25940/98059, Loss: 3.3462\n",
            "Epoch 1/1, Batch 25960/98059, Loss: 3.3736\n",
            "Epoch 1/1, Batch 25980/98059, Loss: 3.1661\n",
            "Epoch 1/1, Batch 26000/98059, Loss: 3.2076\n",
            "Epoch 1/1, Batch 26020/98059, Loss: 2.9232\n",
            "Epoch 1/1, Batch 26040/98059, Loss: 2.6451\n",
            "Epoch 1/1, Batch 26060/98059, Loss: 2.6096\n",
            "Epoch 1/1, Batch 26080/98059, Loss: 3.4399\n",
            "Epoch 1/1, Batch 26100/98059, Loss: 3.0941\n",
            "Epoch 1/1, Batch 26120/98059, Loss: 2.9906\n",
            "Epoch 1/1, Batch 26140/98059, Loss: 3.0829\n",
            "Epoch 1/1, Batch 26160/98059, Loss: 2.9458\n",
            "Epoch 1/1, Batch 26180/98059, Loss: 2.9004\n",
            "Epoch 1/1, Batch 26200/98059, Loss: 2.7638\n",
            "Epoch 1/1, Batch 26220/98059, Loss: 3.5272\n",
            "Epoch 1/1, Batch 26240/98059, Loss: 3.0308\n",
            "Epoch 1/1, Batch 26260/98059, Loss: 3.4534\n",
            "Epoch 1/1, Batch 26280/98059, Loss: 2.9275\n",
            "Epoch 1/1, Batch 26300/98059, Loss: 3.0609\n",
            "Epoch 1/1, Batch 26320/98059, Loss: 3.0480\n",
            "Epoch 1/1, Batch 26340/98059, Loss: 2.9367\n",
            "Epoch 1/1, Batch 26360/98059, Loss: 2.8900\n",
            "Epoch 1/1, Batch 26380/98059, Loss: 3.1482\n",
            "Epoch 1/1, Batch 26400/98059, Loss: 3.3284\n",
            "Epoch 1/1, Batch 26420/98059, Loss: 3.2958\n",
            "Epoch 1/1, Batch 26440/98059, Loss: 3.2294\n",
            "Epoch 1/1, Batch 26460/98059, Loss: 3.1705\n",
            "Epoch 1/1, Batch 26480/98059, Loss: 3.1509\n",
            "Epoch 1/1, Batch 26500/98059, Loss: 3.3783\n",
            "Epoch 1/1, Batch 26520/98059, Loss: 3.0542\n",
            "Epoch 1/1, Batch 26540/98059, Loss: 3.4941\n",
            "Epoch 1/1, Batch 26560/98059, Loss: 3.2866\n",
            "Epoch 1/1, Batch 26580/98059, Loss: 2.9908\n",
            "Epoch 1/1, Batch 26600/98059, Loss: 3.1957\n",
            "Epoch 1/1, Batch 26620/98059, Loss: 3.1477\n",
            "Epoch 1/1, Batch 26640/98059, Loss: 2.9463\n",
            "Epoch 1/1, Batch 26660/98059, Loss: 3.2418\n",
            "Epoch 1/1, Batch 26680/98059, Loss: 3.5305\n",
            "Epoch 1/1, Batch 26700/98059, Loss: 3.0774\n",
            "Epoch 1/1, Batch 26720/98059, Loss: 3.2877\n",
            "Epoch 1/1, Batch 26740/98059, Loss: 3.2952\n",
            "Epoch 1/1, Batch 26760/98059, Loss: 2.8576\n",
            "Epoch 1/1, Batch 26780/98059, Loss: 3.5414\n",
            "Epoch 1/1, Batch 26800/98059, Loss: 3.1517\n",
            "Epoch 1/1, Batch 26820/98059, Loss: 3.3170\n",
            "Epoch 1/1, Batch 26840/98059, Loss: 3.3563\n",
            "Epoch 1/1, Batch 26860/98059, Loss: 3.1214\n",
            "Epoch 1/1, Batch 26880/98059, Loss: 2.9395\n",
            "Epoch 1/1, Batch 26900/98059, Loss: 3.1973\n",
            "Epoch 1/1, Batch 26920/98059, Loss: 2.9397\n",
            "Epoch 1/1, Batch 26940/98059, Loss: 2.9212\n",
            "Epoch 1/1, Batch 26960/98059, Loss: 3.1219\n",
            "Epoch 1/1, Batch 26980/98059, Loss: 2.8905\n",
            "Epoch 1/1, Batch 27000/98059, Loss: 3.4021\n",
            "Epoch 1/1, Batch 27020/98059, Loss: 3.1903\n",
            "Epoch 1/1, Batch 27040/98059, Loss: 3.0402\n",
            "Epoch 1/1, Batch 27060/98059, Loss: 3.2325\n",
            "Epoch 1/1, Batch 27080/98059, Loss: 3.2156\n",
            "Epoch 1/1, Batch 27100/98059, Loss: 2.8686\n",
            "Epoch 1/1, Batch 27120/98059, Loss: 2.8770\n",
            "Epoch 1/1, Batch 27140/98059, Loss: 3.0577\n",
            "Epoch 1/1, Batch 27160/98059, Loss: 3.2931\n",
            "Epoch 1/1, Batch 27180/98059, Loss: 2.8385\n",
            "Epoch 1/1, Batch 27200/98059, Loss: 3.1993\n",
            "Epoch 1/1, Batch 27220/98059, Loss: 3.1349\n",
            "Epoch 1/1, Batch 27240/98059, Loss: 2.9983\n",
            "Epoch 1/1, Batch 27260/98059, Loss: 2.9653\n",
            "Epoch 1/1, Batch 27280/98059, Loss: 3.0931\n",
            "Epoch 1/1, Batch 27300/98059, Loss: 3.4081\n",
            "Epoch 1/1, Batch 27320/98059, Loss: 3.0939\n",
            "Epoch 1/1, Batch 27340/98059, Loss: 3.3955\n",
            "Epoch 1/1, Batch 27360/98059, Loss: 3.5023\n",
            "Epoch 1/1, Batch 27380/98059, Loss: 2.8315\n",
            "Epoch 1/1, Batch 27400/98059, Loss: 3.1680\n",
            "Epoch 1/1, Batch 27420/98059, Loss: 3.1810\n",
            "Epoch 1/1, Batch 27440/98059, Loss: 3.0356\n",
            "Epoch 1/1, Batch 27460/98059, Loss: 3.1476\n",
            "Epoch 1/1, Batch 27480/98059, Loss: 3.2452\n",
            "Epoch 1/1, Batch 27500/98059, Loss: 3.1323\n",
            "Epoch 1/1, Batch 27520/98059, Loss: 3.2288\n",
            "Epoch 1/1, Batch 27540/98059, Loss: 3.0849\n",
            "Epoch 1/1, Batch 27560/98059, Loss: 2.7780\n",
            "Epoch 1/1, Batch 27580/98059, Loss: 3.1124\n",
            "Epoch 1/1, Batch 27600/98059, Loss: 3.1169\n",
            "Epoch 1/1, Batch 27620/98059, Loss: 3.7721\n",
            "Epoch 1/1, Batch 27640/98059, Loss: 3.3025\n",
            "Epoch 1/1, Batch 27660/98059, Loss: 3.1006\n",
            "Epoch 1/1, Batch 27680/98059, Loss: 3.2957\n",
            "Epoch 1/1, Batch 27700/98059, Loss: 3.2239\n",
            "Epoch 1/1, Batch 27720/98059, Loss: 3.1597\n",
            "Epoch 1/1, Batch 27740/98059, Loss: 2.8389\n",
            "Epoch 1/1, Batch 27760/98059, Loss: 3.1006\n",
            "Epoch 1/1, Batch 27780/98059, Loss: 2.9642\n",
            "Epoch 1/1, Batch 27800/98059, Loss: 2.9450\n",
            "Epoch 1/1, Batch 27820/98059, Loss: 2.6980\n",
            "Epoch 1/1, Batch 27840/98059, Loss: 3.0963\n",
            "Epoch 1/1, Batch 27860/98059, Loss: 3.3269\n",
            "Epoch 1/1, Batch 27880/98059, Loss: 3.0715\n",
            "Epoch 1/1, Batch 27900/98059, Loss: 2.9330\n",
            "Epoch 1/1, Batch 27920/98059, Loss: 3.4073\n",
            "Epoch 1/1, Batch 27940/98059, Loss: 3.1149\n",
            "Epoch 1/1, Batch 27960/98059, Loss: 3.2632\n",
            "Epoch 1/1, Batch 27980/98059, Loss: 3.0940\n",
            "Epoch 1/1, Batch 28000/98059, Loss: 3.4889\n",
            "Epoch 1/1, Batch 28020/98059, Loss: 3.1294\n",
            "Epoch 1/1, Batch 28040/98059, Loss: 2.9559\n",
            "Epoch 1/1, Batch 28060/98059, Loss: 3.2675\n",
            "Epoch 1/1, Batch 28080/98059, Loss: 3.7347\n",
            "Epoch 1/1, Batch 28100/98059, Loss: 2.7470\n",
            "Epoch 1/1, Batch 28120/98059, Loss: 3.3434\n",
            "Epoch 1/1, Batch 28140/98059, Loss: 3.1868\n",
            "Epoch 1/1, Batch 28160/98059, Loss: 3.1286\n",
            "Epoch 1/1, Batch 28180/98059, Loss: 3.1162\n",
            "Epoch 1/1, Batch 28200/98059, Loss: 3.1588\n",
            "Epoch 1/1, Batch 28220/98059, Loss: 3.4175\n",
            "Epoch 1/1, Batch 28240/98059, Loss: 3.0015\n",
            "Epoch 1/1, Batch 28260/98059, Loss: 3.1019\n",
            "Epoch 1/1, Batch 28280/98059, Loss: 3.4003\n",
            "Epoch 1/1, Batch 28300/98059, Loss: 3.2453\n",
            "Epoch 1/1, Batch 28320/98059, Loss: 3.7256\n",
            "Epoch 1/1, Batch 28340/98059, Loss: 3.0047\n",
            "Epoch 1/1, Batch 28360/98059, Loss: 3.0732\n",
            "Epoch 1/1, Batch 28380/98059, Loss: 3.1845\n",
            "Epoch 1/1, Batch 28400/98059, Loss: 3.1512\n",
            "Epoch 1/1, Batch 28420/98059, Loss: 3.1568\n",
            "Epoch 1/1, Batch 28440/98059, Loss: 2.9466\n",
            "Epoch 1/1, Batch 28460/98059, Loss: 3.3551\n",
            "Epoch 1/1, Batch 28480/98059, Loss: 3.1570\n",
            "Epoch 1/1, Batch 28500/98059, Loss: 2.9943\n",
            "Epoch 1/1, Batch 28520/98059, Loss: 3.2725\n",
            "Epoch 1/1, Batch 28540/98059, Loss: 3.1598\n",
            "Epoch 1/1, Batch 28560/98059, Loss: 3.3151\n",
            "Epoch 1/1, Batch 28580/98059, Loss: 2.8428\n",
            "Epoch 1/1, Batch 28600/98059, Loss: 2.9474\n",
            "Epoch 1/1, Batch 28620/98059, Loss: 2.6955\n",
            "Epoch 1/1, Batch 28640/98059, Loss: 3.5112\n",
            "Epoch 1/1, Batch 28660/98059, Loss: 3.3352\n",
            "Epoch 1/1, Batch 28680/98059, Loss: 3.0552\n",
            "Epoch 1/1, Batch 28700/98059, Loss: 3.2055\n",
            "Epoch 1/1, Batch 28720/98059, Loss: 3.1622\n",
            "Epoch 1/1, Batch 28740/98059, Loss: 3.3080\n",
            "Epoch 1/1, Batch 28760/98059, Loss: 3.0458\n",
            "Epoch 1/1, Batch 28780/98059, Loss: 2.9902\n",
            "Epoch 1/1, Batch 28800/98059, Loss: 2.7491\n",
            "Epoch 1/1, Batch 28820/98059, Loss: 3.0296\n",
            "Epoch 1/1, Batch 28840/98059, Loss: 2.8841\n",
            "Epoch 1/1, Batch 28860/98059, Loss: 3.5011\n",
            "Epoch 1/1, Batch 28880/98059, Loss: 3.0723\n",
            "Epoch 1/1, Batch 28900/98059, Loss: 3.4701\n",
            "Epoch 1/1, Batch 28920/98059, Loss: 3.1598\n",
            "Epoch 1/1, Batch 28940/98059, Loss: 2.9321\n",
            "Epoch 1/1, Batch 28960/98059, Loss: 3.2806\n",
            "Epoch 1/1, Batch 28980/98059, Loss: 2.9851\n",
            "Epoch 1/1, Batch 29000/98059, Loss: 3.3022\n",
            "Epoch 1/1, Batch 29020/98059, Loss: 2.8083\n",
            "Epoch 1/1, Batch 29040/98059, Loss: 3.2516\n",
            "Epoch 1/1, Batch 29060/98059, Loss: 2.9722\n",
            "Epoch 1/1, Batch 29080/98059, Loss: 3.5189\n",
            "Epoch 1/1, Batch 29100/98059, Loss: 3.2352\n",
            "Epoch 1/1, Batch 29120/98059, Loss: 3.3278\n",
            "Epoch 1/1, Batch 29140/98059, Loss: 3.2037\n",
            "Epoch 1/1, Batch 29160/98059, Loss: 3.0577\n",
            "Epoch 1/1, Batch 29180/98059, Loss: 3.0232\n",
            "Epoch 1/1, Batch 29200/98059, Loss: 3.2314\n",
            "Epoch 1/1, Batch 29220/98059, Loss: 2.9754\n",
            "Epoch 1/1, Batch 29240/98059, Loss: 2.8231\n",
            "Epoch 1/1, Batch 29260/98059, Loss: 2.9579\n",
            "Epoch 1/1, Batch 29280/98059, Loss: 3.4183\n",
            "Epoch 1/1, Batch 29300/98059, Loss: 3.4188\n",
            "Epoch 1/1, Batch 29320/98059, Loss: 2.7910\n",
            "Epoch 1/1, Batch 29340/98059, Loss: 3.2598\n",
            "Epoch 1/1, Batch 29360/98059, Loss: 2.8345\n",
            "Epoch 1/1, Batch 29380/98059, Loss: 3.0689\n",
            "Epoch 1/1, Batch 29400/98059, Loss: 3.0915\n",
            "Epoch 1/1, Batch 29420/98059, Loss: 3.0233\n",
            "Epoch 1/1, Batch 29440/98059, Loss: 3.2966\n",
            "Epoch 1/1, Batch 29460/98059, Loss: 3.1052\n",
            "Epoch 1/1, Batch 29480/98059, Loss: 3.0210\n",
            "Epoch 1/1, Batch 29500/98059, Loss: 2.8382\n",
            "Epoch 1/1, Batch 29520/98059, Loss: 2.8144\n",
            "Epoch 1/1, Batch 29540/98059, Loss: 2.9097\n",
            "Epoch 1/1, Batch 29560/98059, Loss: 3.3110\n",
            "Epoch 1/1, Batch 29580/98059, Loss: 2.9779\n",
            "Epoch 1/1, Batch 29600/98059, Loss: 3.2886\n",
            "Epoch 1/1, Batch 29620/98059, Loss: 3.1841\n",
            "Epoch 1/1, Batch 29640/98059, Loss: 3.2889\n",
            "Epoch 1/1, Batch 29660/98059, Loss: 3.1393\n",
            "Epoch 1/1, Batch 29680/98059, Loss: 3.0530\n",
            "Epoch 1/1, Batch 29700/98059, Loss: 3.2676\n",
            "Epoch 1/1, Batch 29720/98059, Loss: 3.0028\n",
            "Epoch 1/1, Batch 29740/98059, Loss: 3.1642\n",
            "Epoch 1/1, Batch 29760/98059, Loss: 3.1439\n",
            "Epoch 1/1, Batch 29780/98059, Loss: 3.0837\n",
            "Epoch 1/1, Batch 29800/98059, Loss: 3.1988\n",
            "Epoch 1/1, Batch 29820/98059, Loss: 3.2854\n",
            "Epoch 1/1, Batch 29840/98059, Loss: 2.5679\n",
            "Epoch 1/1, Batch 29860/98059, Loss: 3.0135\n",
            "Epoch 1/1, Batch 29880/98059, Loss: 2.8523\n",
            "Epoch 1/1, Batch 29900/98059, Loss: 3.0753\n",
            "Epoch 1/1, Batch 29920/98059, Loss: 3.0107\n",
            "Epoch 1/1, Batch 29940/98059, Loss: 3.3220\n",
            "Epoch 1/1, Batch 29960/98059, Loss: 2.9369\n",
            "Epoch 1/1, Batch 29980/98059, Loss: 3.2437\n",
            "Epoch 1/1, Batch 30000/98059, Loss: 3.3732\n",
            "Epoch 1/1, Batch 30020/98059, Loss: 2.9985\n",
            "Epoch 1/1, Batch 30040/98059, Loss: 3.2725\n",
            "Epoch 1/1, Batch 30060/98059, Loss: 3.3199\n",
            "Epoch 1/1, Batch 30080/98059, Loss: 3.1422\n",
            "Epoch 1/1, Batch 30100/98059, Loss: 3.1217\n",
            "Epoch 1/1, Batch 30120/98059, Loss: 3.2956\n",
            "Epoch 1/1, Batch 30140/98059, Loss: 3.1671\n",
            "Epoch 1/1, Batch 30160/98059, Loss: 3.4376\n",
            "Epoch 1/1, Batch 30180/98059, Loss: 3.0284\n",
            "Epoch 1/1, Batch 30200/98059, Loss: 3.2282\n",
            "Epoch 1/1, Batch 30220/98059, Loss: 3.2946\n",
            "Epoch 1/1, Batch 30240/98059, Loss: 3.1163\n",
            "Epoch 1/1, Batch 30260/98059, Loss: 3.1007\n",
            "Epoch 1/1, Batch 30280/98059, Loss: 3.3412\n",
            "Epoch 1/1, Batch 30300/98059, Loss: 3.0971\n",
            "Epoch 1/1, Batch 30320/98059, Loss: 3.1958\n",
            "Epoch 1/1, Batch 30340/98059, Loss: 2.9643\n",
            "Epoch 1/1, Batch 30360/98059, Loss: 3.0012\n",
            "Epoch 1/1, Batch 30380/98059, Loss: 3.2355\n",
            "Epoch 1/1, Batch 30400/98059, Loss: 3.2411\n",
            "Epoch 1/1, Batch 30420/98059, Loss: 3.4282\n",
            "Epoch 1/1, Batch 30440/98059, Loss: 3.0730\n",
            "Epoch 1/1, Batch 30460/98059, Loss: 3.0585\n",
            "Epoch 1/1, Batch 30480/98059, Loss: 3.3539\n",
            "Epoch 1/1, Batch 30500/98059, Loss: 3.3615\n",
            "Epoch 1/1, Batch 30520/98059, Loss: 3.0754\n",
            "Epoch 1/1, Batch 30540/98059, Loss: 3.4166\n",
            "Epoch 1/1, Batch 30560/98059, Loss: 3.1723\n",
            "Epoch 1/1, Batch 30580/98059, Loss: 2.9881\n",
            "Epoch 1/1, Batch 30600/98059, Loss: 3.3640\n",
            "Epoch 1/1, Batch 30620/98059, Loss: 3.1800\n",
            "Epoch 1/1, Batch 30640/98059, Loss: 3.2032\n",
            "Epoch 1/1, Batch 30660/98059, Loss: 3.6339\n",
            "Epoch 1/1, Batch 30680/98059, Loss: 2.9501\n",
            "Epoch 1/1, Batch 30700/98059, Loss: 3.2621\n",
            "Epoch 1/1, Batch 30720/98059, Loss: 3.2612\n",
            "Epoch 1/1, Batch 30740/98059, Loss: 3.0854\n",
            "Epoch 1/1, Batch 30760/98059, Loss: 2.8956\n",
            "Epoch 1/1, Batch 30780/98059, Loss: 3.2751\n",
            "Epoch 1/1, Batch 30800/98059, Loss: 3.4845\n",
            "Epoch 1/1, Batch 30820/98059, Loss: 3.0751\n",
            "Epoch 1/1, Batch 30840/98059, Loss: 3.3230\n",
            "Epoch 1/1, Batch 30860/98059, Loss: 2.9845\n",
            "Epoch 1/1, Batch 30880/98059, Loss: 2.5625\n",
            "Epoch 1/1, Batch 30900/98059, Loss: 2.7427\n",
            "Epoch 1/1, Batch 30920/98059, Loss: 3.3185\n",
            "Epoch 1/1, Batch 30940/98059, Loss: 3.0015\n",
            "Epoch 1/1, Batch 30960/98059, Loss: 3.4155\n",
            "Epoch 1/1, Batch 30980/98059, Loss: 3.0974\n",
            "Epoch 1/1, Batch 31000/98059, Loss: 3.3034\n",
            "Epoch 1/1, Batch 31020/98059, Loss: 3.0578\n",
            "Epoch 1/1, Batch 31040/98059, Loss: 3.1374\n",
            "Epoch 1/1, Batch 31060/98059, Loss: 3.1067\n",
            "Epoch 1/1, Batch 31080/98059, Loss: 3.0383\n",
            "Epoch 1/1, Batch 31100/98059, Loss: 3.2649\n",
            "Epoch 1/1, Batch 31120/98059, Loss: 2.8215\n",
            "Epoch 1/1, Batch 31140/98059, Loss: 3.2031\n",
            "Epoch 1/1, Batch 31160/98059, Loss: 3.2201\n",
            "Epoch 1/1, Batch 31180/98059, Loss: 3.3975\n",
            "Epoch 1/1, Batch 31200/98059, Loss: 2.7223\n",
            "Epoch 1/1, Batch 31220/98059, Loss: 3.3169\n",
            "Epoch 1/1, Batch 31240/98059, Loss: 3.1206\n",
            "Epoch 1/1, Batch 31260/98059, Loss: 3.1814\n",
            "Epoch 1/1, Batch 31280/98059, Loss: 3.1911\n",
            "Epoch 1/1, Batch 31300/98059, Loss: 3.1185\n",
            "Epoch 1/1, Batch 31320/98059, Loss: 3.2322\n",
            "Epoch 1/1, Batch 31340/98059, Loss: 2.9862\n",
            "Epoch 1/1, Batch 31360/98059, Loss: 2.9944\n",
            "Epoch 1/1, Batch 31380/98059, Loss: 2.9040\n",
            "Epoch 1/1, Batch 31400/98059, Loss: 2.7825\n",
            "Epoch 1/1, Batch 31420/98059, Loss: 2.8923\n",
            "Epoch 1/1, Batch 31440/98059, Loss: 3.0155\n",
            "Epoch 1/1, Batch 31460/98059, Loss: 3.2037\n",
            "Epoch 1/1, Batch 31480/98059, Loss: 3.6449\n",
            "Epoch 1/1, Batch 31500/98059, Loss: 2.8295\n",
            "Epoch 1/1, Batch 31520/98059, Loss: 2.6902\n",
            "Epoch 1/1, Batch 31540/98059, Loss: 3.0331\n",
            "Epoch 1/1, Batch 31560/98059, Loss: 3.3578\n",
            "Epoch 1/1, Batch 31580/98059, Loss: 3.1621\n",
            "Epoch 1/1, Batch 31600/98059, Loss: 3.1267\n",
            "Epoch 1/1, Batch 31620/98059, Loss: 3.1907\n",
            "Epoch 1/1, Batch 31640/98059, Loss: 3.1821\n",
            "Epoch 1/1, Batch 31660/98059, Loss: 3.2495\n",
            "Epoch 1/1, Batch 31680/98059, Loss: 3.3477\n",
            "Epoch 1/1, Batch 31700/98059, Loss: 2.7781\n",
            "Epoch 1/1, Batch 31720/98059, Loss: 3.0889\n",
            "Epoch 1/1, Batch 31740/98059, Loss: 2.9679\n",
            "Epoch 1/1, Batch 31760/98059, Loss: 3.1994\n",
            "Epoch 1/1, Batch 31780/98059, Loss: 3.0966\n",
            "Epoch 1/1, Batch 31800/98059, Loss: 3.1831\n",
            "Epoch 1/1, Batch 31820/98059, Loss: 3.2617\n",
            "Epoch 1/1, Batch 31840/98059, Loss: 3.0672\n",
            "Epoch 1/1, Batch 31860/98059, Loss: 3.1844\n",
            "Epoch 1/1, Batch 31880/98059, Loss: 3.3886\n",
            "Epoch 1/1, Batch 31900/98059, Loss: 2.7714\n",
            "Epoch 1/1, Batch 31920/98059, Loss: 2.7432\n",
            "Epoch 1/1, Batch 31940/98059, Loss: 2.7508\n",
            "Epoch 1/1, Batch 31960/98059, Loss: 2.9228\n",
            "Epoch 1/1, Batch 31980/98059, Loss: 3.0397\n",
            "Epoch 1/1, Batch 32000/98059, Loss: 2.9814\n",
            "Epoch 1/1, Batch 32020/98059, Loss: 3.3953\n",
            "Epoch 1/1, Batch 32040/98059, Loss: 2.7800\n",
            "Epoch 1/1, Batch 32060/98059, Loss: 2.8528\n",
            "Epoch 1/1, Batch 32080/98059, Loss: 3.0906\n",
            "Epoch 1/1, Batch 32100/98059, Loss: 3.2429\n",
            "Epoch 1/1, Batch 32120/98059, Loss: 3.2975\n",
            "Epoch 1/1, Batch 32140/98059, Loss: 2.7409\n",
            "Epoch 1/1, Batch 32160/98059, Loss: 3.3616\n",
            "Epoch 1/1, Batch 32180/98059, Loss: 3.1896\n",
            "Epoch 1/1, Batch 32200/98059, Loss: 3.4348\n",
            "Epoch 1/1, Batch 32220/98059, Loss: 3.1429\n",
            "Epoch 1/1, Batch 32240/98059, Loss: 3.1434\n",
            "Epoch 1/1, Batch 32260/98059, Loss: 3.1969\n",
            "Epoch 1/1, Batch 32280/98059, Loss: 3.3484\n",
            "Epoch 1/1, Batch 32300/98059, Loss: 3.2349\n",
            "Epoch 1/1, Batch 32320/98059, Loss: 3.3938\n",
            "Epoch 1/1, Batch 32340/98059, Loss: 2.9627\n",
            "Epoch 1/1, Batch 32360/98059, Loss: 2.8070\n",
            "Epoch 1/1, Batch 32380/98059, Loss: 2.7916\n",
            "Epoch 1/1, Batch 32400/98059, Loss: 3.2550\n",
            "Epoch 1/1, Batch 32420/98059, Loss: 3.1772\n",
            "Epoch 1/1, Batch 32440/98059, Loss: 3.0014\n",
            "Epoch 1/1, Batch 32460/98059, Loss: 3.1812\n",
            "Epoch 1/1, Batch 32480/98059, Loss: 3.0152\n",
            "Epoch 1/1, Batch 32500/98059, Loss: 3.1205\n",
            "Epoch 1/1, Batch 32520/98059, Loss: 3.1693\n",
            "Epoch 1/1, Batch 32540/98059, Loss: 3.3452\n",
            "Epoch 1/1, Batch 32560/98059, Loss: 3.1827\n",
            "Epoch 1/1, Batch 32580/98059, Loss: 3.2348\n",
            "Epoch 1/1, Batch 32600/98059, Loss: 2.7175\n",
            "Epoch 1/1, Batch 32620/98059, Loss: 3.1611\n",
            "Epoch 1/1, Batch 32640/98059, Loss: 3.1770\n",
            "Epoch 1/1, Batch 32660/98059, Loss: 3.2681\n",
            "Epoch 1/1, Batch 32680/98059, Loss: 3.4013\n",
            "Epoch 1/1, Batch 32700/98059, Loss: 3.2006\n",
            "Epoch 1/1, Batch 32720/98059, Loss: 3.1784\n",
            "Epoch 1/1, Batch 32740/98059, Loss: 2.9493\n",
            "Epoch 1/1, Batch 32760/98059, Loss: 3.3466\n",
            "Epoch 1/1, Batch 32780/98059, Loss: 3.2645\n",
            "Epoch 1/1, Batch 32800/98059, Loss: 3.2518\n",
            "Epoch 1/1, Batch 32820/98059, Loss: 3.3633\n",
            "Epoch 1/1, Batch 32840/98059, Loss: 3.0093\n",
            "Epoch 1/1, Batch 32860/98059, Loss: 3.2671\n",
            "Epoch 1/1, Batch 32880/98059, Loss: 3.0535\n",
            "Epoch 1/1, Batch 32900/98059, Loss: 3.3663\n",
            "Epoch 1/1, Batch 32920/98059, Loss: 3.1293\n",
            "Epoch 1/1, Batch 32940/98059, Loss: 2.7122\n",
            "Epoch 1/1, Batch 32960/98059, Loss: 3.1804\n",
            "Epoch 1/1, Batch 32980/98059, Loss: 3.4047\n",
            "Epoch 1/1, Batch 33000/98059, Loss: 3.3232\n",
            "Epoch 1/1, Batch 33020/98059, Loss: 3.2901\n",
            "Epoch 1/1, Batch 33040/98059, Loss: 3.1708\n",
            "Epoch 1/1, Batch 33060/98059, Loss: 3.3142\n",
            "Epoch 1/1, Batch 33080/98059, Loss: 3.0544\n",
            "Epoch 1/1, Batch 33100/98059, Loss: 3.1466\n",
            "Epoch 1/1, Batch 33120/98059, Loss: 3.3314\n",
            "Epoch 1/1, Batch 33140/98059, Loss: 3.2399\n",
            "Epoch 1/1, Batch 33160/98059, Loss: 2.8916\n",
            "Epoch 1/1, Batch 33180/98059, Loss: 3.1341\n",
            "Epoch 1/1, Batch 33200/98059, Loss: 2.8630\n",
            "Epoch 1/1, Batch 33220/98059, Loss: 3.3569\n",
            "Epoch 1/1, Batch 33240/98059, Loss: 3.3618\n",
            "Epoch 1/1, Batch 33260/98059, Loss: 3.0939\n",
            "Epoch 1/1, Batch 33280/98059, Loss: 2.8864\n",
            "Epoch 1/1, Batch 33300/98059, Loss: 3.6394\n",
            "Epoch 1/1, Batch 33320/98059, Loss: 3.0214\n",
            "Epoch 1/1, Batch 33340/98059, Loss: 3.0736\n",
            "Epoch 1/1, Batch 33360/98059, Loss: 2.8765\n",
            "Epoch 1/1, Batch 33380/98059, Loss: 2.9400\n",
            "Epoch 1/1, Batch 33400/98059, Loss: 3.1230\n",
            "Epoch 1/1, Batch 33420/98059, Loss: 3.1219\n",
            "Epoch 1/1, Batch 33440/98059, Loss: 3.0043\n",
            "Epoch 1/1, Batch 33460/98059, Loss: 3.1475\n",
            "Epoch 1/1, Batch 33480/98059, Loss: 3.2241\n",
            "Epoch 1/1, Batch 33500/98059, Loss: 2.6403\n",
            "Epoch 1/1, Batch 33520/98059, Loss: 3.1573\n",
            "Epoch 1/1, Batch 33540/98059, Loss: 3.3187\n",
            "Epoch 1/1, Batch 33560/98059, Loss: 3.2035\n",
            "Epoch 1/1, Batch 33580/98059, Loss: 3.1299\n",
            "Epoch 1/1, Batch 33600/98059, Loss: 3.5375\n",
            "Epoch 1/1, Batch 33620/98059, Loss: 3.2625\n",
            "Epoch 1/1, Batch 33640/98059, Loss: 3.1817\n",
            "Epoch 1/1, Batch 33660/98059, Loss: 3.2104\n",
            "Epoch 1/1, Batch 33680/98059, Loss: 3.1542\n",
            "Epoch 1/1, Batch 33700/98059, Loss: 2.8226\n",
            "Epoch 1/1, Batch 33720/98059, Loss: 3.0434\n",
            "Epoch 1/1, Batch 33740/98059, Loss: 3.1041\n",
            "Epoch 1/1, Batch 33760/98059, Loss: 3.0336\n",
            "Epoch 1/1, Batch 33780/98059, Loss: 2.8890\n",
            "Epoch 1/1, Batch 33800/98059, Loss: 3.3778\n",
            "Epoch 1/1, Batch 33820/98059, Loss: 2.8396\n",
            "Epoch 1/1, Batch 33840/98059, Loss: 3.3246\n",
            "Epoch 1/1, Batch 33860/98059, Loss: 3.5207\n",
            "Epoch 1/1, Batch 33880/98059, Loss: 3.3837\n",
            "Epoch 1/1, Batch 33900/98059, Loss: 3.3873\n",
            "Epoch 1/1, Batch 33920/98059, Loss: 2.9379\n",
            "Epoch 1/1, Batch 33940/98059, Loss: 3.2165\n",
            "Epoch 1/1, Batch 33960/98059, Loss: 2.8447\n",
            "Epoch 1/1, Batch 33980/98059, Loss: 3.0226\n",
            "Epoch 1/1, Batch 34000/98059, Loss: 2.5732\n",
            "Epoch 1/1, Batch 34020/98059, Loss: 3.1916\n",
            "Epoch 1/1, Batch 34040/98059, Loss: 3.4089\n",
            "Epoch 1/1, Batch 34060/98059, Loss: 3.0697\n",
            "Epoch 1/1, Batch 34080/98059, Loss: 3.1780\n",
            "Epoch 1/1, Batch 34100/98059, Loss: 3.1980\n",
            "Epoch 1/1, Batch 34120/98059, Loss: 3.5346\n",
            "Epoch 1/1, Batch 34140/98059, Loss: 3.0710\n",
            "Epoch 1/1, Batch 34160/98059, Loss: 3.4348\n",
            "Epoch 1/1, Batch 34180/98059, Loss: 3.6137\n",
            "Epoch 1/1, Batch 34200/98059, Loss: 3.7184\n",
            "Epoch 1/1, Batch 34220/98059, Loss: 3.1834\n",
            "Epoch 1/1, Batch 34240/98059, Loss: 3.1218\n",
            "Epoch 1/1, Batch 34260/98059, Loss: 2.8964\n",
            "Epoch 1/1, Batch 34280/98059, Loss: 3.3267\n",
            "Epoch 1/1, Batch 34300/98059, Loss: 3.3676\n",
            "Epoch 1/1, Batch 34320/98059, Loss: 2.9258\n",
            "Epoch 1/1, Batch 34340/98059, Loss: 2.8263\n",
            "Epoch 1/1, Batch 34360/98059, Loss: 3.2849\n",
            "Epoch 1/1, Batch 34380/98059, Loss: 2.9866\n",
            "Epoch 1/1, Batch 34400/98059, Loss: 3.0509\n",
            "Epoch 1/1, Batch 34420/98059, Loss: 2.9111\n",
            "Epoch 1/1, Batch 34440/98059, Loss: 3.0194\n",
            "Epoch 1/1, Batch 34460/98059, Loss: 3.0214\n",
            "Epoch 1/1, Batch 34480/98059, Loss: 3.1000\n",
            "Epoch 1/1, Batch 34500/98059, Loss: 2.8296\n",
            "Epoch 1/1, Batch 34520/98059, Loss: 2.8505\n",
            "Epoch 1/1, Batch 34540/98059, Loss: 3.2506\n",
            "Epoch 1/1, Batch 34560/98059, Loss: 3.1394\n",
            "Epoch 1/1, Batch 34580/98059, Loss: 3.1624\n",
            "Epoch 1/1, Batch 34600/98059, Loss: 3.1113\n",
            "Epoch 1/1, Batch 34620/98059, Loss: 3.0742\n",
            "Epoch 1/1, Batch 34640/98059, Loss: 3.0774\n",
            "Epoch 1/1, Batch 34660/98059, Loss: 3.2153\n",
            "Epoch 1/1, Batch 34680/98059, Loss: 2.9874\n",
            "Epoch 1/1, Batch 34700/98059, Loss: 3.0197\n",
            "Epoch 1/1, Batch 34720/98059, Loss: 2.9211\n",
            "Epoch 1/1, Batch 34740/98059, Loss: 3.2989\n",
            "Epoch 1/1, Batch 34760/98059, Loss: 3.1162\n",
            "Epoch 1/1, Batch 34780/98059, Loss: 3.3086\n",
            "Epoch 1/1, Batch 34800/98059, Loss: 3.1186\n",
            "Epoch 1/1, Batch 34820/98059, Loss: 2.9476\n",
            "Epoch 1/1, Batch 34840/98059, Loss: 2.9523\n",
            "Epoch 1/1, Batch 34860/98059, Loss: 3.5134\n",
            "Epoch 1/1, Batch 34880/98059, Loss: 3.5590\n",
            "Epoch 1/1, Batch 34900/98059, Loss: 3.0079\n",
            "Epoch 1/1, Batch 34920/98059, Loss: 3.2663\n",
            "Epoch 1/1, Batch 34940/98059, Loss: 3.3884\n",
            "Epoch 1/1, Batch 34960/98059, Loss: 2.6366\n",
            "Epoch 1/1, Batch 34980/98059, Loss: 3.0735\n",
            "Epoch 1/1, Batch 35000/98059, Loss: 3.3271\n",
            "Epoch 1/1, Batch 35020/98059, Loss: 2.9148\n",
            "Epoch 1/1, Batch 35040/98059, Loss: 3.3064\n",
            "Epoch 1/1, Batch 35060/98059, Loss: 3.0131\n",
            "Epoch 1/1, Batch 35080/98059, Loss: 3.2407\n",
            "Epoch 1/1, Batch 35100/98059, Loss: 3.1016\n",
            "Epoch 1/1, Batch 35120/98059, Loss: 3.0510\n",
            "Epoch 1/1, Batch 35140/98059, Loss: 3.2611\n",
            "Epoch 1/1, Batch 35160/98059, Loss: 3.0615\n",
            "Epoch 1/1, Batch 35180/98059, Loss: 3.3211\n",
            "Epoch 1/1, Batch 35200/98059, Loss: 3.1139\n",
            "Epoch 1/1, Batch 35220/98059, Loss: 3.1290\n",
            "Epoch 1/1, Batch 35240/98059, Loss: 3.0854\n",
            "Epoch 1/1, Batch 35260/98059, Loss: 3.0798\n",
            "Epoch 1/1, Batch 35280/98059, Loss: 3.2612\n",
            "Epoch 1/1, Batch 35300/98059, Loss: 2.9886\n",
            "Epoch 1/1, Batch 35320/98059, Loss: 3.1695\n",
            "Epoch 1/1, Batch 35340/98059, Loss: 3.0028\n",
            "Epoch 1/1, Batch 35360/98059, Loss: 3.1173\n",
            "Epoch 1/1, Batch 35380/98059, Loss: 3.2951\n",
            "Epoch 1/1, Batch 35400/98059, Loss: 3.0871\n",
            "Epoch 1/1, Batch 35420/98059, Loss: 3.2134\n",
            "Epoch 1/1, Batch 35440/98059, Loss: 3.2016\n",
            "Epoch 1/1, Batch 35460/98059, Loss: 2.9739\n",
            "Epoch 1/1, Batch 35480/98059, Loss: 3.1505\n",
            "Epoch 1/1, Batch 35500/98059, Loss: 3.1827\n",
            "Epoch 1/1, Batch 35520/98059, Loss: 2.9326\n",
            "Epoch 1/1, Batch 35540/98059, Loss: 3.0563\n",
            "Epoch 1/1, Batch 35560/98059, Loss: 2.6246\n",
            "Epoch 1/1, Batch 35580/98059, Loss: 3.4062\n",
            "Epoch 1/1, Batch 35600/98059, Loss: 3.0310\n",
            "Epoch 1/1, Batch 35620/98059, Loss: 2.9850\n",
            "Epoch 1/1, Batch 35640/98059, Loss: 3.0420\n",
            "Epoch 1/1, Batch 35660/98059, Loss: 3.2260\n",
            "Epoch 1/1, Batch 35680/98059, Loss: 2.8925\n",
            "Epoch 1/1, Batch 35700/98059, Loss: 3.0340\n",
            "Epoch 1/1, Batch 35720/98059, Loss: 3.2051\n",
            "Epoch 1/1, Batch 35740/98059, Loss: 3.1093\n",
            "Epoch 1/1, Batch 35760/98059, Loss: 2.9367\n",
            "Epoch 1/1, Batch 35780/98059, Loss: 2.9968\n",
            "Epoch 1/1, Batch 35800/98059, Loss: 3.4349\n",
            "Epoch 1/1, Batch 35820/98059, Loss: 3.1814\n",
            "Epoch 1/1, Batch 35840/98059, Loss: 3.2329\n",
            "Epoch 1/1, Batch 35860/98059, Loss: 2.8759\n",
            "Epoch 1/1, Batch 35880/98059, Loss: 2.8449\n",
            "Epoch 1/1, Batch 35900/98059, Loss: 3.1605\n",
            "Epoch 1/1, Batch 35920/98059, Loss: 3.1098\n",
            "Epoch 1/1, Batch 35940/98059, Loss: 3.0193\n",
            "Epoch 1/1, Batch 35960/98059, Loss: 3.1859\n",
            "Epoch 1/1, Batch 35980/98059, Loss: 3.2698\n",
            "Epoch 1/1, Batch 36000/98059, Loss: 3.1381\n",
            "Epoch 1/1, Batch 36020/98059, Loss: 3.1240\n",
            "Epoch 1/1, Batch 36040/98059, Loss: 3.4883\n",
            "Epoch 1/1, Batch 36060/98059, Loss: 3.1524\n",
            "Epoch 1/1, Batch 36080/98059, Loss: 3.3770\n",
            "Epoch 1/1, Batch 36100/98059, Loss: 3.3666\n",
            "Epoch 1/1, Batch 36120/98059, Loss: 3.3680\n",
            "Epoch 1/1, Batch 36140/98059, Loss: 3.1342\n",
            "Epoch 1/1, Batch 36160/98059, Loss: 3.0924\n",
            "Epoch 1/1, Batch 36180/98059, Loss: 3.0597\n",
            "Epoch 1/1, Batch 36200/98059, Loss: 3.1288\n",
            "Epoch 1/1, Batch 36220/98059, Loss: 3.4583\n",
            "Epoch 1/1, Batch 36240/98059, Loss: 2.8570\n",
            "Epoch 1/1, Batch 36260/98059, Loss: 3.2721\n",
            "Epoch 1/1, Batch 36280/98059, Loss: 3.2661\n",
            "Epoch 1/1, Batch 36300/98059, Loss: 3.5213\n",
            "Epoch 1/1, Batch 36320/98059, Loss: 3.1464\n",
            "Epoch 1/1, Batch 36340/98059, Loss: 3.1100\n",
            "Epoch 1/1, Batch 36360/98059, Loss: 3.3013\n",
            "Epoch 1/1, Batch 36380/98059, Loss: 3.2464\n",
            "Epoch 1/1, Batch 36400/98059, Loss: 3.1883\n",
            "Epoch 1/1, Batch 36420/98059, Loss: 3.0104\n",
            "Epoch 1/1, Batch 36440/98059, Loss: 3.2360\n",
            "Epoch 1/1, Batch 36460/98059, Loss: 3.0645\n",
            "Epoch 1/1, Batch 36480/98059, Loss: 3.1406\n",
            "Epoch 1/1, Batch 36500/98059, Loss: 2.9875\n",
            "Epoch 1/1, Batch 36520/98059, Loss: 2.7821\n",
            "Epoch 1/1, Batch 36540/98059, Loss: 3.1870\n",
            "Epoch 1/1, Batch 36560/98059, Loss: 3.3923\n",
            "Epoch 1/1, Batch 36580/98059, Loss: 2.8974\n",
            "Epoch 1/1, Batch 36600/98059, Loss: 3.5104\n",
            "Epoch 1/1, Batch 36620/98059, Loss: 3.4351\n",
            "Epoch 1/1, Batch 36640/98059, Loss: 3.4217\n",
            "Epoch 1/1, Batch 36660/98059, Loss: 3.3341\n",
            "Epoch 1/1, Batch 36680/98059, Loss: 3.3757\n",
            "Epoch 1/1, Batch 36700/98059, Loss: 3.2864\n",
            "Epoch 1/1, Batch 36720/98059, Loss: 2.4984\n",
            "Epoch 1/1, Batch 36740/98059, Loss: 2.8936\n",
            "Epoch 1/1, Batch 36760/98059, Loss: 2.8884\n",
            "Epoch 1/1, Batch 36780/98059, Loss: 3.0731\n",
            "Epoch 1/1, Batch 36800/98059, Loss: 2.7606\n",
            "Epoch 1/1, Batch 36820/98059, Loss: 3.2800\n",
            "Epoch 1/1, Batch 36840/98059, Loss: 3.1298\n",
            "Epoch 1/1, Batch 36860/98059, Loss: 2.9498\n",
            "Epoch 1/1, Batch 36880/98059, Loss: 3.3718\n",
            "Epoch 1/1, Batch 36900/98059, Loss: 2.9483\n",
            "Epoch 1/1, Batch 36920/98059, Loss: 3.2698\n",
            "Epoch 1/1, Batch 36940/98059, Loss: 3.2533\n",
            "Epoch 1/1, Batch 36960/98059, Loss: 2.7287\n",
            "Epoch 1/1, Batch 36980/98059, Loss: 3.3910\n",
            "Epoch 1/1, Batch 37000/98059, Loss: 2.9287\n",
            "Epoch 1/1, Batch 37020/98059, Loss: 2.9178\n",
            "Epoch 1/1, Batch 37040/98059, Loss: 2.8592\n",
            "Epoch 1/1, Batch 37060/98059, Loss: 3.3936\n",
            "Epoch 1/1, Batch 37080/98059, Loss: 3.1670\n",
            "Epoch 1/1, Batch 37100/98059, Loss: 3.1501\n",
            "Epoch 1/1, Batch 37120/98059, Loss: 3.4168\n",
            "Epoch 1/1, Batch 37140/98059, Loss: 2.8716\n",
            "Epoch 1/1, Batch 37160/98059, Loss: 2.6697\n",
            "Epoch 1/1, Batch 37180/98059, Loss: 3.2756\n",
            "Epoch 1/1, Batch 37200/98059, Loss: 3.1039\n",
            "Epoch 1/1, Batch 37220/98059, Loss: 2.8619\n",
            "Epoch 1/1, Batch 37240/98059, Loss: 3.0165\n",
            "Epoch 1/1, Batch 37260/98059, Loss: 3.2555\n",
            "Epoch 1/1, Batch 37280/98059, Loss: 3.6859\n",
            "Epoch 1/1, Batch 37300/98059, Loss: 3.0454\n",
            "Epoch 1/1, Batch 37320/98059, Loss: 3.0454\n",
            "Epoch 1/1, Batch 37340/98059, Loss: 3.0595\n",
            "Epoch 1/1, Batch 37360/98059, Loss: 3.1416\n",
            "Epoch 1/1, Batch 37380/98059, Loss: 3.1148\n",
            "Epoch 1/1, Batch 37400/98059, Loss: 3.0141\n",
            "Epoch 1/1, Batch 37420/98059, Loss: 3.1377\n",
            "Epoch 1/1, Batch 37440/98059, Loss: 2.9193\n",
            "Epoch 1/1, Batch 37460/98059, Loss: 3.1094\n",
            "Epoch 1/1, Batch 37480/98059, Loss: 3.1692\n",
            "Epoch 1/1, Batch 37500/98059, Loss: 3.0636\n",
            "Epoch 1/1, Batch 37520/98059, Loss: 3.2332\n",
            "Epoch 1/1, Batch 37540/98059, Loss: 2.8623\n",
            "Epoch 1/1, Batch 37560/98059, Loss: 2.7743\n",
            "Epoch 1/1, Batch 37580/98059, Loss: 3.0478\n",
            "Epoch 1/1, Batch 37600/98059, Loss: 3.2158\n",
            "Epoch 1/1, Batch 37620/98059, Loss: 3.3392\n",
            "Epoch 1/1, Batch 37640/98059, Loss: 3.4417\n",
            "Epoch 1/1, Batch 37660/98059, Loss: 3.1480\n",
            "Epoch 1/1, Batch 37680/98059, Loss: 3.2222\n",
            "Epoch 1/1, Batch 37700/98059, Loss: 2.9904\n",
            "Epoch 1/1, Batch 37720/98059, Loss: 3.2088\n",
            "Epoch 1/1, Batch 37740/98059, Loss: 3.2414\n",
            "Epoch 1/1, Batch 37760/98059, Loss: 3.3248\n",
            "Epoch 1/1, Batch 37780/98059, Loss: 3.4783\n",
            "Epoch 1/1, Batch 37800/98059, Loss: 3.2509\n",
            "Epoch 1/1, Batch 37820/98059, Loss: 2.9497\n",
            "Epoch 1/1, Batch 37840/98059, Loss: 3.3303\n",
            "Epoch 1/1, Batch 37860/98059, Loss: 3.4825\n",
            "Epoch 1/1, Batch 37880/98059, Loss: 3.2053\n",
            "Epoch 1/1, Batch 37900/98059, Loss: 3.0468\n",
            "Epoch 1/1, Batch 37920/98059, Loss: 3.1720\n",
            "Epoch 1/1, Batch 37940/98059, Loss: 2.9782\n",
            "Epoch 1/1, Batch 37960/98059, Loss: 3.2306\n",
            "Epoch 1/1, Batch 37980/98059, Loss: 3.4905\n",
            "Epoch 1/1, Batch 38000/98059, Loss: 3.2620\n",
            "Epoch 1/1, Batch 38020/98059, Loss: 2.7691\n",
            "Epoch 1/1, Batch 38040/98059, Loss: 2.6828\n",
            "Epoch 1/1, Batch 38060/98059, Loss: 3.0978\n",
            "Epoch 1/1, Batch 38080/98059, Loss: 2.8466\n",
            "Epoch 1/1, Batch 38100/98059, Loss: 2.8648\n",
            "Epoch 1/1, Batch 38120/98059, Loss: 3.0302\n",
            "Epoch 1/1, Batch 38140/98059, Loss: 3.3595\n",
            "Epoch 1/1, Batch 38160/98059, Loss: 3.0668\n",
            "Epoch 1/1, Batch 38180/98059, Loss: 2.9362\n",
            "Epoch 1/1, Batch 38200/98059, Loss: 3.2756\n",
            "Epoch 1/1, Batch 38220/98059, Loss: 3.0987\n",
            "Epoch 1/1, Batch 38240/98059, Loss: 2.9854\n",
            "Epoch 1/1, Batch 38260/98059, Loss: 3.1995\n",
            "Epoch 1/1, Batch 38280/98059, Loss: 3.2242\n",
            "Epoch 1/1, Batch 38300/98059, Loss: 3.2799\n",
            "Epoch 1/1, Batch 38320/98059, Loss: 2.5844\n",
            "Epoch 1/1, Batch 38340/98059, Loss: 3.0875\n",
            "Epoch 1/1, Batch 38360/98059, Loss: 2.9891\n",
            "Epoch 1/1, Batch 38380/98059, Loss: 3.0391\n",
            "Epoch 1/1, Batch 38400/98059, Loss: 2.7699\n",
            "Epoch 1/1, Batch 38420/98059, Loss: 2.8120\n",
            "Epoch 1/1, Batch 38440/98059, Loss: 2.8659\n",
            "Epoch 1/1, Batch 38460/98059, Loss: 3.1976\n",
            "Epoch 1/1, Batch 38480/98059, Loss: 3.1245\n",
            "Epoch 1/1, Batch 38500/98059, Loss: 3.4061\n",
            "Epoch 1/1, Batch 38520/98059, Loss: 3.0379\n",
            "Epoch 1/1, Batch 38540/98059, Loss: 3.2443\n",
            "Epoch 1/1, Batch 38560/98059, Loss: 3.2629\n",
            "Epoch 1/1, Batch 38580/98059, Loss: 2.9158\n",
            "Epoch 1/1, Batch 38600/98059, Loss: 3.2768\n",
            "Epoch 1/1, Batch 38620/98059, Loss: 2.8417\n",
            "Epoch 1/1, Batch 38640/98059, Loss: 2.9352\n",
            "Epoch 1/1, Batch 38660/98059, Loss: 3.1301\n",
            "Epoch 1/1, Batch 38680/98059, Loss: 3.5332\n",
            "Epoch 1/1, Batch 38700/98059, Loss: 3.1614\n",
            "Epoch 1/1, Batch 38720/98059, Loss: 2.9826\n",
            "Epoch 1/1, Batch 38740/98059, Loss: 3.0211\n",
            "Epoch 1/1, Batch 38760/98059, Loss: 3.0548\n",
            "Epoch 1/1, Batch 38780/98059, Loss: 3.1299\n",
            "Epoch 1/1, Batch 38800/98059, Loss: 3.2296\n",
            "Epoch 1/1, Batch 38820/98059, Loss: 2.9567\n",
            "Epoch 1/1, Batch 38840/98059, Loss: 3.0892\n",
            "Epoch 1/1, Batch 38860/98059, Loss: 3.0178\n",
            "Epoch 1/1, Batch 38880/98059, Loss: 3.3298\n",
            "Epoch 1/1, Batch 38900/98059, Loss: 2.9515\n",
            "Epoch 1/1, Batch 38920/98059, Loss: 3.1421\n",
            "Epoch 1/1, Batch 38940/98059, Loss: 2.9401\n",
            "Epoch 1/1, Batch 38960/98059, Loss: 3.0727\n",
            "Epoch 1/1, Batch 38980/98059, Loss: 2.5361\n",
            "Epoch 1/1, Batch 39000/98059, Loss: 2.9360\n",
            "Epoch 1/1, Batch 39020/98059, Loss: 2.5934\n",
            "Epoch 1/1, Batch 39040/98059, Loss: 3.2922\n",
            "Epoch 1/1, Batch 39060/98059, Loss: 3.2211\n",
            "Epoch 1/1, Batch 39080/98059, Loss: 3.2925\n",
            "Epoch 1/1, Batch 39100/98059, Loss: 2.9290\n",
            "Epoch 1/1, Batch 39120/98059, Loss: 3.2533\n",
            "Epoch 1/1, Batch 39140/98059, Loss: 3.1388\n",
            "Epoch 1/1, Batch 39160/98059, Loss: 2.9668\n",
            "Epoch 1/1, Batch 39180/98059, Loss: 3.2620\n",
            "Epoch 1/1, Batch 39200/98059, Loss: 3.1420\n",
            "Epoch 1/1, Batch 39220/98059, Loss: 2.7008\n",
            "Epoch 1/1, Batch 39240/98059, Loss: 3.3583\n",
            "Epoch 1/1, Batch 39260/98059, Loss: 3.1285\n",
            "Epoch 1/1, Batch 39280/98059, Loss: 2.9105\n",
            "Epoch 1/1, Batch 39300/98059, Loss: 3.3704\n",
            "Epoch 1/1, Batch 39320/98059, Loss: 2.9460\n",
            "Epoch 1/1, Batch 39340/98059, Loss: 3.0989\n",
            "Epoch 1/1, Batch 39360/98059, Loss: 3.1486\n",
            "Epoch 1/1, Batch 39380/98059, Loss: 3.0318\n",
            "Epoch 1/1, Batch 39400/98059, Loss: 3.2356\n",
            "Epoch 1/1, Batch 39420/98059, Loss: 3.4468\n",
            "Epoch 1/1, Batch 39440/98059, Loss: 3.2174\n",
            "Epoch 1/1, Batch 39460/98059, Loss: 2.9405\n",
            "Epoch 1/1, Batch 39480/98059, Loss: 2.8928\n",
            "Epoch 1/1, Batch 39500/98059, Loss: 3.1840\n",
            "Epoch 1/1, Batch 39520/98059, Loss: 3.1686\n",
            "Epoch 1/1, Batch 39540/98059, Loss: 2.9382\n",
            "Epoch 1/1, Batch 39560/98059, Loss: 2.9966\n",
            "Epoch 1/1, Batch 39580/98059, Loss: 3.0139\n",
            "Epoch 1/1, Batch 39600/98059, Loss: 3.1296\n",
            "Epoch 1/1, Batch 39620/98059, Loss: 3.8828\n",
            "Epoch 1/1, Batch 39640/98059, Loss: 2.8269\n",
            "Epoch 1/1, Batch 39660/98059, Loss: 3.3670\n",
            "Epoch 1/1, Batch 39680/98059, Loss: 3.2636\n",
            "Epoch 1/1, Batch 39700/98059, Loss: 3.1497\n",
            "Epoch 1/1, Batch 39720/98059, Loss: 2.9195\n",
            "Epoch 1/1, Batch 39740/98059, Loss: 3.0295\n",
            "Epoch 1/1, Batch 39760/98059, Loss: 3.2384\n",
            "Epoch 1/1, Batch 39780/98059, Loss: 3.0817\n",
            "Epoch 1/1, Batch 39800/98059, Loss: 3.1872\n",
            "Epoch 1/1, Batch 39820/98059, Loss: 2.9490\n",
            "Epoch 1/1, Batch 39840/98059, Loss: 3.2709\n",
            "Epoch 1/1, Batch 39860/98059, Loss: 2.8874\n",
            "Epoch 1/1, Batch 39880/98059, Loss: 3.3436\n",
            "Epoch 1/1, Batch 39900/98059, Loss: 3.0626\n",
            "Epoch 1/1, Batch 39920/98059, Loss: 3.3816\n",
            "Epoch 1/1, Batch 39940/98059, Loss: 2.8984\n",
            "Epoch 1/1, Batch 39960/98059, Loss: 3.2080\n",
            "Epoch 1/1, Batch 39980/98059, Loss: 2.7022\n",
            "Epoch 1/1, Batch 40000/98059, Loss: 3.0589\n",
            "Epoch 1/1, Batch 40020/98059, Loss: 3.3153\n",
            "Epoch 1/1, Batch 40040/98059, Loss: 2.8335\n",
            "Epoch 1/1, Batch 40060/98059, Loss: 2.8654\n",
            "Epoch 1/1, Batch 40080/98059, Loss: 3.2204\n",
            "Epoch 1/1, Batch 40100/98059, Loss: 3.0380\n",
            "Epoch 1/1, Batch 40120/98059, Loss: 2.7821\n",
            "Epoch 1/1, Batch 40140/98059, Loss: 3.1306\n",
            "Epoch 1/1, Batch 40160/98059, Loss: 3.5324\n",
            "Epoch 1/1, Batch 40180/98059, Loss: 3.4550\n",
            "Epoch 1/1, Batch 40200/98059, Loss: 3.3257\n",
            "Epoch 1/1, Batch 40220/98059, Loss: 3.2510\n",
            "Epoch 1/1, Batch 40240/98059, Loss: 3.4774\n",
            "Epoch 1/1, Batch 40260/98059, Loss: 3.3695\n",
            "Epoch 1/1, Batch 40280/98059, Loss: 3.2862\n",
            "Epoch 1/1, Batch 40300/98059, Loss: 2.8068\n",
            "Epoch 1/1, Batch 40320/98059, Loss: 3.4570\n",
            "Epoch 1/1, Batch 40340/98059, Loss: 3.1339\n",
            "Epoch 1/1, Batch 40360/98059, Loss: 3.3786\n",
            "Epoch 1/1, Batch 40380/98059, Loss: 2.9531\n",
            "Epoch 1/1, Batch 40400/98059, Loss: 2.6532\n",
            "Epoch 1/1, Batch 40420/98059, Loss: 3.2802\n",
            "Epoch 1/1, Batch 40440/98059, Loss: 3.0336\n",
            "Epoch 1/1, Batch 40460/98059, Loss: 2.7092\n",
            "Epoch 1/1, Batch 40480/98059, Loss: 3.1681\n",
            "Epoch 1/1, Batch 40500/98059, Loss: 2.8728\n",
            "Epoch 1/1, Batch 40520/98059, Loss: 3.0703\n",
            "Epoch 1/1, Batch 40540/98059, Loss: 3.1452\n",
            "Epoch 1/1, Batch 40560/98059, Loss: 3.1444\n",
            "Epoch 1/1, Batch 40580/98059, Loss: 2.6802\n",
            "Epoch 1/1, Batch 40600/98059, Loss: 3.3576\n",
            "Epoch 1/1, Batch 40620/98059, Loss: 3.2292\n",
            "Epoch 1/1, Batch 40640/98059, Loss: 3.4663\n",
            "Epoch 1/1, Batch 40660/98059, Loss: 2.8293\n",
            "Epoch 1/1, Batch 40680/98059, Loss: 3.4505\n",
            "Epoch 1/1, Batch 40700/98059, Loss: 2.7672\n",
            "Epoch 1/1, Batch 40720/98059, Loss: 3.2274\n",
            "Epoch 1/1, Batch 40740/98059, Loss: 2.8699\n",
            "Epoch 1/1, Batch 40760/98059, Loss: 3.0924\n",
            "Epoch 1/1, Batch 40780/98059, Loss: 3.2918\n",
            "Epoch 1/1, Batch 40800/98059, Loss: 3.3408\n",
            "Epoch 1/1, Batch 40820/98059, Loss: 2.9234\n",
            "Epoch 1/1, Batch 40840/98059, Loss: 3.2339\n",
            "Epoch 1/1, Batch 40860/98059, Loss: 3.1621\n",
            "Epoch 1/1, Batch 40880/98059, Loss: 3.0390\n",
            "Epoch 1/1, Batch 40900/98059, Loss: 3.0120\n",
            "Epoch 1/1, Batch 40920/98059, Loss: 2.7270\n",
            "Epoch 1/1, Batch 40940/98059, Loss: 2.8588\n",
            "Epoch 1/1, Batch 40960/98059, Loss: 3.1432\n",
            "Epoch 1/1, Batch 40980/98059, Loss: 2.9211\n",
            "Epoch 1/1, Batch 41000/98059, Loss: 2.8441\n",
            "Epoch 1/1, Batch 41020/98059, Loss: 3.2711\n",
            "Epoch 1/1, Batch 41040/98059, Loss: 3.1499\n",
            "Epoch 1/1, Batch 41060/98059, Loss: 3.3601\n",
            "Epoch 1/1, Batch 41080/98059, Loss: 2.9639\n",
            "Epoch 1/1, Batch 41100/98059, Loss: 3.2738\n",
            "Epoch 1/1, Batch 41120/98059, Loss: 3.1503\n",
            "Epoch 1/1, Batch 41140/98059, Loss: 3.0230\n",
            "Epoch 1/1, Batch 41160/98059, Loss: 3.1184\n",
            "Epoch 1/1, Batch 41180/98059, Loss: 2.7309\n",
            "Epoch 1/1, Batch 41200/98059, Loss: 3.6576\n",
            "Epoch 1/1, Batch 41220/98059, Loss: 2.7539\n",
            "Epoch 1/1, Batch 41240/98059, Loss: 2.8913\n",
            "Epoch 1/1, Batch 41260/98059, Loss: 3.0600\n",
            "Epoch 1/1, Batch 41280/98059, Loss: 3.0412\n",
            "Epoch 1/1, Batch 41300/98059, Loss: 2.8336\n",
            "Epoch 1/1, Batch 41320/98059, Loss: 3.2160\n",
            "Epoch 1/1, Batch 41340/98059, Loss: 3.2615\n",
            "Epoch 1/1, Batch 41360/98059, Loss: 3.2734\n",
            "Epoch 1/1, Batch 41380/98059, Loss: 2.9867\n",
            "Epoch 1/1, Batch 41400/98059, Loss: 3.3065\n",
            "Epoch 1/1, Batch 41420/98059, Loss: 2.5833\n",
            "Epoch 1/1, Batch 41440/98059, Loss: 2.9622\n",
            "Epoch 1/1, Batch 41460/98059, Loss: 3.2613\n",
            "Epoch 1/1, Batch 41480/98059, Loss: 3.2902\n",
            "Epoch 1/1, Batch 41500/98059, Loss: 3.3241\n",
            "Epoch 1/1, Batch 41520/98059, Loss: 3.2174\n",
            "Epoch 1/1, Batch 41540/98059, Loss: 3.2158\n",
            "Epoch 1/1, Batch 41560/98059, Loss: 3.3513\n",
            "Epoch 1/1, Batch 41580/98059, Loss: 3.1645\n",
            "Epoch 1/1, Batch 41600/98059, Loss: 3.2824\n",
            "Epoch 1/1, Batch 41620/98059, Loss: 3.2861\n",
            "Epoch 1/1, Batch 41640/98059, Loss: 3.1763\n",
            "Epoch 1/1, Batch 41660/98059, Loss: 2.8050\n",
            "Epoch 1/1, Batch 41680/98059, Loss: 3.1562\n",
            "Epoch 1/1, Batch 41700/98059, Loss: 3.1183\n",
            "Epoch 1/1, Batch 41720/98059, Loss: 2.3945\n",
            "Epoch 1/1, Batch 41740/98059, Loss: 3.1151\n",
            "Epoch 1/1, Batch 41760/98059, Loss: 3.0339\n",
            "Epoch 1/1, Batch 41780/98059, Loss: 2.8389\n",
            "Epoch 1/1, Batch 41800/98059, Loss: 3.2667\n",
            "Epoch 1/1, Batch 41820/98059, Loss: 2.8535\n",
            "Epoch 1/1, Batch 41840/98059, Loss: 3.3086\n",
            "Epoch 1/1, Batch 41860/98059, Loss: 3.1416\n",
            "Epoch 1/1, Batch 41880/98059, Loss: 3.3123\n",
            "Epoch 1/1, Batch 41900/98059, Loss: 3.0018\n",
            "Epoch 1/1, Batch 41920/98059, Loss: 2.9657\n",
            "Epoch 1/1, Batch 41940/98059, Loss: 3.1151\n",
            "Epoch 1/1, Batch 41960/98059, Loss: 3.3342\n",
            "Epoch 1/1, Batch 41980/98059, Loss: 3.3235\n",
            "Epoch 1/1, Batch 42000/98059, Loss: 2.9210\n",
            "Epoch 1/1, Batch 42020/98059, Loss: 3.2532\n",
            "Epoch 1/1, Batch 42040/98059, Loss: 3.1519\n",
            "Epoch 1/1, Batch 42060/98059, Loss: 3.1795\n",
            "Epoch 1/1, Batch 42080/98059, Loss: 3.4280\n",
            "Epoch 1/1, Batch 42100/98059, Loss: 3.0757\n",
            "Epoch 1/1, Batch 42120/98059, Loss: 2.9706\n",
            "Epoch 1/1, Batch 42140/98059, Loss: 3.3727\n",
            "Epoch 1/1, Batch 42160/98059, Loss: 2.9580\n",
            "Epoch 1/1, Batch 42180/98059, Loss: 3.0224\n",
            "Epoch 1/1, Batch 42200/98059, Loss: 2.8703\n",
            "Epoch 1/1, Batch 42220/98059, Loss: 2.9714\n",
            "Epoch 1/1, Batch 42240/98059, Loss: 3.1027\n",
            "Epoch 1/1, Batch 42260/98059, Loss: 3.0940\n",
            "Epoch 1/1, Batch 42280/98059, Loss: 2.9902\n",
            "Epoch 1/1, Batch 42300/98059, Loss: 2.8829\n",
            "Epoch 1/1, Batch 42320/98059, Loss: 2.8741\n",
            "Epoch 1/1, Batch 42340/98059, Loss: 2.7140\n",
            "Epoch 1/1, Batch 42360/98059, Loss: 2.8534\n",
            "Epoch 1/1, Batch 42380/98059, Loss: 3.3907\n",
            "Epoch 1/1, Batch 42400/98059, Loss: 2.9388\n",
            "Epoch 1/1, Batch 42420/98059, Loss: 3.3687\n",
            "Epoch 1/1, Batch 42440/98059, Loss: 3.4357\n",
            "Epoch 1/1, Batch 42460/98059, Loss: 2.9688\n",
            "Epoch 1/1, Batch 42480/98059, Loss: 3.0565\n",
            "Epoch 1/1, Batch 42500/98059, Loss: 3.2110\n",
            "Epoch 1/1, Batch 42520/98059, Loss: 2.7722\n",
            "Epoch 1/1, Batch 42540/98059, Loss: 2.8484\n",
            "Epoch 1/1, Batch 42560/98059, Loss: 2.9552\n",
            "Epoch 1/1, Batch 42580/98059, Loss: 3.0360\n",
            "Epoch 1/1, Batch 42600/98059, Loss: 3.2454\n",
            "Epoch 1/1, Batch 42620/98059, Loss: 3.1670\n",
            "Epoch 1/1, Batch 42640/98059, Loss: 3.2291\n",
            "Epoch 1/1, Batch 42660/98059, Loss: 3.0476\n",
            "Epoch 1/1, Batch 42680/98059, Loss: 3.0324\n",
            "Epoch 1/1, Batch 42700/98059, Loss: 3.1224\n",
            "Epoch 1/1, Batch 42720/98059, Loss: 2.9958\n",
            "Epoch 1/1, Batch 42740/98059, Loss: 3.3970\n",
            "Epoch 1/1, Batch 42760/98059, Loss: 3.0468\n",
            "Epoch 1/1, Batch 42780/98059, Loss: 3.1774\n",
            "Epoch 1/1, Batch 42800/98059, Loss: 3.2988\n",
            "Epoch 1/1, Batch 42820/98059, Loss: 3.0579\n",
            "Epoch 1/1, Batch 42840/98059, Loss: 2.9534\n",
            "Epoch 1/1, Batch 42860/98059, Loss: 3.1693\n",
            "Epoch 1/1, Batch 42880/98059, Loss: 3.4717\n",
            "Epoch 1/1, Batch 42900/98059, Loss: 3.2446\n",
            "Epoch 1/1, Batch 42920/98059, Loss: 2.9631\n",
            "Epoch 1/1, Batch 42940/98059, Loss: 2.9579\n",
            "Epoch 1/1, Batch 42960/98059, Loss: 3.2687\n",
            "Epoch 1/1, Batch 42980/98059, Loss: 3.1789\n",
            "Epoch 1/1, Batch 43000/98059, Loss: 2.9072\n",
            "Epoch 1/1, Batch 43020/98059, Loss: 3.3610\n",
            "Epoch 1/1, Batch 43040/98059, Loss: 3.0423\n",
            "Epoch 1/1, Batch 43060/98059, Loss: 2.7277\n",
            "Epoch 1/1, Batch 43080/98059, Loss: 2.9580\n",
            "Epoch 1/1, Batch 43100/98059, Loss: 2.9346\n",
            "Epoch 1/1, Batch 43120/98059, Loss: 3.0753\n",
            "Epoch 1/1, Batch 43140/98059, Loss: 3.1727\n",
            "Epoch 1/1, Batch 43160/98059, Loss: 2.9201\n",
            "Epoch 1/1, Batch 43180/98059, Loss: 2.8731\n",
            "Epoch 1/1, Batch 43200/98059, Loss: 2.9563\n",
            "Epoch 1/1, Batch 43220/98059, Loss: 2.7508\n",
            "Epoch 1/1, Batch 43240/98059, Loss: 3.1537\n",
            "Epoch 1/1, Batch 43260/98059, Loss: 3.0896\n",
            "Epoch 1/1, Batch 43280/98059, Loss: 2.8896\n",
            "Epoch 1/1, Batch 43300/98059, Loss: 2.9949\n",
            "Epoch 1/1, Batch 43320/98059, Loss: 2.7820\n",
            "Epoch 1/1, Batch 43340/98059, Loss: 3.2577\n",
            "Epoch 1/1, Batch 43360/98059, Loss: 3.1201\n",
            "Epoch 1/1, Batch 43380/98059, Loss: 3.0463\n",
            "Epoch 1/1, Batch 43400/98059, Loss: 3.5001\n",
            "Epoch 1/1, Batch 43420/98059, Loss: 2.8978\n",
            "Epoch 1/1, Batch 43440/98059, Loss: 3.1290\n",
            "Epoch 1/1, Batch 43460/98059, Loss: 3.0710\n",
            "Epoch 1/1, Batch 43480/98059, Loss: 2.8598\n",
            "Epoch 1/1, Batch 43500/98059, Loss: 2.7968\n",
            "Epoch 1/1, Batch 43520/98059, Loss: 3.1906\n",
            "Epoch 1/1, Batch 43540/98059, Loss: 2.9267\n",
            "Epoch 1/1, Batch 43560/98059, Loss: 3.0803\n",
            "Epoch 1/1, Batch 43580/98059, Loss: 3.2489\n",
            "Epoch 1/1, Batch 43600/98059, Loss: 2.7660\n",
            "Epoch 1/1, Batch 43620/98059, Loss: 3.1035\n",
            "Epoch 1/1, Batch 43640/98059, Loss: 3.1146\n",
            "Epoch 1/1, Batch 43660/98059, Loss: 3.3020\n",
            "Epoch 1/1, Batch 43680/98059, Loss: 3.2353\n",
            "Epoch 1/1, Batch 43700/98059, Loss: 3.0482\n",
            "Epoch 1/1, Batch 43720/98059, Loss: 2.8986\n",
            "Epoch 1/1, Batch 43740/98059, Loss: 3.0006\n",
            "Epoch 1/1, Batch 43760/98059, Loss: 2.7497\n",
            "Epoch 1/1, Batch 43780/98059, Loss: 3.0021\n",
            "Epoch 1/1, Batch 43800/98059, Loss: 3.4757\n",
            "Epoch 1/1, Batch 43820/98059, Loss: 2.9271\n",
            "Epoch 1/1, Batch 43840/98059, Loss: 3.1586\n",
            "Epoch 1/1, Batch 43860/98059, Loss: 3.0203\n",
            "Epoch 1/1, Batch 43880/98059, Loss: 3.1795\n",
            "Epoch 1/1, Batch 43900/98059, Loss: 3.2009\n",
            "Epoch 1/1, Batch 43920/98059, Loss: 3.3658\n",
            "Epoch 1/1, Batch 43940/98059, Loss: 3.3589\n",
            "Epoch 1/1, Batch 43960/98059, Loss: 2.9349\n",
            "Epoch 1/1, Batch 43980/98059, Loss: 3.0527\n",
            "Epoch 1/1, Batch 44000/98059, Loss: 2.9534\n",
            "Epoch 1/1, Batch 44020/98059, Loss: 2.5610\n",
            "Epoch 1/1, Batch 44040/98059, Loss: 3.2293\n",
            "Epoch 1/1, Batch 44060/98059, Loss: 3.0545\n",
            "Epoch 1/1, Batch 44080/98059, Loss: 3.1281\n",
            "Epoch 1/1, Batch 44100/98059, Loss: 3.1034\n",
            "Epoch 1/1, Batch 44120/98059, Loss: 2.8154\n",
            "Epoch 1/1, Batch 44140/98059, Loss: 3.2626\n",
            "Epoch 1/1, Batch 44160/98059, Loss: 3.0580\n",
            "Epoch 1/1, Batch 44180/98059, Loss: 3.3683\n",
            "Epoch 1/1, Batch 44200/98059, Loss: 3.1949\n",
            "Epoch 1/1, Batch 44220/98059, Loss: 3.1433\n",
            "Epoch 1/1, Batch 44240/98059, Loss: 2.9373\n",
            "Epoch 1/1, Batch 44260/98059, Loss: 2.9228\n",
            "Epoch 1/1, Batch 44280/98059, Loss: 3.0434\n",
            "Epoch 1/1, Batch 44300/98059, Loss: 2.9642\n",
            "Epoch 1/1, Batch 44320/98059, Loss: 3.2242\n",
            "Epoch 1/1, Batch 44340/98059, Loss: 3.3411\n",
            "Epoch 1/1, Batch 44360/98059, Loss: 3.0367\n",
            "Epoch 1/1, Batch 44380/98059, Loss: 2.6717\n",
            "Epoch 1/1, Batch 44400/98059, Loss: 3.4419\n",
            "Epoch 1/1, Batch 44420/98059, Loss: 3.3148\n",
            "Epoch 1/1, Batch 44440/98059, Loss: 3.1027\n",
            "Epoch 1/1, Batch 44460/98059, Loss: 3.2225\n",
            "Epoch 1/1, Batch 44480/98059, Loss: 2.9082\n",
            "Epoch 1/1, Batch 44500/98059, Loss: 3.1521\n",
            "Epoch 1/1, Batch 44520/98059, Loss: 3.1013\n",
            "Epoch 1/1, Batch 44540/98059, Loss: 3.0918\n",
            "Epoch 1/1, Batch 44560/98059, Loss: 3.1215\n",
            "Epoch 1/1, Batch 44580/98059, Loss: 2.9606\n",
            "Epoch 1/1, Batch 44600/98059, Loss: 3.1239\n",
            "Epoch 1/1, Batch 44620/98059, Loss: 3.2961\n",
            "Epoch 1/1, Batch 44640/98059, Loss: 2.9008\n",
            "Epoch 1/1, Batch 44660/98059, Loss: 3.2126\n",
            "Epoch 1/1, Batch 44680/98059, Loss: 3.2393\n",
            "Epoch 1/1, Batch 44700/98059, Loss: 3.1287\n",
            "Epoch 1/1, Batch 44720/98059, Loss: 2.9285\n",
            "Epoch 1/1, Batch 44740/98059, Loss: 3.2340\n",
            "Epoch 1/1, Batch 44760/98059, Loss: 2.9948\n",
            "Epoch 1/1, Batch 44780/98059, Loss: 3.1216\n",
            "Epoch 1/1, Batch 44800/98059, Loss: 3.2769\n",
            "Epoch 1/1, Batch 44820/98059, Loss: 2.7695\n",
            "Epoch 1/1, Batch 44840/98059, Loss: 2.7855\n",
            "Epoch 1/1, Batch 44860/98059, Loss: 2.9184\n",
            "Epoch 1/1, Batch 44880/98059, Loss: 3.0719\n",
            "Epoch 1/1, Batch 44900/98059, Loss: 3.1759\n",
            "Epoch 1/1, Batch 44920/98059, Loss: 2.8707\n",
            "Epoch 1/1, Batch 44940/98059, Loss: 3.1189\n",
            "Epoch 1/1, Batch 44960/98059, Loss: 3.0447\n",
            "Epoch 1/1, Batch 44980/98059, Loss: 2.6845\n",
            "Epoch 1/1, Batch 45000/98059, Loss: 3.4070\n",
            "Epoch 1/1, Batch 45020/98059, Loss: 3.0177\n",
            "Epoch 1/1, Batch 45040/98059, Loss: 3.2694\n",
            "Epoch 1/1, Batch 45060/98059, Loss: 3.0263\n",
            "Epoch 1/1, Batch 45080/98059, Loss: 2.9737\n",
            "Epoch 1/1, Batch 45100/98059, Loss: 2.8664\n",
            "Epoch 1/1, Batch 45120/98059, Loss: 3.2453\n",
            "Epoch 1/1, Batch 45140/98059, Loss: 2.9750\n",
            "Epoch 1/1, Batch 45160/98059, Loss: 3.0873\n",
            "Epoch 1/1, Batch 45180/98059, Loss: 2.8436\n",
            "Epoch 1/1, Batch 45200/98059, Loss: 3.0320\n",
            "Epoch 1/1, Batch 45220/98059, Loss: 3.4286\n",
            "Epoch 1/1, Batch 45240/98059, Loss: 3.0636\n",
            "Epoch 1/1, Batch 45260/98059, Loss: 3.1140\n",
            "Epoch 1/1, Batch 45280/98059, Loss: 2.5475\n",
            "Epoch 1/1, Batch 45300/98059, Loss: 3.1901\n",
            "Epoch 1/1, Batch 45320/98059, Loss: 3.0511\n",
            "Epoch 1/1, Batch 45340/98059, Loss: 3.4194\n",
            "Epoch 1/1, Batch 45360/98059, Loss: 3.1921\n",
            "Epoch 1/1, Batch 45380/98059, Loss: 3.0157\n",
            "Epoch 1/1, Batch 45400/98059, Loss: 2.9532\n",
            "Epoch 1/1, Batch 45420/98059, Loss: 3.1731\n",
            "Epoch 1/1, Batch 45440/98059, Loss: 2.8184\n",
            "Epoch 1/1, Batch 45460/98059, Loss: 2.7688\n",
            "Epoch 1/1, Batch 45480/98059, Loss: 3.1301\n",
            "Epoch 1/1, Batch 45500/98059, Loss: 2.8511\n",
            "Epoch 1/1, Batch 45520/98059, Loss: 2.9470\n",
            "Epoch 1/1, Batch 45540/98059, Loss: 3.3917\n",
            "Epoch 1/1, Batch 45560/98059, Loss: 3.3153\n",
            "Epoch 1/1, Batch 45580/98059, Loss: 3.0671\n",
            "Epoch 1/1, Batch 45600/98059, Loss: 3.2773\n",
            "Epoch 1/1, Batch 45620/98059, Loss: 3.1648\n",
            "Epoch 1/1, Batch 45640/98059, Loss: 2.9527\n",
            "Epoch 1/1, Batch 45660/98059, Loss: 3.0088\n",
            "Epoch 1/1, Batch 45680/98059, Loss: 3.1077\n",
            "Epoch 1/1, Batch 45700/98059, Loss: 2.6870\n",
            "Epoch 1/1, Batch 45720/98059, Loss: 3.2863\n",
            "Epoch 1/1, Batch 45740/98059, Loss: 3.5478\n",
            "Epoch 1/1, Batch 45760/98059, Loss: 2.6091\n",
            "Epoch 1/1, Batch 45780/98059, Loss: 3.1181\n",
            "Epoch 1/1, Batch 45800/98059, Loss: 2.8707\n",
            "Epoch 1/1, Batch 45820/98059, Loss: 2.7481\n",
            "Epoch 1/1, Batch 45840/98059, Loss: 2.7767\n",
            "Epoch 1/1, Batch 45860/98059, Loss: 3.2489\n",
            "Epoch 1/1, Batch 45880/98059, Loss: 3.1590\n",
            "Epoch 1/1, Batch 45900/98059, Loss: 3.2140\n",
            "Epoch 1/1, Batch 45920/98059, Loss: 2.9912\n",
            "Epoch 1/1, Batch 45940/98059, Loss: 2.9766\n",
            "Epoch 1/1, Batch 45960/98059, Loss: 3.3895\n",
            "Epoch 1/1, Batch 45980/98059, Loss: 2.9510\n",
            "Epoch 1/1, Batch 46000/98059, Loss: 3.1636\n",
            "Epoch 1/1, Batch 46020/98059, Loss: 3.2162\n",
            "Epoch 1/1, Batch 46040/98059, Loss: 2.9539\n",
            "Epoch 1/1, Batch 46060/98059, Loss: 3.2456\n",
            "Epoch 1/1, Batch 46080/98059, Loss: 3.1866\n",
            "Epoch 1/1, Batch 46100/98059, Loss: 3.2219\n",
            "Epoch 1/1, Batch 46120/98059, Loss: 2.7141\n",
            "Epoch 1/1, Batch 46140/98059, Loss: 3.0594\n",
            "Epoch 1/1, Batch 46160/98059, Loss: 2.9138\n",
            "Epoch 1/1, Batch 46180/98059, Loss: 2.9489\n",
            "Epoch 1/1, Batch 46200/98059, Loss: 3.2681\n",
            "Epoch 1/1, Batch 46220/98059, Loss: 2.8690\n",
            "Epoch 1/1, Batch 46240/98059, Loss: 2.9201\n",
            "Epoch 1/1, Batch 46260/98059, Loss: 3.0918\n",
            "Epoch 1/1, Batch 46280/98059, Loss: 3.0083\n",
            "Epoch 1/1, Batch 46300/98059, Loss: 2.7941\n",
            "Epoch 1/1, Batch 46320/98059, Loss: 3.0537\n",
            "Epoch 1/1, Batch 46340/98059, Loss: 3.0294\n",
            "Epoch 1/1, Batch 46360/98059, Loss: 3.0260\n",
            "Epoch 1/1, Batch 46380/98059, Loss: 3.0413\n",
            "Epoch 1/1, Batch 46400/98059, Loss: 3.4650\n",
            "Epoch 1/1, Batch 46420/98059, Loss: 2.8728\n",
            "Epoch 1/1, Batch 46440/98059, Loss: 2.9582\n",
            "Epoch 1/1, Batch 46460/98059, Loss: 3.1884\n",
            "Epoch 1/1, Batch 46480/98059, Loss: 3.0627\n",
            "Epoch 1/1, Batch 46500/98059, Loss: 3.0982\n",
            "Epoch 1/1, Batch 46520/98059, Loss: 3.2306\n",
            "Epoch 1/1, Batch 46540/98059, Loss: 2.9765\n",
            "Epoch 1/1, Batch 46560/98059, Loss: 3.0710\n",
            "Epoch 1/1, Batch 46580/98059, Loss: 2.9851\n",
            "Epoch 1/1, Batch 46600/98059, Loss: 3.1549\n",
            "Epoch 1/1, Batch 46620/98059, Loss: 2.9467\n",
            "Epoch 1/1, Batch 46640/98059, Loss: 2.7942\n",
            "Epoch 1/1, Batch 46660/98059, Loss: 3.5452\n",
            "Epoch 1/1, Batch 46680/98059, Loss: 2.9999\n",
            "Epoch 1/1, Batch 46700/98059, Loss: 3.4982\n",
            "Epoch 1/1, Batch 46720/98059, Loss: 2.9160\n",
            "Epoch 1/1, Batch 46740/98059, Loss: 3.0882\n",
            "Epoch 1/1, Batch 46760/98059, Loss: 3.1302\n",
            "Epoch 1/1, Batch 46780/98059, Loss: 3.3900\n",
            "Epoch 1/1, Batch 46800/98059, Loss: 3.0485\n",
            "Epoch 1/1, Batch 46820/98059, Loss: 3.0060\n",
            "Epoch 1/1, Batch 46840/98059, Loss: 3.3249\n",
            "Epoch 1/1, Batch 46860/98059, Loss: 2.9060\n",
            "Epoch 1/1, Batch 46880/98059, Loss: 3.0797\n",
            "Epoch 1/1, Batch 46900/98059, Loss: 2.7506\n",
            "Epoch 1/1, Batch 46920/98059, Loss: 3.2728\n",
            "Epoch 1/1, Batch 46940/98059, Loss: 3.0926\n",
            "Epoch 1/1, Batch 46960/98059, Loss: 2.7960\n",
            "Epoch 1/1, Batch 46980/98059, Loss: 3.1974\n",
            "Epoch 1/1, Batch 47000/98059, Loss: 3.1485\n",
            "Epoch 1/1, Batch 47020/98059, Loss: 2.8182\n",
            "Epoch 1/1, Batch 47040/98059, Loss: 2.9870\n",
            "Epoch 1/1, Batch 47060/98059, Loss: 3.0750\n",
            "Epoch 1/1, Batch 47080/98059, Loss: 2.8780\n",
            "Epoch 1/1, Batch 47100/98059, Loss: 3.5576\n",
            "Epoch 1/1, Batch 47120/98059, Loss: 3.0051\n",
            "Epoch 1/1, Batch 47140/98059, Loss: 3.5479\n",
            "Epoch 1/1, Batch 47160/98059, Loss: 3.0392\n",
            "Epoch 1/1, Batch 47180/98059, Loss: 2.8841\n",
            "Epoch 1/1, Batch 47200/98059, Loss: 2.8848\n",
            "Epoch 1/1, Batch 47220/98059, Loss: 2.6691\n",
            "Epoch 1/1, Batch 47240/98059, Loss: 2.9400\n",
            "Epoch 1/1, Batch 47260/98059, Loss: 3.2008\n",
            "Epoch 1/1, Batch 47280/98059, Loss: 3.1985\n",
            "Epoch 1/1, Batch 47300/98059, Loss: 3.0883\n",
            "Epoch 1/1, Batch 47320/98059, Loss: 2.8803\n",
            "Epoch 1/1, Batch 47340/98059, Loss: 3.3496\n",
            "Epoch 1/1, Batch 47360/98059, Loss: 2.6154\n",
            "Epoch 1/1, Batch 47380/98059, Loss: 2.7886\n",
            "Epoch 1/1, Batch 47400/98059, Loss: 2.8498\n",
            "Epoch 1/1, Batch 47420/98059, Loss: 2.9791\n",
            "Epoch 1/1, Batch 47440/98059, Loss: 3.0674\n",
            "Epoch 1/1, Batch 47460/98059, Loss: 3.0267\n",
            "Epoch 1/1, Batch 47480/98059, Loss: 3.1281\n",
            "Epoch 1/1, Batch 47500/98059, Loss: 3.0344\n",
            "Epoch 1/1, Batch 47520/98059, Loss: 3.2246\n",
            "Epoch 1/1, Batch 47540/98059, Loss: 2.8887\n",
            "Epoch 1/1, Batch 47560/98059, Loss: 3.1760\n",
            "Epoch 1/1, Batch 47580/98059, Loss: 3.1174\n",
            "Epoch 1/1, Batch 47600/98059, Loss: 3.0759\n",
            "Epoch 1/1, Batch 47620/98059, Loss: 3.0419\n",
            "Epoch 1/1, Batch 47640/98059, Loss: 3.2102\n",
            "Epoch 1/1, Batch 47660/98059, Loss: 3.5198\n",
            "Epoch 1/1, Batch 47680/98059, Loss: 3.0126\n",
            "Epoch 1/1, Batch 47700/98059, Loss: 2.9488\n",
            "Epoch 1/1, Batch 47720/98059, Loss: 3.1530\n",
            "Epoch 1/1, Batch 47740/98059, Loss: 2.8512\n",
            "Epoch 1/1, Batch 47760/98059, Loss: 3.1767\n",
            "Epoch 1/1, Batch 47780/98059, Loss: 3.0387\n",
            "Epoch 1/1, Batch 47800/98059, Loss: 2.9665\n",
            "Epoch 1/1, Batch 47820/98059, Loss: 3.0736\n",
            "Epoch 1/1, Batch 47840/98059, Loss: 3.1162\n",
            "Epoch 1/1, Batch 47860/98059, Loss: 2.8396\n",
            "Epoch 1/1, Batch 47880/98059, Loss: 3.2668\n",
            "Epoch 1/1, Batch 47900/98059, Loss: 2.9232\n",
            "Epoch 1/1, Batch 47920/98059, Loss: 2.9224\n",
            "Epoch 1/1, Batch 47940/98059, Loss: 3.3560\n",
            "Epoch 1/1, Batch 47960/98059, Loss: 3.0831\n",
            "Epoch 1/1, Batch 47980/98059, Loss: 3.0701\n",
            "Epoch 1/1, Batch 48000/98059, Loss: 3.3389\n",
            "Epoch 1/1, Batch 48020/98059, Loss: 2.9511\n",
            "Epoch 1/1, Batch 48040/98059, Loss: 3.0029\n",
            "Epoch 1/1, Batch 48060/98059, Loss: 3.2102\n",
            "Epoch 1/1, Batch 48080/98059, Loss: 3.1663\n",
            "Epoch 1/1, Batch 48100/98059, Loss: 3.4549\n",
            "Epoch 1/1, Batch 48120/98059, Loss: 2.9763\n",
            "Epoch 1/1, Batch 48140/98059, Loss: 2.7366\n",
            "Epoch 1/1, Batch 48160/98059, Loss: 3.2526\n",
            "Epoch 1/1, Batch 48180/98059, Loss: 3.0586\n",
            "Epoch 1/1, Batch 48200/98059, Loss: 2.9927\n",
            "Epoch 1/1, Batch 48220/98059, Loss: 3.1469\n",
            "Epoch 1/1, Batch 48240/98059, Loss: 2.6593\n",
            "Epoch 1/1, Batch 48260/98059, Loss: 2.9531\n",
            "Epoch 1/1, Batch 48280/98059, Loss: 3.0251\n",
            "Epoch 1/1, Batch 48300/98059, Loss: 2.8838\n",
            "Epoch 1/1, Batch 48320/98059, Loss: 2.9449\n",
            "Epoch 1/1, Batch 48340/98059, Loss: 2.8702\n",
            "Epoch 1/1, Batch 48360/98059, Loss: 3.0926\n",
            "Epoch 1/1, Batch 48380/98059, Loss: 3.2803\n",
            "Epoch 1/1, Batch 48400/98059, Loss: 3.6044\n",
            "Epoch 1/1, Batch 48420/98059, Loss: 2.9816\n",
            "Epoch 1/1, Batch 48440/98059, Loss: 3.0602\n",
            "Epoch 1/1, Batch 48460/98059, Loss: 3.0982\n",
            "Epoch 1/1, Batch 48480/98059, Loss: 3.0876\n",
            "Epoch 1/1, Batch 48500/98059, Loss: 2.7271\n",
            "Epoch 1/1, Batch 48520/98059, Loss: 2.9141\n",
            "Epoch 1/1, Batch 48540/98059, Loss: 3.0355\n",
            "Epoch 1/1, Batch 48560/98059, Loss: 2.9285\n",
            "Epoch 1/1, Batch 48580/98059, Loss: 3.1528\n",
            "Epoch 1/1, Batch 48600/98059, Loss: 3.0833\n",
            "Epoch 1/1, Batch 48620/98059, Loss: 3.1527\n",
            "Epoch 1/1, Batch 48640/98059, Loss: 2.7191\n",
            "Epoch 1/1, Batch 48660/98059, Loss: 2.8516\n",
            "Epoch 1/1, Batch 48680/98059, Loss: 2.9954\n",
            "Epoch 1/1, Batch 48700/98059, Loss: 2.8870\n",
            "Epoch 1/1, Batch 48720/98059, Loss: 2.8958\n",
            "Epoch 1/1, Batch 48740/98059, Loss: 3.1001\n",
            "Epoch 1/1, Batch 48760/98059, Loss: 2.7422\n",
            "Epoch 1/1, Batch 48780/98059, Loss: 2.9984\n",
            "Epoch 1/1, Batch 48800/98059, Loss: 2.7427\n",
            "Epoch 1/1, Batch 48820/98059, Loss: 3.0325\n",
            "Epoch 1/1, Batch 48840/98059, Loss: 3.2964\n",
            "Epoch 1/1, Batch 48860/98059, Loss: 2.9037\n",
            "Epoch 1/1, Batch 48880/98059, Loss: 3.0590\n",
            "Epoch 1/1, Batch 48900/98059, Loss: 2.8672\n",
            "Epoch 1/1, Batch 48920/98059, Loss: 2.7648\n",
            "Epoch 1/1, Batch 48940/98059, Loss: 3.1359\n",
            "Epoch 1/1, Batch 48960/98059, Loss: 2.5757\n",
            "Epoch 1/1, Batch 48980/98059, Loss: 3.1263\n",
            "Epoch 1/1, Batch 49000/98059, Loss: 3.0682\n",
            "Epoch 1/1, Batch 49020/98059, Loss: 3.0925\n",
            "Epoch 1/1, Batch 49040/98059, Loss: 2.6286\n",
            "Epoch 1/1, Batch 49060/98059, Loss: 3.2729\n",
            "Epoch 1/1, Batch 49080/98059, Loss: 2.8431\n",
            "Epoch 1/1, Batch 49100/98059, Loss: 3.0720\n",
            "Epoch 1/1, Batch 49120/98059, Loss: 3.1238\n",
            "Epoch 1/1, Batch 49140/98059, Loss: 3.2463\n",
            "Epoch 1/1, Batch 49160/98059, Loss: 3.0262\n",
            "Epoch 1/1, Batch 49180/98059, Loss: 3.0027\n",
            "Epoch 1/1, Batch 49200/98059, Loss: 2.9958\n",
            "Epoch 1/1, Batch 49220/98059, Loss: 3.1982\n",
            "Epoch 1/1, Batch 49240/98059, Loss: 2.8327\n",
            "Epoch 1/1, Batch 49260/98059, Loss: 2.9942\n",
            "Epoch 1/1, Batch 49280/98059, Loss: 3.0937\n",
            "Epoch 1/1, Batch 49300/98059, Loss: 2.9758\n",
            "Epoch 1/1, Batch 49320/98059, Loss: 3.1839\n",
            "Epoch 1/1, Batch 49340/98059, Loss: 3.2058\n",
            "Epoch 1/1, Batch 49360/98059, Loss: 3.2950\n",
            "Epoch 1/1, Batch 49380/98059, Loss: 2.5120\n",
            "Epoch 1/1, Batch 49400/98059, Loss: 3.3537\n",
            "Epoch 1/1, Batch 49420/98059, Loss: 3.5465\n",
            "Epoch 1/1, Batch 49440/98059, Loss: 3.0544\n",
            "Epoch 1/1, Batch 49460/98059, Loss: 3.1360\n",
            "Epoch 1/1, Batch 49480/98059, Loss: 3.1162\n",
            "Epoch 1/1, Batch 49500/98059, Loss: 2.6816\n",
            "Epoch 1/1, Batch 49520/98059, Loss: 3.3185\n",
            "Epoch 1/1, Batch 49540/98059, Loss: 2.8910\n",
            "Epoch 1/1, Batch 49560/98059, Loss: 3.1999\n",
            "Epoch 1/1, Batch 49580/98059, Loss: 3.3992\n",
            "Epoch 1/1, Batch 49600/98059, Loss: 2.8756\n",
            "Epoch 1/1, Batch 49620/98059, Loss: 3.0308\n",
            "Epoch 1/1, Batch 49640/98059, Loss: 2.6176\n",
            "Epoch 1/1, Batch 49660/98059, Loss: 2.8969\n",
            "Epoch 1/1, Batch 49680/98059, Loss: 2.8473\n",
            "Epoch 1/1, Batch 49700/98059, Loss: 3.1542\n",
            "Epoch 1/1, Batch 49720/98059, Loss: 3.3312\n",
            "Epoch 1/1, Batch 49740/98059, Loss: 3.0768\n",
            "Epoch 1/1, Batch 49760/98059, Loss: 3.4388\n",
            "Epoch 1/1, Batch 49780/98059, Loss: 3.5288\n",
            "Epoch 1/1, Batch 49800/98059, Loss: 2.8808\n",
            "Epoch 1/1, Batch 49820/98059, Loss: 3.1437\n",
            "Epoch 1/1, Batch 49840/98059, Loss: 3.3627\n",
            "Epoch 1/1, Batch 49860/98059, Loss: 2.9542\n",
            "Epoch 1/1, Batch 49880/98059, Loss: 3.1435\n",
            "Epoch 1/1, Batch 49900/98059, Loss: 3.1717\n",
            "Epoch 1/1, Batch 49920/98059, Loss: 3.3239\n",
            "Epoch 1/1, Batch 49940/98059, Loss: 3.2859\n",
            "Epoch 1/1, Batch 49960/98059, Loss: 3.2459\n",
            "Epoch 1/1, Batch 49980/98059, Loss: 3.4813\n",
            "Epoch 1/1, Batch 50000/98059, Loss: 2.8326\n",
            "Epoch 1/1, Batch 50020/98059, Loss: 3.1077\n",
            "Epoch 1/1, Batch 50040/98059, Loss: 2.8962\n",
            "Epoch 1/1, Batch 50060/98059, Loss: 3.0408\n",
            "Epoch 1/1, Batch 50080/98059, Loss: 3.2132\n",
            "Epoch 1/1, Batch 50100/98059, Loss: 2.9780\n",
            "Epoch 1/1, Batch 50120/98059, Loss: 3.0740\n",
            "Epoch 1/1, Batch 50140/98059, Loss: 3.0222\n",
            "Epoch 1/1, Batch 50160/98059, Loss: 2.9429\n",
            "Epoch 1/1, Batch 50180/98059, Loss: 3.1129\n",
            "Epoch 1/1, Batch 50200/98059, Loss: 2.9594\n",
            "Epoch 1/1, Batch 50220/98059, Loss: 2.8596\n",
            "Epoch 1/1, Batch 50240/98059, Loss: 3.0283\n",
            "Epoch 1/1, Batch 50260/98059, Loss: 2.9682\n",
            "Epoch 1/1, Batch 50280/98059, Loss: 3.2944\n",
            "Epoch 1/1, Batch 50300/98059, Loss: 3.0221\n",
            "Epoch 1/1, Batch 50320/98059, Loss: 2.8692\n",
            "Epoch 1/1, Batch 50340/98059, Loss: 3.1238\n",
            "Epoch 1/1, Batch 50360/98059, Loss: 2.8555\n",
            "Epoch 1/1, Batch 50380/98059, Loss: 3.1042\n",
            "Epoch 1/1, Batch 50400/98059, Loss: 3.0494\n",
            "Epoch 1/1, Batch 50420/98059, Loss: 2.9793\n",
            "Epoch 1/1, Batch 50440/98059, Loss: 3.3383\n",
            "Epoch 1/1, Batch 50460/98059, Loss: 3.3164\n",
            "Epoch 1/1, Batch 50480/98059, Loss: 3.0906\n",
            "Epoch 1/1, Batch 50500/98059, Loss: 3.0324\n",
            "Epoch 1/1, Batch 50520/98059, Loss: 3.3742\n",
            "Epoch 1/1, Batch 50540/98059, Loss: 3.1312\n",
            "Epoch 1/1, Batch 50560/98059, Loss: 2.8900\n",
            "Epoch 1/1, Batch 50580/98059, Loss: 3.5361\n",
            "Epoch 1/1, Batch 50600/98059, Loss: 3.1463\n",
            "Epoch 1/1, Batch 50620/98059, Loss: 3.1236\n",
            "Epoch 1/1, Batch 50640/98059, Loss: 3.2191\n",
            "Epoch 1/1, Batch 50660/98059, Loss: 3.2503\n",
            "Epoch 1/1, Batch 50680/98059, Loss: 3.2273\n",
            "Epoch 1/1, Batch 50700/98059, Loss: 3.1227\n",
            "Epoch 1/1, Batch 50720/98059, Loss: 2.7599\n",
            "Epoch 1/1, Batch 50740/98059, Loss: 2.9215\n",
            "Epoch 1/1, Batch 50760/98059, Loss: 2.8949\n",
            "Epoch 1/1, Batch 50780/98059, Loss: 2.9422\n",
            "Epoch 1/1, Batch 50800/98059, Loss: 2.8934\n",
            "Epoch 1/1, Batch 50820/98059, Loss: 2.4959\n",
            "Epoch 1/1, Batch 50840/98059, Loss: 3.2777\n",
            "Epoch 1/1, Batch 50860/98059, Loss: 2.8950\n",
            "Epoch 1/1, Batch 50880/98059, Loss: 2.7003\n",
            "Epoch 1/1, Batch 50900/98059, Loss: 2.9413\n",
            "Epoch 1/1, Batch 50920/98059, Loss: 3.1466\n",
            "Epoch 1/1, Batch 50940/98059, Loss: 2.9430\n",
            "Epoch 1/1, Batch 50960/98059, Loss: 2.9900\n",
            "Epoch 1/1, Batch 50980/98059, Loss: 3.2071\n",
            "Epoch 1/1, Batch 51000/98059, Loss: 2.9121\n",
            "Epoch 1/1, Batch 51020/98059, Loss: 2.9925\n",
            "Epoch 1/1, Batch 51040/98059, Loss: 2.6946\n",
            "Epoch 1/1, Batch 51060/98059, Loss: 3.2167\n",
            "Epoch 1/1, Batch 51080/98059, Loss: 3.0148\n",
            "Epoch 1/1, Batch 51100/98059, Loss: 3.2098\n",
            "Epoch 1/1, Batch 51120/98059, Loss: 2.6007\n",
            "Epoch 1/1, Batch 51140/98059, Loss: 2.6972\n",
            "Epoch 1/1, Batch 51160/98059, Loss: 3.1662\n",
            "Epoch 1/1, Batch 51180/98059, Loss: 2.8358\n",
            "Epoch 1/1, Batch 51200/98059, Loss: 3.2358\n",
            "Epoch 1/1, Batch 51220/98059, Loss: 3.2243\n",
            "Epoch 1/1, Batch 51240/98059, Loss: 2.8723\n",
            "Epoch 1/1, Batch 51260/98059, Loss: 3.0325\n",
            "Epoch 1/1, Batch 51280/98059, Loss: 2.9522\n",
            "Epoch 1/1, Batch 51300/98059, Loss: 2.9385\n",
            "Epoch 1/1, Batch 51320/98059, Loss: 3.0618\n",
            "Epoch 1/1, Batch 51340/98059, Loss: 3.1968\n",
            "Epoch 1/1, Batch 51360/98059, Loss: 3.2329\n",
            "Epoch 1/1, Batch 51380/98059, Loss: 2.9631\n",
            "Epoch 1/1, Batch 51400/98059, Loss: 3.4066\n",
            "Epoch 1/1, Batch 51420/98059, Loss: 2.7935\n",
            "Epoch 1/1, Batch 51440/98059, Loss: 2.8349\n",
            "Epoch 1/1, Batch 51460/98059, Loss: 3.2478\n",
            "Epoch 1/1, Batch 51480/98059, Loss: 2.9483\n",
            "Epoch 1/1, Batch 51500/98059, Loss: 3.2542\n",
            "Epoch 1/1, Batch 51520/98059, Loss: 2.7530\n",
            "Epoch 1/1, Batch 51540/98059, Loss: 3.2197\n",
            "Epoch 1/1, Batch 51560/98059, Loss: 2.9656\n",
            "Epoch 1/1, Batch 51580/98059, Loss: 3.1548\n",
            "Epoch 1/1, Batch 51600/98059, Loss: 3.2244\n",
            "Epoch 1/1, Batch 51620/98059, Loss: 2.9884\n",
            "Epoch 1/1, Batch 51640/98059, Loss: 3.0601\n",
            "Epoch 1/1, Batch 51660/98059, Loss: 3.1837\n",
            "Epoch 1/1, Batch 51680/98059, Loss: 2.7625\n",
            "Epoch 1/1, Batch 51700/98059, Loss: 2.7383\n",
            "Epoch 1/1, Batch 51720/98059, Loss: 2.5090\n",
            "Epoch 1/1, Batch 51740/98059, Loss: 3.0351\n",
            "Epoch 1/1, Batch 51760/98059, Loss: 3.4850\n",
            "Epoch 1/1, Batch 51780/98059, Loss: 2.7817\n",
            "Epoch 1/1, Batch 51800/98059, Loss: 2.7941\n",
            "Epoch 1/1, Batch 51820/98059, Loss: 3.0539\n",
            "Epoch 1/1, Batch 51840/98059, Loss: 3.0862\n",
            "Epoch 1/1, Batch 51860/98059, Loss: 3.2684\n",
            "Epoch 1/1, Batch 51880/98059, Loss: 3.2118\n",
            "Epoch 1/1, Batch 51900/98059, Loss: 3.0679\n",
            "Epoch 1/1, Batch 51920/98059, Loss: 2.9884\n",
            "Epoch 1/1, Batch 51940/98059, Loss: 3.3435\n",
            "Epoch 1/1, Batch 51960/98059, Loss: 3.1001\n",
            "Epoch 1/1, Batch 51980/98059, Loss: 3.2573\n",
            "Epoch 1/1, Batch 52000/98059, Loss: 3.0436\n",
            "Epoch 1/1, Batch 52020/98059, Loss: 3.1559\n",
            "Epoch 1/1, Batch 52040/98059, Loss: 3.6243\n",
            "Epoch 1/1, Batch 52060/98059, Loss: 3.2086\n",
            "Epoch 1/1, Batch 52080/98059, Loss: 3.0946\n",
            "Epoch 1/1, Batch 52100/98059, Loss: 2.8303\n",
            "Epoch 1/1, Batch 52120/98059, Loss: 2.9891\n",
            "Epoch 1/1, Batch 52140/98059, Loss: 2.7127\n",
            "Epoch 1/1, Batch 52160/98059, Loss: 3.0775\n",
            "Epoch 1/1, Batch 52180/98059, Loss: 2.7265\n",
            "Epoch 1/1, Batch 52200/98059, Loss: 3.1986\n",
            "Epoch 1/1, Batch 52220/98059, Loss: 3.3643\n",
            "Epoch 1/1, Batch 52240/98059, Loss: 2.7959\n",
            "Epoch 1/1, Batch 52260/98059, Loss: 3.2448\n",
            "Epoch 1/1, Batch 52280/98059, Loss: 2.9517\n",
            "Epoch 1/1, Batch 52300/98059, Loss: 2.9708\n",
            "Epoch 1/1, Batch 52320/98059, Loss: 3.2153\n",
            "Epoch 1/1, Batch 52340/98059, Loss: 3.1494\n",
            "Epoch 1/1, Batch 52360/98059, Loss: 2.8433\n",
            "Epoch 1/1, Batch 52380/98059, Loss: 3.2239\n",
            "Epoch 1/1, Batch 52400/98059, Loss: 2.6177\n",
            "Epoch 1/1, Batch 52420/98059, Loss: 3.0038\n",
            "Epoch 1/1, Batch 52440/98059, Loss: 3.2983\n",
            "Epoch 1/1, Batch 52460/98059, Loss: 2.9747\n",
            "Epoch 1/1, Batch 52480/98059, Loss: 3.1614\n",
            "Epoch 1/1, Batch 52500/98059, Loss: 3.3271\n",
            "Epoch 1/1, Batch 52520/98059, Loss: 2.8771\n",
            "Epoch 1/1, Batch 52540/98059, Loss: 2.8824\n",
            "Epoch 1/1, Batch 52560/98059, Loss: 2.8968\n",
            "Epoch 1/1, Batch 52580/98059, Loss: 2.9762\n",
            "Epoch 1/1, Batch 52600/98059, Loss: 2.8117\n",
            "Epoch 1/1, Batch 52620/98059, Loss: 3.1220\n",
            "Epoch 1/1, Batch 52640/98059, Loss: 3.1571\n",
            "Epoch 1/1, Batch 52660/98059, Loss: 2.9296\n",
            "Epoch 1/1, Batch 52680/98059, Loss: 3.0584\n",
            "Epoch 1/1, Batch 52700/98059, Loss: 3.1838\n",
            "Epoch 1/1, Batch 52720/98059, Loss: 2.9935\n",
            "Epoch 1/1, Batch 52740/98059, Loss: 3.3911\n",
            "Epoch 1/1, Batch 52760/98059, Loss: 3.4014\n",
            "Epoch 1/1, Batch 52780/98059, Loss: 2.7677\n",
            "Epoch 1/1, Batch 52800/98059, Loss: 3.0171\n",
            "Epoch 1/1, Batch 52820/98059, Loss: 2.8764\n",
            "Epoch 1/1, Batch 52840/98059, Loss: 3.0015\n",
            "Epoch 1/1, Batch 52860/98059, Loss: 2.9631\n",
            "Epoch 1/1, Batch 52880/98059, Loss: 2.9343\n",
            "Epoch 1/1, Batch 52900/98059, Loss: 2.9076\n",
            "Epoch 1/1, Batch 52920/98059, Loss: 3.3398\n",
            "Epoch 1/1, Batch 52940/98059, Loss: 2.7881\n",
            "Epoch 1/1, Batch 52960/98059, Loss: 3.0588\n",
            "Epoch 1/1, Batch 52980/98059, Loss: 2.9063\n",
            "Epoch 1/1, Batch 53000/98059, Loss: 3.0928\n",
            "Epoch 1/1, Batch 53020/98059, Loss: 3.0335\n",
            "Epoch 1/1, Batch 53040/98059, Loss: 3.3279\n",
            "Epoch 1/1, Batch 53060/98059, Loss: 3.2656\n",
            "Epoch 1/1, Batch 53080/98059, Loss: 2.7756\n",
            "Epoch 1/1, Batch 53100/98059, Loss: 3.0124\n",
            "Epoch 1/1, Batch 53120/98059, Loss: 2.9673\n",
            "Epoch 1/1, Batch 53140/98059, Loss: 2.7579\n",
            "Epoch 1/1, Batch 53160/98059, Loss: 3.0323\n",
            "Epoch 1/1, Batch 53180/98059, Loss: 2.8002\n",
            "Epoch 1/1, Batch 53200/98059, Loss: 2.7409\n",
            "Epoch 1/1, Batch 53220/98059, Loss: 3.1239\n",
            "Epoch 1/1, Batch 53240/98059, Loss: 3.2300\n",
            "Epoch 1/1, Batch 53260/98059, Loss: 2.7363\n",
            "Epoch 1/1, Batch 53280/98059, Loss: 3.2751\n",
            "Epoch 1/1, Batch 53300/98059, Loss: 2.9871\n",
            "Epoch 1/1, Batch 53320/98059, Loss: 3.0524\n",
            "Epoch 1/1, Batch 53340/98059, Loss: 3.1440\n",
            "Epoch 1/1, Batch 53360/98059, Loss: 2.4304\n",
            "Epoch 1/1, Batch 53380/98059, Loss: 2.9273\n",
            "Epoch 1/1, Batch 53400/98059, Loss: 3.0000\n",
            "Epoch 1/1, Batch 53420/98059, Loss: 3.1961\n",
            "Epoch 1/1, Batch 53440/98059, Loss: 2.9662\n",
            "Epoch 1/1, Batch 53460/98059, Loss: 2.7940\n",
            "Epoch 1/1, Batch 53480/98059, Loss: 3.2575\n",
            "Epoch 1/1, Batch 53500/98059, Loss: 2.8424\n",
            "Epoch 1/1, Batch 53520/98059, Loss: 3.2972\n",
            "Epoch 1/1, Batch 53540/98059, Loss: 2.7284\n",
            "Epoch 1/1, Batch 53560/98059, Loss: 3.0811\n",
            "Epoch 1/1, Batch 53580/98059, Loss: 3.0148\n",
            "Epoch 1/1, Batch 53600/98059, Loss: 3.1773\n",
            "Epoch 1/1, Batch 53620/98059, Loss: 3.0912\n",
            "Epoch 1/1, Batch 53640/98059, Loss: 2.9388\n",
            "Epoch 1/1, Batch 53660/98059, Loss: 2.9602\n",
            "Epoch 1/1, Batch 53680/98059, Loss: 3.0159\n",
            "Epoch 1/1, Batch 53700/98059, Loss: 3.2865\n",
            "Epoch 1/1, Batch 53720/98059, Loss: 3.0138\n",
            "Epoch 1/1, Batch 53740/98059, Loss: 3.3052\n",
            "Epoch 1/1, Batch 53760/98059, Loss: 2.7995\n",
            "Epoch 1/1, Batch 53780/98059, Loss: 3.2549\n",
            "Epoch 1/1, Batch 53800/98059, Loss: 3.1397\n",
            "Epoch 1/1, Batch 53820/98059, Loss: 3.2336\n",
            "Epoch 1/1, Batch 53840/98059, Loss: 2.8385\n",
            "Epoch 1/1, Batch 53860/98059, Loss: 3.1843\n",
            "Epoch 1/1, Batch 53880/98059, Loss: 3.2145\n",
            "Epoch 1/1, Batch 53900/98059, Loss: 2.9892\n",
            "Epoch 1/1, Batch 53920/98059, Loss: 2.8619\n",
            "Epoch 1/1, Batch 53940/98059, Loss: 3.1390\n",
            "Epoch 1/1, Batch 53960/98059, Loss: 2.6816\n",
            "Epoch 1/1, Batch 53980/98059, Loss: 2.8988\n",
            "Epoch 1/1, Batch 54000/98059, Loss: 2.9840\n",
            "Epoch 1/1, Batch 54020/98059, Loss: 3.0775\n",
            "Epoch 1/1, Batch 54040/98059, Loss: 3.1174\n",
            "Epoch 1/1, Batch 54060/98059, Loss: 3.0448\n",
            "Epoch 1/1, Batch 54080/98059, Loss: 3.1184\n",
            "Epoch 1/1, Batch 54100/98059, Loss: 3.0783\n",
            "Epoch 1/1, Batch 54120/98059, Loss: 3.1639\n",
            "Epoch 1/1, Batch 54140/98059, Loss: 3.1030\n",
            "Epoch 1/1, Batch 54160/98059, Loss: 2.7340\n",
            "Epoch 1/1, Batch 54180/98059, Loss: 3.0719\n",
            "Epoch 1/1, Batch 54200/98059, Loss: 3.0047\n",
            "Epoch 1/1, Batch 54220/98059, Loss: 2.8636\n",
            "Epoch 1/1, Batch 54240/98059, Loss: 3.5441\n",
            "Epoch 1/1, Batch 54260/98059, Loss: 2.8986\n",
            "Epoch 1/1, Batch 54280/98059, Loss: 2.7953\n",
            "Epoch 1/1, Batch 54300/98059, Loss: 2.9985\n",
            "Epoch 1/1, Batch 54320/98059, Loss: 3.1734\n",
            "Epoch 1/1, Batch 54340/98059, Loss: 2.7059\n",
            "Epoch 1/1, Batch 54360/98059, Loss: 2.9263\n",
            "Epoch 1/1, Batch 54380/98059, Loss: 3.1646\n",
            "Epoch 1/1, Batch 54400/98059, Loss: 2.9680\n",
            "Epoch 1/1, Batch 54420/98059, Loss: 2.9086\n",
            "Epoch 1/1, Batch 54440/98059, Loss: 2.7783\n",
            "Epoch 1/1, Batch 54460/98059, Loss: 3.0220\n",
            "Epoch 1/1, Batch 54480/98059, Loss: 2.9452\n",
            "Epoch 1/1, Batch 54500/98059, Loss: 3.3648\n",
            "Epoch 1/1, Batch 54520/98059, Loss: 3.0336\n",
            "Epoch 1/1, Batch 54540/98059, Loss: 3.2969\n",
            "Epoch 1/1, Batch 54560/98059, Loss: 2.9612\n",
            "Epoch 1/1, Batch 54580/98059, Loss: 2.4476\n",
            "Epoch 1/1, Batch 54600/98059, Loss: 3.2162\n",
            "Epoch 1/1, Batch 54620/98059, Loss: 3.2652\n",
            "Epoch 1/1, Batch 54640/98059, Loss: 3.3960\n",
            "Epoch 1/1, Batch 54660/98059, Loss: 3.1841\n",
            "Epoch 1/1, Batch 54680/98059, Loss: 3.0369\n",
            "Epoch 1/1, Batch 54700/98059, Loss: 3.2681\n",
            "Epoch 1/1, Batch 54720/98059, Loss: 3.0439\n",
            "Epoch 1/1, Batch 54740/98059, Loss: 3.0011\n",
            "Epoch 1/1, Batch 54760/98059, Loss: 3.2178\n",
            "Epoch 1/1, Batch 54780/98059, Loss: 2.9266\n",
            "Epoch 1/1, Batch 54800/98059, Loss: 2.6794\n",
            "Epoch 1/1, Batch 54820/98059, Loss: 3.0413\n",
            "Epoch 1/1, Batch 54840/98059, Loss: 2.9989\n",
            "Epoch 1/1, Batch 54860/98059, Loss: 3.3206\n",
            "Epoch 1/1, Batch 54880/98059, Loss: 2.7744\n",
            "Epoch 1/1, Batch 54900/98059, Loss: 3.1610\n",
            "Epoch 1/1, Batch 54920/98059, Loss: 2.6776\n",
            "Epoch 1/1, Batch 54940/98059, Loss: 2.7540\n",
            "Epoch 1/1, Batch 54960/98059, Loss: 2.9343\n",
            "Epoch 1/1, Batch 54980/98059, Loss: 3.1612\n",
            "Epoch 1/1, Batch 55000/98059, Loss: 3.1140\n",
            "Epoch 1/1, Batch 55020/98059, Loss: 3.1154\n",
            "Epoch 1/1, Batch 55040/98059, Loss: 2.9803\n",
            "Epoch 1/1, Batch 55060/98059, Loss: 3.0980\n",
            "Epoch 1/1, Batch 55080/98059, Loss: 2.6551\n",
            "Epoch 1/1, Batch 55100/98059, Loss: 2.8334\n",
            "Epoch 1/1, Batch 55120/98059, Loss: 3.0333\n",
            "Epoch 1/1, Batch 55140/98059, Loss: 3.0319\n",
            "Epoch 1/1, Batch 55160/98059, Loss: 2.9731\n",
            "Epoch 1/1, Batch 55180/98059, Loss: 3.3437\n",
            "Epoch 1/1, Batch 55200/98059, Loss: 3.4861\n",
            "Epoch 1/1, Batch 55220/98059, Loss: 3.2254\n",
            "Epoch 1/1, Batch 55240/98059, Loss: 2.9511\n",
            "Epoch 1/1, Batch 55260/98059, Loss: 3.3143\n",
            "Epoch 1/1, Batch 55280/98059, Loss: 2.8514\n",
            "Epoch 1/1, Batch 55300/98059, Loss: 3.2037\n",
            "Epoch 1/1, Batch 55320/98059, Loss: 3.1039\n",
            "Epoch 1/1, Batch 55340/98059, Loss: 3.4765\n",
            "Epoch 1/1, Batch 55360/98059, Loss: 3.2193\n",
            "Epoch 1/1, Batch 55380/98059, Loss: 2.9461\n",
            "Epoch 1/1, Batch 55400/98059, Loss: 3.1811\n",
            "Epoch 1/1, Batch 55420/98059, Loss: 3.1098\n",
            "Epoch 1/1, Batch 55440/98059, Loss: 2.7935\n",
            "Epoch 1/1, Batch 55460/98059, Loss: 3.2401\n",
            "Epoch 1/1, Batch 55480/98059, Loss: 3.1939\n",
            "Epoch 1/1, Batch 55500/98059, Loss: 3.1651\n",
            "Epoch 1/1, Batch 55520/98059, Loss: 2.7616\n",
            "Epoch 1/1, Batch 55540/98059, Loss: 2.8957\n",
            "Epoch 1/1, Batch 55560/98059, Loss: 2.9471\n",
            "Epoch 1/1, Batch 55580/98059, Loss: 3.1010\n",
            "Epoch 1/1, Batch 55600/98059, Loss: 2.7823\n",
            "Epoch 1/1, Batch 55620/98059, Loss: 2.9967\n",
            "Epoch 1/1, Batch 55640/98059, Loss: 3.1956\n",
            "Epoch 1/1, Batch 55660/98059, Loss: 3.1705\n",
            "Epoch 1/1, Batch 55680/98059, Loss: 3.2116\n",
            "Epoch 1/1, Batch 55700/98059, Loss: 3.0794\n",
            "Epoch 1/1, Batch 55720/98059, Loss: 3.0054\n",
            "Epoch 1/1, Batch 55740/98059, Loss: 2.8673\n",
            "Epoch 1/1, Batch 55760/98059, Loss: 3.2447\n",
            "Epoch 1/1, Batch 55780/98059, Loss: 3.0628\n",
            "Epoch 1/1, Batch 55800/98059, Loss: 3.1915\n",
            "Epoch 1/1, Batch 55820/98059, Loss: 3.0059\n",
            "Epoch 1/1, Batch 55840/98059, Loss: 3.0724\n",
            "Epoch 1/1, Batch 55860/98059, Loss: 3.1427\n",
            "Epoch 1/1, Batch 55880/98059, Loss: 2.9844\n",
            "Epoch 1/1, Batch 55900/98059, Loss: 2.8681\n",
            "Epoch 1/1, Batch 55920/98059, Loss: 2.9828\n",
            "Epoch 1/1, Batch 55940/98059, Loss: 3.3201\n",
            "Epoch 1/1, Batch 55960/98059, Loss: 3.3648\n",
            "Epoch 1/1, Batch 55980/98059, Loss: 3.0672\n",
            "Epoch 1/1, Batch 56000/98059, Loss: 3.0458\n",
            "Epoch 1/1, Batch 56020/98059, Loss: 3.3626\n",
            "Epoch 1/1, Batch 56040/98059, Loss: 3.2693\n",
            "Epoch 1/1, Batch 56060/98059, Loss: 3.1313\n",
            "Epoch 1/1, Batch 56080/98059, Loss: 3.0496\n",
            "Epoch 1/1, Batch 56100/98059, Loss: 2.8027\n",
            "Epoch 1/1, Batch 56120/98059, Loss: 3.0943\n",
            "Epoch 1/1, Batch 56140/98059, Loss: 3.0563\n",
            "Epoch 1/1, Batch 56160/98059, Loss: 2.9480\n",
            "Epoch 1/1, Batch 56180/98059, Loss: 3.1229\n",
            "Epoch 1/1, Batch 56200/98059, Loss: 3.0163\n",
            "Epoch 1/1, Batch 56220/98059, Loss: 2.9756\n",
            "Epoch 1/1, Batch 56240/98059, Loss: 2.9559\n",
            "Epoch 1/1, Batch 56260/98059, Loss: 3.0576\n",
            "Epoch 1/1, Batch 56280/98059, Loss: 2.8624\n",
            "Epoch 1/1, Batch 56300/98059, Loss: 3.0767\n",
            "Epoch 1/1, Batch 56320/98059, Loss: 3.2323\n",
            "Epoch 1/1, Batch 56340/98059, Loss: 3.1179\n",
            "Epoch 1/1, Batch 56360/98059, Loss: 3.0441\n",
            "Epoch 1/1, Batch 56380/98059, Loss: 2.9308\n",
            "Epoch 1/1, Batch 56400/98059, Loss: 3.0707\n",
            "Epoch 1/1, Batch 56420/98059, Loss: 2.9315\n",
            "Epoch 1/1, Batch 56440/98059, Loss: 3.2472\n",
            "Epoch 1/1, Batch 56460/98059, Loss: 3.0969\n",
            "Epoch 1/1, Batch 56480/98059, Loss: 3.2338\n",
            "Epoch 1/1, Batch 56500/98059, Loss: 2.9993\n",
            "Epoch 1/1, Batch 56520/98059, Loss: 2.8549\n",
            "Epoch 1/1, Batch 56540/98059, Loss: 3.1414\n",
            "Epoch 1/1, Batch 56560/98059, Loss: 3.2340\n",
            "Epoch 1/1, Batch 56580/98059, Loss: 2.8976\n",
            "Epoch 1/1, Batch 56600/98059, Loss: 2.7851\n",
            "Epoch 1/1, Batch 56620/98059, Loss: 3.2639\n",
            "Epoch 1/1, Batch 56640/98059, Loss: 3.1467\n",
            "Epoch 1/1, Batch 56660/98059, Loss: 2.8847\n",
            "Epoch 1/1, Batch 56680/98059, Loss: 2.9972\n",
            "Epoch 1/1, Batch 56700/98059, Loss: 2.9232\n",
            "Epoch 1/1, Batch 56720/98059, Loss: 2.7625\n",
            "Epoch 1/1, Batch 56740/98059, Loss: 2.7146\n",
            "Epoch 1/1, Batch 56760/98059, Loss: 2.9457\n",
            "Epoch 1/1, Batch 56780/98059, Loss: 3.0316\n",
            "Epoch 1/1, Batch 56800/98059, Loss: 3.1198\n",
            "Epoch 1/1, Batch 56820/98059, Loss: 3.2515\n",
            "Epoch 1/1, Batch 56840/98059, Loss: 3.4367\n",
            "Epoch 1/1, Batch 56860/98059, Loss: 3.0498\n",
            "Epoch 1/1, Batch 56880/98059, Loss: 3.4630\n",
            "Epoch 1/1, Batch 56900/98059, Loss: 2.9987\n",
            "Epoch 1/1, Batch 56920/98059, Loss: 2.9939\n",
            "Epoch 1/1, Batch 56940/98059, Loss: 3.0618\n",
            "Epoch 1/1, Batch 56960/98059, Loss: 3.0234\n",
            "Epoch 1/1, Batch 56980/98059, Loss: 2.9418\n",
            "Epoch 1/1, Batch 57000/98059, Loss: 2.5128\n",
            "Epoch 1/1, Batch 57020/98059, Loss: 3.0359\n",
            "Epoch 1/1, Batch 57040/98059, Loss: 3.0469\n",
            "Epoch 1/1, Batch 57060/98059, Loss: 2.9077\n",
            "Epoch 1/1, Batch 57080/98059, Loss: 3.0719\n",
            "Epoch 1/1, Batch 57100/98059, Loss: 3.4718\n",
            "Epoch 1/1, Batch 57120/98059, Loss: 3.2975\n",
            "Epoch 1/1, Batch 57140/98059, Loss: 3.3274\n",
            "Epoch 1/1, Batch 57160/98059, Loss: 2.8444\n",
            "Epoch 1/1, Batch 57180/98059, Loss: 2.8804\n",
            "Epoch 1/1, Batch 57200/98059, Loss: 2.6965\n",
            "Epoch 1/1, Batch 57220/98059, Loss: 2.8645\n",
            "Epoch 1/1, Batch 57240/98059, Loss: 3.0984\n",
            "Epoch 1/1, Batch 57260/98059, Loss: 3.0970\n",
            "Epoch 1/1, Batch 57280/98059, Loss: 3.1291\n",
            "Epoch 1/1, Batch 57300/98059, Loss: 3.1890\n",
            "Epoch 1/1, Batch 57320/98059, Loss: 3.0948\n",
            "Epoch 1/1, Batch 57340/98059, Loss: 2.8273\n",
            "Epoch 1/1, Batch 57360/98059, Loss: 2.7817\n",
            "Epoch 1/1, Batch 57380/98059, Loss: 2.6734\n",
            "Epoch 1/1, Batch 57400/98059, Loss: 3.4557\n",
            "Epoch 1/1, Batch 57420/98059, Loss: 2.5674\n",
            "Epoch 1/1, Batch 57440/98059, Loss: 2.9549\n",
            "Epoch 1/1, Batch 57460/98059, Loss: 3.2240\n",
            "Epoch 1/1, Batch 57480/98059, Loss: 2.7002\n",
            "Epoch 1/1, Batch 57500/98059, Loss: 3.2118\n",
            "Epoch 1/1, Batch 57520/98059, Loss: 2.9431\n",
            "Epoch 1/1, Batch 57540/98059, Loss: 2.9864\n",
            "Epoch 1/1, Batch 57560/98059, Loss: 3.1238\n",
            "Epoch 1/1, Batch 57580/98059, Loss: 3.0871\n",
            "Epoch 1/1, Batch 57600/98059, Loss: 2.8891\n",
            "Epoch 1/1, Batch 57620/98059, Loss: 3.1389\n",
            "Epoch 1/1, Batch 57640/98059, Loss: 2.8597\n",
            "Epoch 1/1, Batch 57660/98059, Loss: 3.4264\n",
            "Epoch 1/1, Batch 57680/98059, Loss: 2.8731\n",
            "Epoch 1/1, Batch 57700/98059, Loss: 3.3270\n",
            "Epoch 1/1, Batch 57720/98059, Loss: 3.0563\n",
            "Epoch 1/1, Batch 57740/98059, Loss: 3.1297\n",
            "Epoch 1/1, Batch 57760/98059, Loss: 3.2767\n",
            "Epoch 1/1, Batch 57780/98059, Loss: 2.9953\n",
            "Epoch 1/1, Batch 57800/98059, Loss: 2.9198\n",
            "Epoch 1/1, Batch 57820/98059, Loss: 3.0534\n",
            "Epoch 1/1, Batch 57840/98059, Loss: 2.9886\n",
            "Epoch 1/1, Batch 57860/98059, Loss: 2.8901\n",
            "Epoch 1/1, Batch 57880/98059, Loss: 2.9980\n",
            "Epoch 1/1, Batch 57900/98059, Loss: 2.8940\n",
            "Epoch 1/1, Batch 57920/98059, Loss: 3.3352\n",
            "Epoch 1/1, Batch 57940/98059, Loss: 2.8730\n",
            "Epoch 1/1, Batch 57960/98059, Loss: 3.3723\n",
            "Epoch 1/1, Batch 57980/98059, Loss: 3.2717\n",
            "Epoch 1/1, Batch 58000/98059, Loss: 2.8077\n",
            "Epoch 1/1, Batch 58020/98059, Loss: 3.3795\n",
            "Epoch 1/1, Batch 58040/98059, Loss: 3.2421\n",
            "Epoch 1/1, Batch 58060/98059, Loss: 3.3059\n",
            "Epoch 1/1, Batch 58080/98059, Loss: 3.2217\n",
            "Epoch 1/1, Batch 58100/98059, Loss: 3.1555\n",
            "Epoch 1/1, Batch 58120/98059, Loss: 3.2526\n",
            "Epoch 1/1, Batch 58140/98059, Loss: 3.0301\n",
            "Epoch 1/1, Batch 58160/98059, Loss: 2.7136\n",
            "Epoch 1/1, Batch 58180/98059, Loss: 2.8273\n",
            "Epoch 1/1, Batch 58200/98059, Loss: 2.9980\n",
            "Epoch 1/1, Batch 58220/98059, Loss: 2.7879\n",
            "Epoch 1/1, Batch 58240/98059, Loss: 3.0315\n",
            "Epoch 1/1, Batch 58260/98059, Loss: 2.7274\n",
            "Epoch 1/1, Batch 58280/98059, Loss: 2.5037\n",
            "Epoch 1/1, Batch 58300/98059, Loss: 2.5585\n",
            "Epoch 1/1, Batch 58320/98059, Loss: 2.8549\n",
            "Epoch 1/1, Batch 58340/98059, Loss: 3.1460\n",
            "Epoch 1/1, Batch 58360/98059, Loss: 3.2151\n",
            "Epoch 1/1, Batch 58380/98059, Loss: 2.8719\n",
            "Epoch 1/1, Batch 58400/98059, Loss: 3.2642\n",
            "Epoch 1/1, Batch 58420/98059, Loss: 2.9451\n",
            "Epoch 1/1, Batch 58440/98059, Loss: 2.8414\n",
            "Epoch 1/1, Batch 58460/98059, Loss: 3.1172\n",
            "Epoch 1/1, Batch 58480/98059, Loss: 3.0181\n",
            "Epoch 1/1, Batch 58500/98059, Loss: 2.9560\n",
            "Epoch 1/1, Batch 58520/98059, Loss: 3.0951\n",
            "Epoch 1/1, Batch 58540/98059, Loss: 3.1116\n",
            "Epoch 1/1, Batch 58560/98059, Loss: 3.2812\n",
            "Epoch 1/1, Batch 58580/98059, Loss: 3.1415\n",
            "Epoch 1/1, Batch 58600/98059, Loss: 3.2807\n",
            "Epoch 1/1, Batch 58620/98059, Loss: 2.7533\n",
            "Epoch 1/1, Batch 58640/98059, Loss: 3.3597\n",
            "Epoch 1/1, Batch 58660/98059, Loss: 2.6257\n",
            "Epoch 1/1, Batch 58680/98059, Loss: 2.7484\n",
            "Epoch 1/1, Batch 58700/98059, Loss: 2.7597\n",
            "Epoch 1/1, Batch 58720/98059, Loss: 3.1673\n",
            "Epoch 1/1, Batch 58740/98059, Loss: 2.9575\n",
            "Epoch 1/1, Batch 58760/98059, Loss: 2.8641\n",
            "Epoch 1/1, Batch 58780/98059, Loss: 2.6300\n",
            "Epoch 1/1, Batch 58800/98059, Loss: 3.0401\n",
            "Epoch 1/1, Batch 58820/98059, Loss: 3.1278\n",
            "Epoch 1/1, Batch 58840/98059, Loss: 3.4694\n",
            "Epoch 1/1, Batch 58860/98059, Loss: 2.6347\n",
            "Epoch 1/1, Batch 58880/98059, Loss: 2.8383\n",
            "Epoch 1/1, Batch 58900/98059, Loss: 3.2176\n",
            "Epoch 1/1, Batch 58920/98059, Loss: 3.2772\n",
            "Epoch 1/1, Batch 58940/98059, Loss: 3.1389\n",
            "Epoch 1/1, Batch 58960/98059, Loss: 3.5630\n",
            "Epoch 1/1, Batch 58980/98059, Loss: 3.1718\n",
            "Epoch 1/1, Batch 59000/98059, Loss: 2.9787\n",
            "Epoch 1/1, Batch 59020/98059, Loss: 3.2161\n",
            "Epoch 1/1, Batch 59040/98059, Loss: 2.9160\n",
            "Epoch 1/1, Batch 59060/98059, Loss: 2.5385\n",
            "Epoch 1/1, Batch 59080/98059, Loss: 3.3793\n",
            "Epoch 1/1, Batch 59100/98059, Loss: 2.8011\n",
            "Epoch 1/1, Batch 59120/98059, Loss: 3.3538\n",
            "Epoch 1/1, Batch 59140/98059, Loss: 3.3336\n",
            "Epoch 1/1, Batch 59160/98059, Loss: 3.1128\n",
            "Epoch 1/1, Batch 59180/98059, Loss: 2.7864\n",
            "Epoch 1/1, Batch 59200/98059, Loss: 2.7751\n",
            "Epoch 1/1, Batch 59220/98059, Loss: 2.8655\n",
            "Epoch 1/1, Batch 59240/98059, Loss: 3.1239\n",
            "Epoch 1/1, Batch 59260/98059, Loss: 3.1296\n",
            "Epoch 1/1, Batch 59280/98059, Loss: 2.6010\n",
            "Epoch 1/1, Batch 59300/98059, Loss: 2.9828\n",
            "Epoch 1/1, Batch 59320/98059, Loss: 2.9045\n",
            "Epoch 1/1, Batch 59340/98059, Loss: 2.8178\n",
            "Epoch 1/1, Batch 59360/98059, Loss: 2.8013\n",
            "Epoch 1/1, Batch 59380/98059, Loss: 3.0988\n",
            "Epoch 1/1, Batch 59400/98059, Loss: 2.7645\n",
            "Epoch 1/1, Batch 59420/98059, Loss: 2.9879\n",
            "Epoch 1/1, Batch 59440/98059, Loss: 2.7637\n",
            "Epoch 1/1, Batch 59460/98059, Loss: 3.3421\n",
            "Epoch 1/1, Batch 59480/98059, Loss: 2.9857\n",
            "Epoch 1/1, Batch 59500/98059, Loss: 3.2201\n",
            "Epoch 1/1, Batch 59520/98059, Loss: 3.1935\n",
            "Epoch 1/1, Batch 59540/98059, Loss: 3.1671\n",
            "Epoch 1/1, Batch 59560/98059, Loss: 3.0180\n",
            "Epoch 1/1, Batch 59580/98059, Loss: 3.0164\n",
            "Epoch 1/1, Batch 59600/98059, Loss: 3.1617\n",
            "Epoch 1/1, Batch 59620/98059, Loss: 3.0069\n",
            "Epoch 1/1, Batch 59640/98059, Loss: 3.0838\n",
            "Epoch 1/1, Batch 59660/98059, Loss: 3.3015\n",
            "Epoch 1/1, Batch 59680/98059, Loss: 2.6922\n",
            "Epoch 1/1, Batch 59700/98059, Loss: 3.3851\n",
            "Epoch 1/1, Batch 59720/98059, Loss: 2.7937\n",
            "Epoch 1/1, Batch 59740/98059, Loss: 2.9583\n",
            "Epoch 1/1, Batch 59760/98059, Loss: 3.0684\n",
            "Epoch 1/1, Batch 59780/98059, Loss: 3.0448\n",
            "Epoch 1/1, Batch 59800/98059, Loss: 2.9197\n",
            "Epoch 1/1, Batch 59820/98059, Loss: 3.1062\n",
            "Epoch 1/1, Batch 59840/98059, Loss: 3.2351\n",
            "Epoch 1/1, Batch 59860/98059, Loss: 2.8271\n",
            "Epoch 1/1, Batch 59880/98059, Loss: 3.0820\n",
            "Epoch 1/1, Batch 59900/98059, Loss: 3.0351\n",
            "Epoch 1/1, Batch 59920/98059, Loss: 3.3380\n",
            "Epoch 1/1, Batch 59940/98059, Loss: 3.2277\n",
            "Epoch 1/1, Batch 59960/98059, Loss: 3.0355\n",
            "Epoch 1/1, Batch 59980/98059, Loss: 3.0679\n",
            "Epoch 1/1, Batch 60000/98059, Loss: 3.1846\n",
            "Epoch 1/1, Batch 60020/98059, Loss: 3.0030\n",
            "Epoch 1/1, Batch 60040/98059, Loss: 3.1045\n",
            "Epoch 1/1, Batch 60060/98059, Loss: 2.6092\n",
            "Epoch 1/1, Batch 60080/98059, Loss: 2.9037\n",
            "Epoch 1/1, Batch 60100/98059, Loss: 3.3540\n",
            "Epoch 1/1, Batch 60120/98059, Loss: 2.9125\n",
            "Epoch 1/1, Batch 60140/98059, Loss: 3.1010\n",
            "Epoch 1/1, Batch 60160/98059, Loss: 2.7654\n",
            "Epoch 1/1, Batch 60180/98059, Loss: 3.0142\n",
            "Epoch 1/1, Batch 60200/98059, Loss: 3.1528\n",
            "Epoch 1/1, Batch 60220/98059, Loss: 3.2133\n",
            "Epoch 1/1, Batch 60240/98059, Loss: 2.9323\n",
            "Epoch 1/1, Batch 60260/98059, Loss: 3.0836\n",
            "Epoch 1/1, Batch 60280/98059, Loss: 3.2663\n",
            "Epoch 1/1, Batch 60300/98059, Loss: 3.0911\n",
            "Epoch 1/1, Batch 60320/98059, Loss: 2.5809\n",
            "Epoch 1/1, Batch 60340/98059, Loss: 3.1918\n",
            "Epoch 1/1, Batch 60360/98059, Loss: 3.0087\n",
            "Epoch 1/1, Batch 60380/98059, Loss: 2.8075\n",
            "Epoch 1/1, Batch 60400/98059, Loss: 3.1078\n",
            "Epoch 1/1, Batch 60420/98059, Loss: 3.2196\n",
            "Epoch 1/1, Batch 60440/98059, Loss: 3.0476\n",
            "Epoch 1/1, Batch 60460/98059, Loss: 3.0283\n",
            "Epoch 1/1, Batch 60480/98059, Loss: 2.8667\n",
            "Epoch 1/1, Batch 60500/98059, Loss: 2.8621\n",
            "Epoch 1/1, Batch 60520/98059, Loss: 2.9715\n",
            "Epoch 1/1, Batch 60540/98059, Loss: 3.1705\n",
            "Epoch 1/1, Batch 60560/98059, Loss: 3.0246\n",
            "Epoch 1/1, Batch 60580/98059, Loss: 2.8112\n",
            "Epoch 1/1, Batch 60600/98059, Loss: 2.9449\n",
            "Epoch 1/1, Batch 60620/98059, Loss: 3.0818\n",
            "Epoch 1/1, Batch 60640/98059, Loss: 3.1849\n",
            "Epoch 1/1, Batch 60660/98059, Loss: 3.1133\n",
            "Epoch 1/1, Batch 60680/98059, Loss: 2.9001\n",
            "Epoch 1/1, Batch 60700/98059, Loss: 3.1466\n",
            "Epoch 1/1, Batch 60720/98059, Loss: 3.2561\n",
            "Epoch 1/1, Batch 60740/98059, Loss: 2.9138\n",
            "Epoch 1/1, Batch 60760/98059, Loss: 2.8758\n",
            "Epoch 1/1, Batch 60780/98059, Loss: 3.0857\n",
            "Epoch 1/1, Batch 60800/98059, Loss: 2.9913\n",
            "Epoch 1/1, Batch 60820/98059, Loss: 3.0674\n",
            "Epoch 1/1, Batch 60840/98059, Loss: 3.3044\n",
            "Epoch 1/1, Batch 60860/98059, Loss: 3.1653\n",
            "Epoch 1/1, Batch 60880/98059, Loss: 3.3000\n",
            "Epoch 1/1, Batch 60900/98059, Loss: 3.1340\n",
            "Epoch 1/1, Batch 60920/98059, Loss: 3.2253\n",
            "Epoch 1/1, Batch 60940/98059, Loss: 2.8605\n",
            "Epoch 1/1, Batch 60960/98059, Loss: 3.1939\n",
            "Epoch 1/1, Batch 60980/98059, Loss: 2.7091\n",
            "Epoch 1/1, Batch 61000/98059, Loss: 3.0059\n",
            "Epoch 1/1, Batch 61020/98059, Loss: 2.9062\n",
            "Epoch 1/1, Batch 61040/98059, Loss: 3.1631\n",
            "Epoch 1/1, Batch 61060/98059, Loss: 3.1455\n",
            "Epoch 1/1, Batch 61080/98059, Loss: 3.0767\n",
            "Epoch 1/1, Batch 61100/98059, Loss: 3.0334\n",
            "Epoch 1/1, Batch 61120/98059, Loss: 3.2330\n",
            "Epoch 1/1, Batch 61140/98059, Loss: 2.8333\n",
            "Epoch 1/1, Batch 61160/98059, Loss: 2.8411\n",
            "Epoch 1/1, Batch 61180/98059, Loss: 2.8049\n",
            "Epoch 1/1, Batch 61200/98059, Loss: 2.7826\n",
            "Epoch 1/1, Batch 61220/98059, Loss: 2.6482\n",
            "Epoch 1/1, Batch 61240/98059, Loss: 3.1215\n",
            "Epoch 1/1, Batch 61260/98059, Loss: 2.7614\n",
            "Epoch 1/1, Batch 61280/98059, Loss: 2.7645\n",
            "Epoch 1/1, Batch 61300/98059, Loss: 2.8061\n",
            "Epoch 1/1, Batch 61320/98059, Loss: 3.0636\n",
            "Epoch 1/1, Batch 61340/98059, Loss: 3.0327\n",
            "Epoch 1/1, Batch 61360/98059, Loss: 2.9149\n",
            "Epoch 1/1, Batch 61380/98059, Loss: 3.3245\n",
            "Epoch 1/1, Batch 61400/98059, Loss: 3.0285\n",
            "Epoch 1/1, Batch 61420/98059, Loss: 3.1315\n",
            "Epoch 1/1, Batch 61440/98059, Loss: 2.9235\n",
            "Epoch 1/1, Batch 61460/98059, Loss: 3.1187\n",
            "Epoch 1/1, Batch 61480/98059, Loss: 3.0787\n",
            "Epoch 1/1, Batch 61500/98059, Loss: 3.1061\n",
            "Epoch 1/1, Batch 61520/98059, Loss: 3.0094\n",
            "Epoch 1/1, Batch 61540/98059, Loss: 2.8613\n",
            "Epoch 1/1, Batch 61560/98059, Loss: 2.9097\n",
            "Epoch 1/1, Batch 61580/98059, Loss: 3.1125\n",
            "Epoch 1/1, Batch 61600/98059, Loss: 2.9383\n",
            "Epoch 1/1, Batch 61620/98059, Loss: 3.3823\n",
            "Epoch 1/1, Batch 61640/98059, Loss: 2.9942\n",
            "Epoch 1/1, Batch 61660/98059, Loss: 3.0155\n",
            "Epoch 1/1, Batch 61680/98059, Loss: 2.8620\n",
            "Epoch 1/1, Batch 61700/98059, Loss: 2.8935\n",
            "Epoch 1/1, Batch 61720/98059, Loss: 3.0076\n",
            "Epoch 1/1, Batch 61740/98059, Loss: 3.2177\n",
            "Epoch 1/1, Batch 61760/98059, Loss: 3.2175\n",
            "Epoch 1/1, Batch 61780/98059, Loss: 2.9414\n",
            "Epoch 1/1, Batch 61800/98059, Loss: 3.3242\n",
            "Epoch 1/1, Batch 61820/98059, Loss: 3.1384\n",
            "Epoch 1/1, Batch 61840/98059, Loss: 2.8880\n",
            "Epoch 1/1, Batch 61860/98059, Loss: 2.8854\n",
            "Epoch 1/1, Batch 61880/98059, Loss: 2.8637\n",
            "Epoch 1/1, Batch 61900/98059, Loss: 3.0781\n",
            "Epoch 1/1, Batch 61920/98059, Loss: 2.9157\n",
            "Epoch 1/1, Batch 61940/98059, Loss: 3.1813\n",
            "Epoch 1/1, Batch 61960/98059, Loss: 2.9003\n",
            "Epoch 1/1, Batch 61980/98059, Loss: 3.1632\n",
            "Epoch 1/1, Batch 62000/98059, Loss: 2.8861\n",
            "Epoch 1/1, Batch 62020/98059, Loss: 3.0555\n",
            "Epoch 1/1, Batch 62040/98059, Loss: 2.8927\n",
            "Epoch 1/1, Batch 62060/98059, Loss: 3.2882\n",
            "Epoch 1/1, Batch 62080/98059, Loss: 2.3989\n",
            "Epoch 1/1, Batch 62100/98059, Loss: 3.0814\n",
            "Epoch 1/1, Batch 62120/98059, Loss: 3.0466\n",
            "Epoch 1/1, Batch 62140/98059, Loss: 3.1297\n",
            "Epoch 1/1, Batch 62160/98059, Loss: 2.9467\n",
            "Epoch 1/1, Batch 62180/98059, Loss: 2.5457\n",
            "Epoch 1/1, Batch 62200/98059, Loss: 3.0947\n",
            "Epoch 1/1, Batch 62220/98059, Loss: 2.9203\n",
            "Epoch 1/1, Batch 62240/98059, Loss: 3.2670\n",
            "Epoch 1/1, Batch 62260/98059, Loss: 2.8844\n",
            "Epoch 1/1, Batch 62280/98059, Loss: 3.1200\n",
            "Epoch 1/1, Batch 62300/98059, Loss: 3.1363\n",
            "Epoch 1/1, Batch 62320/98059, Loss: 2.7496\n",
            "Epoch 1/1, Batch 62340/98059, Loss: 2.7997\n",
            "Epoch 1/1, Batch 62360/98059, Loss: 3.0958\n",
            "Epoch 1/1, Batch 62380/98059, Loss: 2.8097\n",
            "Epoch 1/1, Batch 62400/98059, Loss: 2.8007\n",
            "Epoch 1/1, Batch 62420/98059, Loss: 2.8735\n",
            "Epoch 1/1, Batch 62440/98059, Loss: 3.0991\n",
            "Epoch 1/1, Batch 62460/98059, Loss: 3.0470\n",
            "Epoch 1/1, Batch 62480/98059, Loss: 3.0707\n",
            "Epoch 1/1, Batch 62500/98059, Loss: 3.0201\n",
            "Epoch 1/1, Batch 62520/98059, Loss: 2.9862\n",
            "Epoch 1/1, Batch 62540/98059, Loss: 3.3257\n",
            "Epoch 1/1, Batch 62560/98059, Loss: 2.9024\n",
            "Epoch 1/1, Batch 62580/98059, Loss: 3.0284\n",
            "Epoch 1/1, Batch 62600/98059, Loss: 3.1523\n",
            "Epoch 1/1, Batch 62620/98059, Loss: 2.5801\n",
            "Epoch 1/1, Batch 62640/98059, Loss: 3.1011\n",
            "Epoch 1/1, Batch 62660/98059, Loss: 3.4211\n",
            "Epoch 1/1, Batch 62680/98059, Loss: 3.2673\n",
            "Epoch 1/1, Batch 62700/98059, Loss: 2.8623\n",
            "Epoch 1/1, Batch 62720/98059, Loss: 2.8458\n",
            "Epoch 1/1, Batch 62740/98059, Loss: 3.0881\n",
            "Epoch 1/1, Batch 62760/98059, Loss: 3.2259\n",
            "Epoch 1/1, Batch 62780/98059, Loss: 3.0360\n",
            "Epoch 1/1, Batch 62800/98059, Loss: 3.0360\n",
            "Epoch 1/1, Batch 62820/98059, Loss: 3.1249\n",
            "Epoch 1/1, Batch 62840/98059, Loss: 2.7873\n",
            "Epoch 1/1, Batch 62860/98059, Loss: 2.7750\n",
            "Epoch 1/1, Batch 62880/98059, Loss: 3.1049\n",
            "Epoch 1/1, Batch 62900/98059, Loss: 3.1018\n",
            "Epoch 1/1, Batch 62920/98059, Loss: 3.2485\n",
            "Epoch 1/1, Batch 62940/98059, Loss: 2.9252\n",
            "Epoch 1/1, Batch 62960/98059, Loss: 3.0739\n",
            "Epoch 1/1, Batch 62980/98059, Loss: 3.1373\n",
            "Epoch 1/1, Batch 63000/98059, Loss: 3.3868\n",
            "Epoch 1/1, Batch 63020/98059, Loss: 3.2022\n",
            "Epoch 1/1, Batch 63040/98059, Loss: 3.0051\n",
            "Epoch 1/1, Batch 63060/98059, Loss: 2.7828\n",
            "Epoch 1/1, Batch 63080/98059, Loss: 3.0794\n",
            "Epoch 1/1, Batch 63100/98059, Loss: 2.6469\n",
            "Epoch 1/1, Batch 63120/98059, Loss: 3.0192\n",
            "Epoch 1/1, Batch 63140/98059, Loss: 3.2555\n",
            "Epoch 1/1, Batch 63160/98059, Loss: 2.5392\n",
            "Epoch 1/1, Batch 63180/98059, Loss: 3.0372\n",
            "Epoch 1/1, Batch 63200/98059, Loss: 2.9646\n",
            "Epoch 1/1, Batch 63220/98059, Loss: 3.4650\n",
            "Epoch 1/1, Batch 63240/98059, Loss: 3.1446\n",
            "Epoch 1/1, Batch 63260/98059, Loss: 2.7445\n",
            "Epoch 1/1, Batch 63280/98059, Loss: 3.1688\n",
            "Epoch 1/1, Batch 63300/98059, Loss: 3.0521\n",
            "Epoch 1/1, Batch 63320/98059, Loss: 2.9707\n",
            "Epoch 1/1, Batch 63340/98059, Loss: 2.9023\n",
            "Epoch 1/1, Batch 63360/98059, Loss: 3.1135\n",
            "Epoch 1/1, Batch 63380/98059, Loss: 3.0951\n",
            "Epoch 1/1, Batch 63400/98059, Loss: 3.0259\n",
            "Epoch 1/1, Batch 63420/98059, Loss: 2.9307\n",
            "Epoch 1/1, Batch 63440/98059, Loss: 2.8275\n",
            "Epoch 1/1, Batch 63460/98059, Loss: 2.8426\n",
            "Epoch 1/1, Batch 63480/98059, Loss: 3.0451\n",
            "Epoch 1/1, Batch 63500/98059, Loss: 2.9600\n",
            "Epoch 1/1, Batch 63520/98059, Loss: 3.0046\n",
            "Epoch 1/1, Batch 63540/98059, Loss: 2.7901\n",
            "Epoch 1/1, Batch 63560/98059, Loss: 3.3836\n",
            "Epoch 1/1, Batch 63580/98059, Loss: 3.5141\n",
            "Epoch 1/1, Batch 63600/98059, Loss: 2.8560\n",
            "Epoch 1/1, Batch 63620/98059, Loss: 3.1281\n",
            "Epoch 1/1, Batch 63640/98059, Loss: 2.7729\n",
            "Epoch 1/1, Batch 63660/98059, Loss: 3.0845\n",
            "Epoch 1/1, Batch 63680/98059, Loss: 3.1837\n",
            "Epoch 1/1, Batch 63700/98059, Loss: 3.0067\n",
            "Epoch 1/1, Batch 63720/98059, Loss: 3.1062\n",
            "Epoch 1/1, Batch 63740/98059, Loss: 3.2041\n",
            "Epoch 1/1, Batch 63760/98059, Loss: 3.4998\n",
            "Epoch 1/1, Batch 63780/98059, Loss: 3.0452\n",
            "Epoch 1/1, Batch 63800/98059, Loss: 2.5616\n",
            "Epoch 1/1, Batch 63820/98059, Loss: 2.9857\n",
            "Epoch 1/1, Batch 63840/98059, Loss: 3.3863\n",
            "Epoch 1/1, Batch 63860/98059, Loss: 3.1741\n",
            "Epoch 1/1, Batch 63880/98059, Loss: 2.9264\n",
            "Epoch 1/1, Batch 63900/98059, Loss: 2.5948\n",
            "Epoch 1/1, Batch 63920/98059, Loss: 3.1047\n",
            "Epoch 1/1, Batch 63940/98059, Loss: 2.9720\n",
            "Epoch 1/1, Batch 63960/98059, Loss: 3.0387\n",
            "Epoch 1/1, Batch 63980/98059, Loss: 2.7809\n",
            "Epoch 1/1, Batch 64000/98059, Loss: 3.1714\n",
            "Epoch 1/1, Batch 64020/98059, Loss: 2.9573\n",
            "Epoch 1/1, Batch 64040/98059, Loss: 2.9180\n",
            "Epoch 1/1, Batch 64060/98059, Loss: 3.0135\n",
            "Epoch 1/1, Batch 64080/98059, Loss: 3.0078\n",
            "Epoch 1/1, Batch 64100/98059, Loss: 3.3419\n",
            "Epoch 1/1, Batch 64120/98059, Loss: 3.1196\n",
            "Epoch 1/1, Batch 64140/98059, Loss: 3.2543\n",
            "Epoch 1/1, Batch 64160/98059, Loss: 3.1055\n",
            "Epoch 1/1, Batch 64180/98059, Loss: 2.7778\n",
            "Epoch 1/1, Batch 64200/98059, Loss: 2.9298\n",
            "Epoch 1/1, Batch 64220/98059, Loss: 2.7155\n",
            "Epoch 1/1, Batch 64240/98059, Loss: 2.8744\n",
            "Epoch 1/1, Batch 64260/98059, Loss: 3.0857\n",
            "Epoch 1/1, Batch 64280/98059, Loss: 3.0756\n",
            "Epoch 1/1, Batch 64300/98059, Loss: 2.9693\n",
            "Epoch 1/1, Batch 64320/98059, Loss: 3.1798\n",
            "Epoch 1/1, Batch 64340/98059, Loss: 3.2162\n",
            "Epoch 1/1, Batch 64360/98059, Loss: 2.9287\n",
            "Epoch 1/1, Batch 64380/98059, Loss: 2.9209\n",
            "Epoch 1/1, Batch 64400/98059, Loss: 2.9583\n",
            "Epoch 1/1, Batch 64420/98059, Loss: 2.6329\n",
            "Epoch 1/1, Batch 64440/98059, Loss: 2.9328\n",
            "Epoch 1/1, Batch 64460/98059, Loss: 2.8799\n",
            "Epoch 1/1, Batch 64480/98059, Loss: 3.2266\n",
            "Epoch 1/1, Batch 64500/98059, Loss: 3.2350\n",
            "Epoch 1/1, Batch 64520/98059, Loss: 3.0515\n",
            "Epoch 1/1, Batch 64540/98059, Loss: 3.3217\n",
            "Epoch 1/1, Batch 64560/98059, Loss: 3.1422\n",
            "Epoch 1/1, Batch 64580/98059, Loss: 2.9836\n",
            "Epoch 1/1, Batch 64600/98059, Loss: 3.1789\n",
            "Epoch 1/1, Batch 64620/98059, Loss: 2.8044\n",
            "Epoch 1/1, Batch 64640/98059, Loss: 3.2434\n",
            "Epoch 1/1, Batch 64660/98059, Loss: 3.1714\n",
            "Epoch 1/1, Batch 64680/98059, Loss: 2.9281\n",
            "Epoch 1/1, Batch 64700/98059, Loss: 2.9464\n",
            "Epoch 1/1, Batch 64720/98059, Loss: 3.0125\n",
            "Epoch 1/1, Batch 64740/98059, Loss: 2.8878\n",
            "Epoch 1/1, Batch 64760/98059, Loss: 3.0058\n",
            "Epoch 1/1, Batch 64780/98059, Loss: 3.0458\n",
            "Epoch 1/1, Batch 64800/98059, Loss: 3.0448\n",
            "Epoch 1/1, Batch 64820/98059, Loss: 3.1431\n",
            "Epoch 1/1, Batch 64840/98059, Loss: 2.5068\n",
            "Epoch 1/1, Batch 64860/98059, Loss: 2.7355\n",
            "Epoch 1/1, Batch 64880/98059, Loss: 3.2499\n",
            "Epoch 1/1, Batch 64900/98059, Loss: 2.7196\n",
            "Epoch 1/1, Batch 64920/98059, Loss: 2.9463\n",
            "Epoch 1/1, Batch 64940/98059, Loss: 3.0636\n",
            "Epoch 1/1, Batch 64960/98059, Loss: 3.0318\n",
            "Epoch 1/1, Batch 64980/98059, Loss: 3.1128\n",
            "Epoch 1/1, Batch 65000/98059, Loss: 2.8356\n",
            "Epoch 1/1, Batch 65020/98059, Loss: 2.5920\n",
            "Epoch 1/1, Batch 65040/98059, Loss: 2.9046\n",
            "Epoch 1/1, Batch 65060/98059, Loss: 3.1155\n",
            "Epoch 1/1, Batch 65080/98059, Loss: 3.2211\n",
            "Epoch 1/1, Batch 65100/98059, Loss: 2.9310\n",
            "Epoch 1/1, Batch 65120/98059, Loss: 2.9225\n",
            "Epoch 1/1, Batch 65140/98059, Loss: 2.8546\n",
            "Epoch 1/1, Batch 65160/98059, Loss: 3.3214\n",
            "Epoch 1/1, Batch 65180/98059, Loss: 3.1693\n",
            "Epoch 1/1, Batch 65200/98059, Loss: 2.8839\n",
            "Epoch 1/1, Batch 65220/98059, Loss: 2.8115\n",
            "Epoch 1/1, Batch 65240/98059, Loss: 2.8882\n",
            "Epoch 1/1, Batch 65260/98059, Loss: 3.0817\n",
            "Epoch 1/1, Batch 65280/98059, Loss: 2.9911\n",
            "Epoch 1/1, Batch 65300/98059, Loss: 3.1234\n",
            "Epoch 1/1, Batch 65320/98059, Loss: 2.9424\n",
            "Epoch 1/1, Batch 65340/98059, Loss: 3.1396\n",
            "Epoch 1/1, Batch 65360/98059, Loss: 2.8678\n",
            "Epoch 1/1, Batch 65380/98059, Loss: 2.8721\n",
            "Epoch 1/1, Batch 65400/98059, Loss: 2.7951\n",
            "Epoch 1/1, Batch 65420/98059, Loss: 3.0390\n",
            "Epoch 1/1, Batch 65440/98059, Loss: 2.9585\n",
            "Epoch 1/1, Batch 65460/98059, Loss: 3.1237\n",
            "Epoch 1/1, Batch 65480/98059, Loss: 3.0920\n",
            "Epoch 1/1, Batch 65500/98059, Loss: 2.9359\n",
            "Epoch 1/1, Batch 65520/98059, Loss: 3.1088\n",
            "Epoch 1/1, Batch 65540/98059, Loss: 3.2868\n",
            "Epoch 1/1, Batch 65560/98059, Loss: 2.8566\n",
            "Epoch 1/1, Batch 65580/98059, Loss: 3.0255\n",
            "Epoch 1/1, Batch 65600/98059, Loss: 2.7003\n",
            "Epoch 1/1, Batch 65620/98059, Loss: 3.0439\n",
            "Epoch 1/1, Batch 65640/98059, Loss: 2.6101\n",
            "Epoch 1/1, Batch 65660/98059, Loss: 2.6729\n",
            "Epoch 1/1, Batch 65680/98059, Loss: 3.0646\n",
            "Epoch 1/1, Batch 65700/98059, Loss: 2.7279\n",
            "Epoch 1/1, Batch 65720/98059, Loss: 2.8906\n",
            "Epoch 1/1, Batch 65740/98059, Loss: 2.9127\n",
            "Epoch 1/1, Batch 65760/98059, Loss: 3.0562\n",
            "Epoch 1/1, Batch 65780/98059, Loss: 2.7605\n",
            "Epoch 1/1, Batch 65800/98059, Loss: 2.9648\n",
            "Epoch 1/1, Batch 65820/98059, Loss: 3.0977\n",
            "Epoch 1/1, Batch 65840/98059, Loss: 3.2346\n",
            "Epoch 1/1, Batch 65860/98059, Loss: 3.3828\n",
            "Epoch 1/1, Batch 65880/98059, Loss: 3.2068\n",
            "Epoch 1/1, Batch 65900/98059, Loss: 2.8147\n",
            "Epoch 1/1, Batch 65920/98059, Loss: 2.8139\n",
            "Epoch 1/1, Batch 65940/98059, Loss: 2.7291\n",
            "Epoch 1/1, Batch 65960/98059, Loss: 2.9834\n",
            "Epoch 1/1, Batch 65980/98059, Loss: 3.0809\n",
            "Epoch 1/1, Batch 66000/98059, Loss: 3.0870\n",
            "Epoch 1/1, Batch 66020/98059, Loss: 2.9497\n",
            "Epoch 1/1, Batch 66040/98059, Loss: 2.9768\n",
            "Epoch 1/1, Batch 66060/98059, Loss: 2.9308\n",
            "Epoch 1/1, Batch 66080/98059, Loss: 2.9776\n",
            "Epoch 1/1, Batch 66100/98059, Loss: 2.8270\n",
            "Epoch 1/1, Batch 66120/98059, Loss: 3.1608\n",
            "Epoch 1/1, Batch 66140/98059, Loss: 3.2124\n",
            "Epoch 1/1, Batch 66160/98059, Loss: 3.0044\n",
            "Epoch 1/1, Batch 66180/98059, Loss: 2.8151\n",
            "Epoch 1/1, Batch 66200/98059, Loss: 3.0380\n",
            "Epoch 1/1, Batch 66220/98059, Loss: 3.4141\n",
            "Epoch 1/1, Batch 66240/98059, Loss: 3.0687\n",
            "Epoch 1/1, Batch 66260/98059, Loss: 3.1831\n",
            "Epoch 1/1, Batch 66280/98059, Loss: 2.8457\n",
            "Epoch 1/1, Batch 66300/98059, Loss: 3.0295\n",
            "Epoch 1/1, Batch 66320/98059, Loss: 3.2871\n",
            "Epoch 1/1, Batch 66340/98059, Loss: 2.8908\n",
            "Epoch 1/1, Batch 66360/98059, Loss: 2.9046\n",
            "Epoch 1/1, Batch 66380/98059, Loss: 3.0246\n",
            "Epoch 1/1, Batch 66400/98059, Loss: 3.1253\n",
            "Epoch 1/1, Batch 66420/98059, Loss: 2.9040\n",
            "Epoch 1/1, Batch 66440/98059, Loss: 2.7416\n",
            "Epoch 1/1, Batch 66460/98059, Loss: 2.8280\n",
            "Epoch 1/1, Batch 66480/98059, Loss: 3.0295\n",
            "Epoch 1/1, Batch 66500/98059, Loss: 3.1094\n",
            "Epoch 1/1, Batch 66520/98059, Loss: 3.0040\n",
            "Epoch 1/1, Batch 66540/98059, Loss: 3.0631\n",
            "Epoch 1/1, Batch 66560/98059, Loss: 3.4342\n",
            "Epoch 1/1, Batch 66580/98059, Loss: 2.6740\n",
            "Epoch 1/1, Batch 66600/98059, Loss: 3.0356\n",
            "Epoch 1/1, Batch 66620/98059, Loss: 3.0506\n",
            "Epoch 1/1, Batch 66640/98059, Loss: 3.0556\n",
            "Epoch 1/1, Batch 66660/98059, Loss: 3.0175\n",
            "Epoch 1/1, Batch 66680/98059, Loss: 3.2001\n",
            "Epoch 1/1, Batch 66700/98059, Loss: 3.2361\n",
            "Epoch 1/1, Batch 66720/98059, Loss: 2.7615\n",
            "Epoch 1/1, Batch 66740/98059, Loss: 3.2499\n",
            "Epoch 1/1, Batch 66760/98059, Loss: 2.7083\n",
            "Epoch 1/1, Batch 66780/98059, Loss: 3.2224\n",
            "Epoch 1/1, Batch 66800/98059, Loss: 2.8954\n",
            "Epoch 1/1, Batch 66820/98059, Loss: 3.1183\n",
            "Epoch 1/1, Batch 66840/98059, Loss: 2.6151\n",
            "Epoch 1/1, Batch 66860/98059, Loss: 3.2488\n",
            "Epoch 1/1, Batch 66880/98059, Loss: 3.1046\n",
            "Epoch 1/1, Batch 66900/98059, Loss: 3.0488\n",
            "Epoch 1/1, Batch 66920/98059, Loss: 3.2098\n",
            "Epoch 1/1, Batch 66940/98059, Loss: 3.4042\n",
            "Epoch 1/1, Batch 66960/98059, Loss: 3.4080\n",
            "Epoch 1/1, Batch 66980/98059, Loss: 3.0707\n",
            "Epoch 1/1, Batch 67000/98059, Loss: 3.1536\n",
            "Epoch 1/1, Batch 67020/98059, Loss: 2.6518\n",
            "Epoch 1/1, Batch 67040/98059, Loss: 3.2805\n",
            "Epoch 1/1, Batch 67060/98059, Loss: 2.6399\n",
            "Epoch 1/1, Batch 67080/98059, Loss: 2.7016\n",
            "Epoch 1/1, Batch 67100/98059, Loss: 2.9746\n",
            "Epoch 1/1, Batch 67120/98059, Loss: 2.8007\n",
            "Epoch 1/1, Batch 67140/98059, Loss: 3.2611\n",
            "Epoch 1/1, Batch 67160/98059, Loss: 2.8083\n",
            "Epoch 1/1, Batch 67180/98059, Loss: 3.2589\n",
            "Epoch 1/1, Batch 67200/98059, Loss: 3.3024\n",
            "Epoch 1/1, Batch 67220/98059, Loss: 3.0210\n",
            "Epoch 1/1, Batch 67240/98059, Loss: 2.9983\n",
            "Epoch 1/1, Batch 67260/98059, Loss: 2.8932\n",
            "Epoch 1/1, Batch 67280/98059, Loss: 3.3074\n",
            "Epoch 1/1, Batch 67300/98059, Loss: 2.9796\n",
            "Epoch 1/1, Batch 67320/98059, Loss: 2.6611\n",
            "Epoch 1/1, Batch 67340/98059, Loss: 3.0980\n",
            "Epoch 1/1, Batch 67360/98059, Loss: 3.0074\n",
            "Epoch 1/1, Batch 67380/98059, Loss: 3.1133\n",
            "Epoch 1/1, Batch 67400/98059, Loss: 2.8060\n",
            "Epoch 1/1, Batch 67420/98059, Loss: 3.1421\n",
            "Epoch 1/1, Batch 67440/98059, Loss: 3.0307\n",
            "Epoch 1/1, Batch 67460/98059, Loss: 2.8321\n",
            "Epoch 1/1, Batch 67480/98059, Loss: 3.2266\n",
            "Epoch 1/1, Batch 67500/98059, Loss: 3.1110\n",
            "Epoch 1/1, Batch 67520/98059, Loss: 2.8248\n",
            "Epoch 1/1, Batch 67540/98059, Loss: 3.3208\n",
            "Epoch 1/1, Batch 67560/98059, Loss: 3.4505\n",
            "Epoch 1/1, Batch 67580/98059, Loss: 3.0274\n",
            "Epoch 1/1, Batch 67600/98059, Loss: 3.0709\n",
            "Epoch 1/1, Batch 67620/98059, Loss: 3.1240\n",
            "Epoch 1/1, Batch 67640/98059, Loss: 2.9526\n",
            "Epoch 1/1, Batch 67660/98059, Loss: 3.0545\n",
            "Epoch 1/1, Batch 67680/98059, Loss: 2.7936\n",
            "Epoch 1/1, Batch 67700/98059, Loss: 3.1816\n",
            "Epoch 1/1, Batch 67720/98059, Loss: 3.0076\n",
            "Epoch 1/1, Batch 67740/98059, Loss: 3.1480\n",
            "Epoch 1/1, Batch 67760/98059, Loss: 2.6377\n",
            "Epoch 1/1, Batch 67780/98059, Loss: 3.0559\n",
            "Epoch 1/1, Batch 67800/98059, Loss: 2.9026\n",
            "Epoch 1/1, Batch 67820/98059, Loss: 3.0054\n",
            "Epoch 1/1, Batch 67840/98059, Loss: 3.0301\n",
            "Epoch 1/1, Batch 67860/98059, Loss: 2.8423\n",
            "Epoch 1/1, Batch 67880/98059, Loss: 3.2106\n",
            "Epoch 1/1, Batch 67900/98059, Loss: 3.2067\n",
            "Epoch 1/1, Batch 67920/98059, Loss: 2.8855\n",
            "Epoch 1/1, Batch 67940/98059, Loss: 3.2249\n",
            "Epoch 1/1, Batch 67960/98059, Loss: 2.9613\n",
            "Epoch 1/1, Batch 67980/98059, Loss: 3.0036\n",
            "Epoch 1/1, Batch 68000/98059, Loss: 3.0531\n",
            "Epoch 1/1, Batch 68020/98059, Loss: 2.9416\n",
            "Epoch 1/1, Batch 68040/98059, Loss: 2.7885\n",
            "Epoch 1/1, Batch 68060/98059, Loss: 2.9922\n",
            "Epoch 1/1, Batch 68080/98059, Loss: 2.6988\n",
            "Epoch 1/1, Batch 68100/98059, Loss: 3.2091\n",
            "Epoch 1/1, Batch 68120/98059, Loss: 3.4491\n",
            "Epoch 1/1, Batch 68140/98059, Loss: 3.0786\n",
            "Epoch 1/1, Batch 68160/98059, Loss: 2.7753\n",
            "Epoch 1/1, Batch 68180/98059, Loss: 3.1077\n",
            "Epoch 1/1, Batch 68200/98059, Loss: 3.1223\n",
            "Epoch 1/1, Batch 68220/98059, Loss: 2.8734\n",
            "Epoch 1/1, Batch 68240/98059, Loss: 3.1484\n",
            "Epoch 1/1, Batch 68260/98059, Loss: 2.9288\n",
            "Epoch 1/1, Batch 68280/98059, Loss: 2.8386\n",
            "Epoch 1/1, Batch 68300/98059, Loss: 2.8084\n",
            "Epoch 1/1, Batch 68320/98059, Loss: 2.8798\n",
            "Epoch 1/1, Batch 68340/98059, Loss: 2.9320\n",
            "Epoch 1/1, Batch 68360/98059, Loss: 3.0116\n",
            "Epoch 1/1, Batch 68380/98059, Loss: 2.9856\n",
            "Epoch 1/1, Batch 68400/98059, Loss: 2.8387\n",
            "Epoch 1/1, Batch 68420/98059, Loss: 3.2548\n",
            "Epoch 1/1, Batch 68440/98059, Loss: 3.2519\n",
            "Epoch 1/1, Batch 68460/98059, Loss: 2.8125\n",
            "Epoch 1/1, Batch 68480/98059, Loss: 3.3453\n",
            "Epoch 1/1, Batch 68500/98059, Loss: 3.1956\n",
            "Epoch 1/1, Batch 68520/98059, Loss: 3.1361\n",
            "Epoch 1/1, Batch 68540/98059, Loss: 2.8904\n",
            "Epoch 1/1, Batch 68560/98059, Loss: 3.0269\n",
            "Epoch 1/1, Batch 68580/98059, Loss: 3.1900\n",
            "Epoch 1/1, Batch 68600/98059, Loss: 2.9655\n",
            "Epoch 1/1, Batch 68620/98059, Loss: 2.9948\n",
            "Epoch 1/1, Batch 68640/98059, Loss: 2.8202\n",
            "Epoch 1/1, Batch 68660/98059, Loss: 3.1907\n",
            "Epoch 1/1, Batch 68680/98059, Loss: 3.2488\n",
            "Epoch 1/1, Batch 68700/98059, Loss: 2.9185\n",
            "Epoch 1/1, Batch 68720/98059, Loss: 3.2213\n",
            "Epoch 1/1, Batch 68740/98059, Loss: 3.1797\n",
            "Epoch 1/1, Batch 68760/98059, Loss: 3.1156\n",
            "Epoch 1/1, Batch 68780/98059, Loss: 3.2603\n",
            "Epoch 1/1, Batch 68800/98059, Loss: 3.0622\n",
            "Epoch 1/1, Batch 68820/98059, Loss: 3.0815\n",
            "Epoch 1/1, Batch 68840/98059, Loss: 2.9716\n",
            "Epoch 1/1, Batch 68860/98059, Loss: 2.8486\n",
            "Epoch 1/1, Batch 68880/98059, Loss: 2.7499\n",
            "Epoch 1/1, Batch 68900/98059, Loss: 3.2141\n",
            "Epoch 1/1, Batch 68920/98059, Loss: 3.0889\n",
            "Epoch 1/1, Batch 68940/98059, Loss: 2.9741\n",
            "Epoch 1/1, Batch 68960/98059, Loss: 2.9795\n",
            "Epoch 1/1, Batch 68980/98059, Loss: 2.8954\n",
            "Epoch 1/1, Batch 69000/98059, Loss: 3.0768\n",
            "Epoch 1/1, Batch 69020/98059, Loss: 3.0454\n",
            "Epoch 1/1, Batch 69040/98059, Loss: 2.9382\n",
            "Epoch 1/1, Batch 69060/98059, Loss: 2.9729\n",
            "Epoch 1/1, Batch 69080/98059, Loss: 3.0093\n",
            "Epoch 1/1, Batch 69100/98059, Loss: 2.8199\n",
            "Epoch 1/1, Batch 69120/98059, Loss: 2.9607\n",
            "Epoch 1/1, Batch 69140/98059, Loss: 3.4864\n",
            "Epoch 1/1, Batch 69160/98059, Loss: 2.5181\n",
            "Epoch 1/1, Batch 69180/98059, Loss: 2.7920\n",
            "Epoch 1/1, Batch 69200/98059, Loss: 2.6676\n",
            "Epoch 1/1, Batch 69220/98059, Loss: 2.8517\n",
            "Epoch 1/1, Batch 69240/98059, Loss: 2.8333\n",
            "Epoch 1/1, Batch 69260/98059, Loss: 2.8455\n",
            "Epoch 1/1, Batch 69280/98059, Loss: 3.0758\n",
            "Epoch 1/1, Batch 69300/98059, Loss: 2.7056\n",
            "Epoch 1/1, Batch 69320/98059, Loss: 2.9481\n",
            "Epoch 1/1, Batch 69340/98059, Loss: 2.9822\n",
            "Epoch 1/1, Batch 69360/98059, Loss: 3.3027\n",
            "Epoch 1/1, Batch 69380/98059, Loss: 3.1449\n",
            "Epoch 1/1, Batch 69400/98059, Loss: 2.8637\n",
            "Epoch 1/1, Batch 69420/98059, Loss: 2.8901\n",
            "Epoch 1/1, Batch 69440/98059, Loss: 3.2203\n",
            "Epoch 1/1, Batch 69460/98059, Loss: 3.0741\n",
            "Epoch 1/1, Batch 69480/98059, Loss: 3.2956\n",
            "Epoch 1/1, Batch 69500/98059, Loss: 2.8460\n",
            "Epoch 1/1, Batch 69520/98059, Loss: 2.7301\n",
            "Epoch 1/1, Batch 69540/98059, Loss: 3.0540\n",
            "Epoch 1/1, Batch 69560/98059, Loss: 3.2184\n",
            "Epoch 1/1, Batch 69580/98059, Loss: 3.0814\n",
            "Epoch 1/1, Batch 69600/98059, Loss: 3.1648\n",
            "Epoch 1/1, Batch 69620/98059, Loss: 3.0963\n",
            "Epoch 1/1, Batch 69640/98059, Loss: 2.8151\n",
            "Epoch 1/1, Batch 69660/98059, Loss: 2.9661\n",
            "Epoch 1/1, Batch 69680/98059, Loss: 2.6759\n",
            "Epoch 1/1, Batch 69700/98059, Loss: 2.5198\n",
            "Epoch 1/1, Batch 69720/98059, Loss: 2.8163\n",
            "Epoch 1/1, Batch 69740/98059, Loss: 3.1751\n",
            "Epoch 1/1, Batch 69760/98059, Loss: 3.0602\n",
            "Epoch 1/1, Batch 69780/98059, Loss: 3.0429\n",
            "Epoch 1/1, Batch 69800/98059, Loss: 2.9690\n",
            "Epoch 1/1, Batch 69820/98059, Loss: 2.9225\n",
            "Epoch 1/1, Batch 69840/98059, Loss: 3.4038\n",
            "Epoch 1/1, Batch 69860/98059, Loss: 2.9243\n",
            "Epoch 1/1, Batch 69880/98059, Loss: 2.9543\n",
            "Epoch 1/1, Batch 69900/98059, Loss: 3.0131\n",
            "Epoch 1/1, Batch 69920/98059, Loss: 3.1626\n",
            "Epoch 1/1, Batch 69940/98059, Loss: 3.1386\n",
            "Epoch 1/1, Batch 69960/98059, Loss: 2.8608\n",
            "Epoch 1/1, Batch 69980/98059, Loss: 2.6854\n",
            "Epoch 1/1, Batch 70000/98059, Loss: 2.8721\n",
            "Epoch 1/1, Batch 70020/98059, Loss: 2.3980\n",
            "Epoch 1/1, Batch 70040/98059, Loss: 2.9320\n",
            "Epoch 1/1, Batch 70060/98059, Loss: 3.0166\n",
            "Epoch 1/1, Batch 70080/98059, Loss: 3.3458\n",
            "Epoch 1/1, Batch 70100/98059, Loss: 2.9693\n",
            "Epoch 1/1, Batch 70120/98059, Loss: 2.9379\n",
            "Epoch 1/1, Batch 70140/98059, Loss: 3.1743\n",
            "Epoch 1/1, Batch 70160/98059, Loss: 3.1953\n",
            "Epoch 1/1, Batch 70180/98059, Loss: 2.7693\n",
            "Epoch 1/1, Batch 70200/98059, Loss: 3.0539\n",
            "Epoch 1/1, Batch 70220/98059, Loss: 2.9337\n",
            "Epoch 1/1, Batch 70240/98059, Loss: 2.6717\n",
            "Epoch 1/1, Batch 70260/98059, Loss: 2.7368\n",
            "Epoch 1/1, Batch 70280/98059, Loss: 3.1362\n",
            "Epoch 1/1, Batch 70300/98059, Loss: 3.2394\n",
            "Epoch 1/1, Batch 70320/98059, Loss: 3.3210\n",
            "Epoch 1/1, Batch 70340/98059, Loss: 3.3514\n",
            "Epoch 1/1, Batch 70360/98059, Loss: 2.8109\n",
            "Epoch 1/1, Batch 70380/98059, Loss: 3.0018\n",
            "Epoch 1/1, Batch 70400/98059, Loss: 3.0027\n",
            "Epoch 1/1, Batch 70420/98059, Loss: 2.6201\n",
            "Epoch 1/1, Batch 70440/98059, Loss: 3.2644\n",
            "Epoch 1/1, Batch 70460/98059, Loss: 2.9298\n",
            "Epoch 1/1, Batch 70480/98059, Loss: 2.8620\n",
            "Epoch 1/1, Batch 70500/98059, Loss: 3.1584\n",
            "Epoch 1/1, Batch 70520/98059, Loss: 3.2583\n",
            "Epoch 1/1, Batch 70540/98059, Loss: 3.3470\n",
            "Epoch 1/1, Batch 70560/98059, Loss: 2.9343\n",
            "Epoch 1/1, Batch 70580/98059, Loss: 2.9454\n",
            "Epoch 1/1, Batch 70600/98059, Loss: 2.8917\n",
            "Epoch 1/1, Batch 70620/98059, Loss: 2.8025\n",
            "Epoch 1/1, Batch 70640/98059, Loss: 2.7985\n",
            "Epoch 1/1, Batch 70660/98059, Loss: 2.8557\n",
            "Epoch 1/1, Batch 70680/98059, Loss: 2.6473\n",
            "Epoch 1/1, Batch 70700/98059, Loss: 2.9075\n",
            "Epoch 1/1, Batch 70720/98059, Loss: 2.8977\n",
            "Epoch 1/1, Batch 70740/98059, Loss: 3.2117\n",
            "Epoch 1/1, Batch 70760/98059, Loss: 3.0414\n",
            "Epoch 1/1, Batch 70780/98059, Loss: 2.9103\n",
            "Epoch 1/1, Batch 70800/98059, Loss: 3.1933\n",
            "Epoch 1/1, Batch 70820/98059, Loss: 3.0017\n",
            "Epoch 1/1, Batch 70840/98059, Loss: 2.9951\n",
            "Epoch 1/1, Batch 70860/98059, Loss: 2.9453\n",
            "Epoch 1/1, Batch 70880/98059, Loss: 2.8789\n",
            "Epoch 1/1, Batch 70900/98059, Loss: 2.8761\n",
            "Epoch 1/1, Batch 70920/98059, Loss: 3.0338\n",
            "Epoch 1/1, Batch 70940/98059, Loss: 3.0421\n",
            "Epoch 1/1, Batch 70960/98059, Loss: 2.8616\n",
            "Epoch 1/1, Batch 70980/98059, Loss: 3.4397\n",
            "Epoch 1/1, Batch 71000/98059, Loss: 2.8871\n",
            "Epoch 1/1, Batch 71020/98059, Loss: 2.8183\n",
            "Epoch 1/1, Batch 71040/98059, Loss: 2.7584\n",
            "Epoch 1/1, Batch 71060/98059, Loss: 2.7720\n",
            "Epoch 1/1, Batch 71080/98059, Loss: 3.0691\n",
            "Epoch 1/1, Batch 71100/98059, Loss: 3.2118\n",
            "Epoch 1/1, Batch 71120/98059, Loss: 2.7023\n",
            "Epoch 1/1, Batch 71140/98059, Loss: 3.2483\n",
            "Epoch 1/1, Batch 71160/98059, Loss: 3.4955\n",
            "Epoch 1/1, Batch 71180/98059, Loss: 3.0582\n",
            "Epoch 1/1, Batch 71200/98059, Loss: 3.2280\n",
            "Epoch 1/1, Batch 71220/98059, Loss: 2.7933\n",
            "Epoch 1/1, Batch 71240/98059, Loss: 2.5959\n",
            "Epoch 1/1, Batch 71260/98059, Loss: 3.2204\n",
            "Epoch 1/1, Batch 71280/98059, Loss: 3.2623\n",
            "Epoch 1/1, Batch 71300/98059, Loss: 3.0035\n",
            "Epoch 1/1, Batch 71320/98059, Loss: 2.8381\n",
            "Epoch 1/1, Batch 71340/98059, Loss: 2.9469\n",
            "Epoch 1/1, Batch 71360/98059, Loss: 2.8886\n",
            "Epoch 1/1, Batch 71380/98059, Loss: 2.6909\n",
            "Epoch 1/1, Batch 71400/98059, Loss: 3.0438\n",
            "Epoch 1/1, Batch 71420/98059, Loss: 3.0477\n",
            "Epoch 1/1, Batch 71440/98059, Loss: 2.8523\n",
            "Epoch 1/1, Batch 71460/98059, Loss: 3.2074\n",
            "Epoch 1/1, Batch 71480/98059, Loss: 2.9612\n",
            "Epoch 1/1, Batch 71500/98059, Loss: 3.2212\n",
            "Epoch 1/1, Batch 71520/98059, Loss: 2.8809\n",
            "Epoch 1/1, Batch 71540/98059, Loss: 3.2683\n",
            "Epoch 1/1, Batch 71560/98059, Loss: 3.0865\n",
            "Epoch 1/1, Batch 71580/98059, Loss: 2.7097\n",
            "Epoch 1/1, Batch 71600/98059, Loss: 3.0519\n",
            "Epoch 1/1, Batch 71620/98059, Loss: 2.8576\n",
            "Epoch 1/1, Batch 71640/98059, Loss: 2.7642\n",
            "Epoch 1/1, Batch 71660/98059, Loss: 3.1547\n",
            "Epoch 1/1, Batch 71680/98059, Loss: 3.0691\n",
            "Epoch 1/1, Batch 71700/98059, Loss: 2.8074\n",
            "Epoch 1/1, Batch 71720/98059, Loss: 2.9376\n",
            "Epoch 1/1, Batch 71740/98059, Loss: 2.9726\n",
            "Epoch 1/1, Batch 71760/98059, Loss: 2.8977\n",
            "Epoch 1/1, Batch 71780/98059, Loss: 2.9989\n",
            "Epoch 1/1, Batch 71800/98059, Loss: 3.0582\n",
            "Epoch 1/1, Batch 71820/98059, Loss: 3.0151\n",
            "Epoch 1/1, Batch 71840/98059, Loss: 2.8220\n",
            "Epoch 1/1, Batch 71860/98059, Loss: 2.9631\n",
            "Epoch 1/1, Batch 71880/98059, Loss: 2.7935\n",
            "Epoch 1/1, Batch 71900/98059, Loss: 2.7712\n",
            "Epoch 1/1, Batch 71920/98059, Loss: 3.0248\n",
            "Epoch 1/1, Batch 71940/98059, Loss: 3.0222\n",
            "Epoch 1/1, Batch 71960/98059, Loss: 3.0921\n",
            "Epoch 1/1, Batch 71980/98059, Loss: 3.0053\n",
            "Epoch 1/1, Batch 72000/98059, Loss: 2.9491\n",
            "Epoch 1/1, Batch 72020/98059, Loss: 3.0451\n",
            "Epoch 1/1, Batch 72040/98059, Loss: 2.6731\n",
            "Epoch 1/1, Batch 72060/98059, Loss: 3.3815\n",
            "Epoch 1/1, Batch 72080/98059, Loss: 2.8981\n",
            "Epoch 1/1, Batch 72100/98059, Loss: 3.0138\n",
            "Epoch 1/1, Batch 72120/98059, Loss: 3.3324\n",
            "Epoch 1/1, Batch 72140/98059, Loss: 3.0701\n",
            "Epoch 1/1, Batch 72160/98059, Loss: 2.8138\n",
            "Epoch 1/1, Batch 72180/98059, Loss: 2.8940\n",
            "Epoch 1/1, Batch 72200/98059, Loss: 2.8843\n",
            "Epoch 1/1, Batch 72220/98059, Loss: 2.9889\n",
            "Epoch 1/1, Batch 72240/98059, Loss: 3.0730\n",
            "Epoch 1/1, Batch 72260/98059, Loss: 3.2342\n",
            "Epoch 1/1, Batch 72280/98059, Loss: 2.8029\n",
            "Epoch 1/1, Batch 72300/98059, Loss: 2.9734\n",
            "Epoch 1/1, Batch 72320/98059, Loss: 2.8931\n",
            "Epoch 1/1, Batch 72340/98059, Loss: 3.1041\n",
            "Epoch 1/1, Batch 72360/98059, Loss: 2.9794\n",
            "Epoch 1/1, Batch 72380/98059, Loss: 3.2928\n",
            "Epoch 1/1, Batch 72400/98059, Loss: 3.0502\n",
            "Epoch 1/1, Batch 72420/98059, Loss: 3.1024\n",
            "Epoch 1/1, Batch 72440/98059, Loss: 2.6862\n",
            "Epoch 1/1, Batch 72460/98059, Loss: 3.1384\n",
            "Epoch 1/1, Batch 72480/98059, Loss: 2.9825\n",
            "Epoch 1/1, Batch 72500/98059, Loss: 3.1222\n",
            "Epoch 1/1, Batch 72520/98059, Loss: 2.8760\n",
            "Epoch 1/1, Batch 72540/98059, Loss: 2.8610\n",
            "Epoch 1/1, Batch 72560/98059, Loss: 2.8722\n",
            "Epoch 1/1, Batch 72580/98059, Loss: 3.1304\n",
            "Epoch 1/1, Batch 72600/98059, Loss: 2.9178\n",
            "Epoch 1/1, Batch 72620/98059, Loss: 3.1997\n",
            "Epoch 1/1, Batch 72640/98059, Loss: 3.2546\n",
            "Epoch 1/1, Batch 72660/98059, Loss: 3.0005\n",
            "Epoch 1/1, Batch 72680/98059, Loss: 2.7630\n",
            "Epoch 1/1, Batch 72700/98059, Loss: 2.7147\n",
            "Epoch 1/1, Batch 72720/98059, Loss: 3.1080\n",
            "Epoch 1/1, Batch 72740/98059, Loss: 2.9434\n",
            "Epoch 1/1, Batch 72760/98059, Loss: 2.8853\n",
            "Epoch 1/1, Batch 72780/98059, Loss: 3.2105\n",
            "Epoch 1/1, Batch 72800/98059, Loss: 2.8490\n",
            "Epoch 1/1, Batch 72820/98059, Loss: 2.7763\n",
            "Epoch 1/1, Batch 72840/98059, Loss: 2.8224\n",
            "Epoch 1/1, Batch 72860/98059, Loss: 2.8184\n",
            "Epoch 1/1, Batch 72880/98059, Loss: 2.7276\n",
            "Epoch 1/1, Batch 72900/98059, Loss: 3.1667\n",
            "Epoch 1/1, Batch 72920/98059, Loss: 3.0561\n",
            "Epoch 1/1, Batch 72940/98059, Loss: 3.0216\n",
            "Epoch 1/1, Batch 72960/98059, Loss: 3.1526\n",
            "Epoch 1/1, Batch 72980/98059, Loss: 3.0134\n",
            "Epoch 1/1, Batch 73000/98059, Loss: 2.6757\n",
            "Epoch 1/1, Batch 73020/98059, Loss: 2.6584\n",
            "Epoch 1/1, Batch 73040/98059, Loss: 2.9101\n",
            "Epoch 1/1, Batch 73060/98059, Loss: 3.1570\n",
            "Epoch 1/1, Batch 73080/98059, Loss: 2.7485\n",
            "Epoch 1/1, Batch 73100/98059, Loss: 3.0261\n",
            "Epoch 1/1, Batch 73120/98059, Loss: 3.2977\n",
            "Epoch 1/1, Batch 73140/98059, Loss: 2.9426\n",
            "Epoch 1/1, Batch 73160/98059, Loss: 2.5655\n",
            "Epoch 1/1, Batch 73180/98059, Loss: 2.5731\n",
            "Epoch 1/1, Batch 73200/98059, Loss: 3.2424\n",
            "Epoch 1/1, Batch 73220/98059, Loss: 2.7059\n",
            "Epoch 1/1, Batch 73240/98059, Loss: 2.8096\n",
            "Epoch 1/1, Batch 73260/98059, Loss: 3.0388\n",
            "Epoch 1/1, Batch 73280/98059, Loss: 2.9772\n",
            "Epoch 1/1, Batch 73300/98059, Loss: 3.4305\n",
            "Epoch 1/1, Batch 73320/98059, Loss: 2.9833\n",
            "Epoch 1/1, Batch 73340/98059, Loss: 3.4450\n",
            "Epoch 1/1, Batch 73360/98059, Loss: 3.2933\n",
            "Epoch 1/1, Batch 73380/98059, Loss: 2.9181\n",
            "Epoch 1/1, Batch 73400/98059, Loss: 3.0663\n",
            "Epoch 1/1, Batch 73420/98059, Loss: 2.9406\n",
            "Epoch 1/1, Batch 73440/98059, Loss: 2.7280\n",
            "Epoch 1/1, Batch 73460/98059, Loss: 3.1085\n",
            "Epoch 1/1, Batch 73480/98059, Loss: 2.9666\n",
            "Epoch 1/1, Batch 73500/98059, Loss: 2.6024\n",
            "Epoch 1/1, Batch 73520/98059, Loss: 3.1524\n",
            "Epoch 1/1, Batch 73540/98059, Loss: 2.9300\n",
            "Epoch 1/1, Batch 73560/98059, Loss: 2.9315\n",
            "Epoch 1/1, Batch 73580/98059, Loss: 2.8879\n",
            "Epoch 1/1, Batch 73600/98059, Loss: 3.1656\n",
            "Epoch 1/1, Batch 73620/98059, Loss: 3.1336\n",
            "Epoch 1/1, Batch 73640/98059, Loss: 2.8826\n",
            "Epoch 1/1, Batch 73660/98059, Loss: 3.1518\n",
            "Epoch 1/1, Batch 73680/98059, Loss: 3.1229\n",
            "Epoch 1/1, Batch 73700/98059, Loss: 3.1083\n",
            "Epoch 1/1, Batch 73720/98059, Loss: 3.1540\n",
            "Epoch 1/1, Batch 73740/98059, Loss: 2.6826\n",
            "Epoch 1/1, Batch 73760/98059, Loss: 2.7536\n",
            "Epoch 1/1, Batch 73780/98059, Loss: 2.9543\n",
            "Epoch 1/1, Batch 73800/98059, Loss: 2.7141\n",
            "Epoch 1/1, Batch 73820/98059, Loss: 3.0578\n",
            "Epoch 1/1, Batch 73840/98059, Loss: 2.9763\n",
            "Epoch 1/1, Batch 73860/98059, Loss: 3.2445\n",
            "Epoch 1/1, Batch 73880/98059, Loss: 2.9274\n",
            "Epoch 1/1, Batch 73900/98059, Loss: 2.7039\n",
            "Epoch 1/1, Batch 73920/98059, Loss: 3.0487\n",
            "Epoch 1/1, Batch 73940/98059, Loss: 2.7015\n",
            "Epoch 1/1, Batch 73960/98059, Loss: 3.0742\n",
            "Epoch 1/1, Batch 73980/98059, Loss: 3.0414\n",
            "Epoch 1/1, Batch 74000/98059, Loss: 2.9738\n",
            "Epoch 1/1, Batch 74020/98059, Loss: 3.2539\n",
            "Epoch 1/1, Batch 74040/98059, Loss: 3.4327\n",
            "Epoch 1/1, Batch 74060/98059, Loss: 2.9507\n",
            "Epoch 1/1, Batch 74080/98059, Loss: 2.7031\n",
            "Epoch 1/1, Batch 74100/98059, Loss: 3.3846\n",
            "Epoch 1/1, Batch 74120/98059, Loss: 3.2411\n",
            "Epoch 1/1, Batch 74140/98059, Loss: 2.9516\n",
            "Epoch 1/1, Batch 74160/98059, Loss: 3.4180\n",
            "Epoch 1/1, Batch 74180/98059, Loss: 3.0523\n",
            "Epoch 1/1, Batch 74200/98059, Loss: 3.2479\n",
            "Epoch 1/1, Batch 74220/98059, Loss: 3.0847\n",
            "Epoch 1/1, Batch 74240/98059, Loss: 3.3457\n",
            "Epoch 1/1, Batch 74260/98059, Loss: 3.2576\n",
            "Epoch 1/1, Batch 74280/98059, Loss: 3.0214\n",
            "Epoch 1/1, Batch 74300/98059, Loss: 2.6402\n",
            "Epoch 1/1, Batch 74320/98059, Loss: 2.9582\n",
            "Epoch 1/1, Batch 74340/98059, Loss: 2.5111\n",
            "Epoch 1/1, Batch 74360/98059, Loss: 3.0958\n",
            "Epoch 1/1, Batch 74380/98059, Loss: 3.2018\n",
            "Epoch 1/1, Batch 74400/98059, Loss: 3.0501\n",
            "Epoch 1/1, Batch 74420/98059, Loss: 2.9190\n",
            "Epoch 1/1, Batch 74440/98059, Loss: 2.9035\n",
            "Epoch 1/1, Batch 74460/98059, Loss: 2.8876\n",
            "Epoch 1/1, Batch 74480/98059, Loss: 2.9495\n",
            "Epoch 1/1, Batch 74500/98059, Loss: 3.1472\n",
            "Epoch 1/1, Batch 74520/98059, Loss: 3.1787\n",
            "Epoch 1/1, Batch 74540/98059, Loss: 3.1214\n",
            "Epoch 1/1, Batch 74560/98059, Loss: 2.8266\n",
            "Epoch 1/1, Batch 74580/98059, Loss: 3.1123\n",
            "Epoch 1/1, Batch 74600/98059, Loss: 2.4948\n",
            "Epoch 1/1, Batch 74620/98059, Loss: 3.1988\n",
            "Epoch 1/1, Batch 74640/98059, Loss: 3.0408\n",
            "Epoch 1/1, Batch 74660/98059, Loss: 2.9950\n",
            "Epoch 1/1, Batch 74680/98059, Loss: 3.0553\n",
            "Epoch 1/1, Batch 74700/98059, Loss: 2.8753\n",
            "Epoch 1/1, Batch 74720/98059, Loss: 3.1899\n",
            "Epoch 1/1, Batch 74740/98059, Loss: 3.0316\n",
            "Epoch 1/1, Batch 74760/98059, Loss: 3.0241\n",
            "Epoch 1/1, Batch 74780/98059, Loss: 3.0117\n",
            "Epoch 1/1, Batch 74800/98059, Loss: 3.1495\n",
            "Epoch 1/1, Batch 74820/98059, Loss: 3.0908\n",
            "Epoch 1/1, Batch 74840/98059, Loss: 2.7941\n",
            "Epoch 1/1, Batch 74860/98059, Loss: 2.7137\n",
            "Epoch 1/1, Batch 74880/98059, Loss: 3.0950\n",
            "Epoch 1/1, Batch 74900/98059, Loss: 3.0209\n",
            "Epoch 1/1, Batch 74920/98059, Loss: 2.9935\n",
            "Epoch 1/1, Batch 74940/98059, Loss: 2.9578\n",
            "Epoch 1/1, Batch 74960/98059, Loss: 2.9406\n",
            "Epoch 1/1, Batch 74980/98059, Loss: 3.2472\n",
            "Epoch 1/1, Batch 75000/98059, Loss: 3.2405\n",
            "Epoch 1/1, Batch 75020/98059, Loss: 2.9776\n",
            "Epoch 1/1, Batch 75040/98059, Loss: 3.1779\n",
            "Epoch 1/1, Batch 75060/98059, Loss: 2.7635\n",
            "Epoch 1/1, Batch 75080/98059, Loss: 2.9276\n",
            "Epoch 1/1, Batch 75100/98059, Loss: 2.7249\n",
            "Epoch 1/1, Batch 75120/98059, Loss: 2.9709\n",
            "Epoch 1/1, Batch 75140/98059, Loss: 3.2603\n",
            "Epoch 1/1, Batch 75160/98059, Loss: 3.2803\n",
            "Epoch 1/1, Batch 75180/98059, Loss: 2.9581\n",
            "Epoch 1/1, Batch 75200/98059, Loss: 3.0058\n",
            "Epoch 1/1, Batch 75220/98059, Loss: 3.2347\n",
            "Epoch 1/1, Batch 75240/98059, Loss: 3.1180\n",
            "Epoch 1/1, Batch 75260/98059, Loss: 2.6984\n",
            "Epoch 1/1, Batch 75280/98059, Loss: 3.0337\n",
            "Epoch 1/1, Batch 75300/98059, Loss: 2.7813\n",
            "Epoch 1/1, Batch 75320/98059, Loss: 2.9123\n",
            "Epoch 1/1, Batch 75340/98059, Loss: 2.9765\n",
            "Epoch 1/1, Batch 75360/98059, Loss: 2.6350\n",
            "Epoch 1/1, Batch 75380/98059, Loss: 2.9764\n",
            "Epoch 1/1, Batch 75400/98059, Loss: 2.4711\n",
            "Epoch 1/1, Batch 75420/98059, Loss: 2.7968\n",
            "Epoch 1/1, Batch 75440/98059, Loss: 3.0436\n",
            "Epoch 1/1, Batch 75460/98059, Loss: 3.0018\n",
            "Epoch 1/1, Batch 75480/98059, Loss: 3.2349\n",
            "Epoch 1/1, Batch 75500/98059, Loss: 2.9529\n",
            "Epoch 1/1, Batch 75520/98059, Loss: 2.7590\n",
            "Epoch 1/1, Batch 75540/98059, Loss: 3.3012\n",
            "Epoch 1/1, Batch 75560/98059, Loss: 3.0216\n",
            "Epoch 1/1, Batch 75580/98059, Loss: 3.2521\n",
            "Epoch 1/1, Batch 75600/98059, Loss: 3.0192\n",
            "Epoch 1/1, Batch 75620/98059, Loss: 2.9518\n",
            "Epoch 1/1, Batch 75640/98059, Loss: 3.0309\n",
            "Epoch 1/1, Batch 75660/98059, Loss: 3.1747\n",
            "Epoch 1/1, Batch 75680/98059, Loss: 2.6455\n",
            "Epoch 1/1, Batch 75700/98059, Loss: 2.6796\n",
            "Epoch 1/1, Batch 75720/98059, Loss: 2.8643\n",
            "Epoch 1/1, Batch 75740/98059, Loss: 2.9430\n",
            "Epoch 1/1, Batch 75760/98059, Loss: 2.8250\n",
            "Epoch 1/1, Batch 75780/98059, Loss: 2.8415\n",
            "Epoch 1/1, Batch 75800/98059, Loss: 3.0851\n",
            "Epoch 1/1, Batch 75820/98059, Loss: 3.0218\n",
            "Epoch 1/1, Batch 75840/98059, Loss: 2.8293\n",
            "Epoch 1/1, Batch 75860/98059, Loss: 2.8113\n",
            "Epoch 1/1, Batch 75880/98059, Loss: 2.7754\n",
            "Epoch 1/1, Batch 75900/98059, Loss: 2.7159\n",
            "Epoch 1/1, Batch 75920/98059, Loss: 2.9583\n",
            "Epoch 1/1, Batch 75940/98059, Loss: 2.7772\n",
            "Epoch 1/1, Batch 75960/98059, Loss: 2.8254\n",
            "Epoch 1/1, Batch 75980/98059, Loss: 2.9790\n",
            "Epoch 1/1, Batch 76000/98059, Loss: 3.1686\n",
            "Epoch 1/1, Batch 76020/98059, Loss: 3.1342\n",
            "Epoch 1/1, Batch 76040/98059, Loss: 2.8328\n",
            "Epoch 1/1, Batch 76060/98059, Loss: 3.1544\n",
            "Epoch 1/1, Batch 76080/98059, Loss: 2.7997\n",
            "Epoch 1/1, Batch 76100/98059, Loss: 3.1305\n",
            "Epoch 1/1, Batch 76120/98059, Loss: 2.9058\n",
            "Epoch 1/1, Batch 76140/98059, Loss: 2.8730\n",
            "Epoch 1/1, Batch 76160/98059, Loss: 2.7688\n",
            "Epoch 1/1, Batch 76180/98059, Loss: 3.0442\n",
            "Epoch 1/1, Batch 76200/98059, Loss: 3.2756\n",
            "Epoch 1/1, Batch 76220/98059, Loss: 3.0432\n",
            "Epoch 1/1, Batch 76240/98059, Loss: 3.1934\n",
            "Epoch 1/1, Batch 76260/98059, Loss: 3.0330\n",
            "Epoch 1/1, Batch 76280/98059, Loss: 2.7572\n",
            "Epoch 1/1, Batch 76300/98059, Loss: 2.9451\n",
            "Epoch 1/1, Batch 76320/98059, Loss: 2.3957\n",
            "Epoch 1/1, Batch 76340/98059, Loss: 2.6592\n",
            "Epoch 1/1, Batch 76360/98059, Loss: 2.8007\n",
            "Epoch 1/1, Batch 76380/98059, Loss: 2.9332\n",
            "Epoch 1/1, Batch 76400/98059, Loss: 2.7876\n",
            "Epoch 1/1, Batch 76420/98059, Loss: 3.0371\n",
            "Epoch 1/1, Batch 76440/98059, Loss: 2.6502\n",
            "Epoch 1/1, Batch 76460/98059, Loss: 2.8422\n",
            "Epoch 1/1, Batch 76480/98059, Loss: 3.0785\n",
            "Epoch 1/1, Batch 76500/98059, Loss: 2.9734\n",
            "Epoch 1/1, Batch 76520/98059, Loss: 3.0905\n",
            "Epoch 1/1, Batch 76540/98059, Loss: 3.0615\n",
            "Epoch 1/1, Batch 76560/98059, Loss: 3.2286\n",
            "Epoch 1/1, Batch 76580/98059, Loss: 2.8954\n",
            "Epoch 1/1, Batch 76600/98059, Loss: 2.9197\n",
            "Epoch 1/1, Batch 76620/98059, Loss: 2.8780\n",
            "Epoch 1/1, Batch 76640/98059, Loss: 2.7981\n",
            "Epoch 1/1, Batch 76660/98059, Loss: 3.0545\n",
            "Epoch 1/1, Batch 76680/98059, Loss: 3.0173\n",
            "Epoch 1/1, Batch 76700/98059, Loss: 2.7167\n",
            "Epoch 1/1, Batch 76720/98059, Loss: 2.8860\n",
            "Epoch 1/1, Batch 76740/98059, Loss: 3.1031\n",
            "Epoch 1/1, Batch 76760/98059, Loss: 2.8978\n",
            "Epoch 1/1, Batch 76780/98059, Loss: 2.9647\n",
            "Epoch 1/1, Batch 76800/98059, Loss: 2.8615\n",
            "Epoch 1/1, Batch 76820/98059, Loss: 2.9397\n",
            "Epoch 1/1, Batch 76840/98059, Loss: 2.7471\n",
            "Epoch 1/1, Batch 76860/98059, Loss: 3.0407\n",
            "Epoch 1/1, Batch 76880/98059, Loss: 2.9875\n",
            "Epoch 1/1, Batch 76900/98059, Loss: 3.3906\n",
            "Epoch 1/1, Batch 76920/98059, Loss: 3.5323\n",
            "Epoch 1/1, Batch 76940/98059, Loss: 3.0341\n",
            "Epoch 1/1, Batch 76960/98059, Loss: 2.8664\n",
            "Epoch 1/1, Batch 76980/98059, Loss: 3.1302\n",
            "Epoch 1/1, Batch 77000/98059, Loss: 3.0681\n",
            "Epoch 1/1, Batch 77020/98059, Loss: 3.3227\n",
            "Epoch 1/1, Batch 77040/98059, Loss: 2.9029\n",
            "Epoch 1/1, Batch 77060/98059, Loss: 3.2004\n",
            "Epoch 1/1, Batch 77080/98059, Loss: 3.2052\n",
            "Epoch 1/1, Batch 77100/98059, Loss: 3.1906\n",
            "Epoch 1/1, Batch 77120/98059, Loss: 2.7882\n",
            "Epoch 1/1, Batch 77140/98059, Loss: 3.2470\n",
            "Epoch 1/1, Batch 77160/98059, Loss: 2.6906\n",
            "Epoch 1/1, Batch 77180/98059, Loss: 2.8364\n",
            "Epoch 1/1, Batch 77200/98059, Loss: 2.8560\n",
            "Epoch 1/1, Batch 77220/98059, Loss: 3.3367\n",
            "Epoch 1/1, Batch 77240/98059, Loss: 3.1255\n",
            "Epoch 1/1, Batch 77260/98059, Loss: 2.7398\n",
            "Epoch 1/1, Batch 77280/98059, Loss: 3.1888\n",
            "Epoch 1/1, Batch 77300/98059, Loss: 2.7960\n",
            "Epoch 1/1, Batch 77320/98059, Loss: 2.9316\n",
            "Epoch 1/1, Batch 77340/98059, Loss: 3.3564\n",
            "Epoch 1/1, Batch 77360/98059, Loss: 3.1782\n",
            "Epoch 1/1, Batch 77380/98059, Loss: 2.9849\n",
            "Epoch 1/1, Batch 77400/98059, Loss: 3.3815\n",
            "Epoch 1/1, Batch 77420/98059, Loss: 3.0933\n",
            "Epoch 1/1, Batch 77440/98059, Loss: 2.9413\n",
            "Epoch 1/1, Batch 77460/98059, Loss: 2.9940\n",
            "Epoch 1/1, Batch 77480/98059, Loss: 3.1149\n",
            "Epoch 1/1, Batch 77500/98059, Loss: 2.9114\n",
            "Epoch 1/1, Batch 77520/98059, Loss: 3.1275\n",
            "Epoch 1/1, Batch 77540/98059, Loss: 3.1840\n",
            "Epoch 1/1, Batch 77560/98059, Loss: 3.2224\n",
            "Epoch 1/1, Batch 77580/98059, Loss: 3.1146\n",
            "Epoch 1/1, Batch 77600/98059, Loss: 2.8667\n",
            "Epoch 1/1, Batch 77620/98059, Loss: 2.9472\n",
            "Epoch 1/1, Batch 77640/98059, Loss: 2.4532\n",
            "Epoch 1/1, Batch 77660/98059, Loss: 3.2029\n",
            "Epoch 1/1, Batch 77680/98059, Loss: 2.8918\n",
            "Epoch 1/1, Batch 77700/98059, Loss: 2.8846\n",
            "Epoch 1/1, Batch 77720/98059, Loss: 3.2406\n",
            "Epoch 1/1, Batch 77740/98059, Loss: 2.9923\n",
            "Epoch 1/1, Batch 77760/98059, Loss: 2.9347\n",
            "Epoch 1/1, Batch 77780/98059, Loss: 3.2086\n",
            "Epoch 1/1, Batch 77800/98059, Loss: 2.7680\n",
            "Epoch 1/1, Batch 77820/98059, Loss: 3.0438\n",
            "Epoch 1/1, Batch 77840/98059, Loss: 2.8620\n",
            "Epoch 1/1, Batch 77860/98059, Loss: 3.2438\n",
            "Epoch 1/1, Batch 77880/98059, Loss: 2.9819\n",
            "Epoch 1/1, Batch 77900/98059, Loss: 2.9396\n",
            "Epoch 1/1, Batch 77920/98059, Loss: 3.0179\n",
            "Epoch 1/1, Batch 77940/98059, Loss: 3.1196\n",
            "Epoch 1/1, Batch 77960/98059, Loss: 3.2703\n",
            "Epoch 1/1, Batch 77980/98059, Loss: 3.1339\n",
            "Epoch 1/1, Batch 78000/98059, Loss: 3.2847\n",
            "Epoch 1/1, Batch 78020/98059, Loss: 3.0420\n",
            "Epoch 1/1, Batch 78040/98059, Loss: 3.2063\n",
            "Epoch 1/1, Batch 78060/98059, Loss: 2.6947\n",
            "Epoch 1/1, Batch 78080/98059, Loss: 2.9997\n",
            "Epoch 1/1, Batch 78100/98059, Loss: 2.6649\n",
            "Epoch 1/1, Batch 78120/98059, Loss: 3.0291\n",
            "Epoch 1/1, Batch 78140/98059, Loss: 2.6790\n",
            "Epoch 1/1, Batch 78160/98059, Loss: 3.0624\n",
            "Epoch 1/1, Batch 78180/98059, Loss: 2.7829\n",
            "Epoch 1/1, Batch 78200/98059, Loss: 2.3720\n",
            "Epoch 1/1, Batch 78220/98059, Loss: 2.9478\n",
            "Epoch 1/1, Batch 78240/98059, Loss: 3.1787\n",
            "Epoch 1/1, Batch 78260/98059, Loss: 3.2678\n",
            "Epoch 1/1, Batch 78280/98059, Loss: 3.2162\n",
            "Epoch 1/1, Batch 78300/98059, Loss: 3.2680\n",
            "Epoch 1/1, Batch 78320/98059, Loss: 2.7334\n",
            "Epoch 1/1, Batch 78340/98059, Loss: 2.9208\n",
            "Epoch 1/1, Batch 78360/98059, Loss: 2.9252\n",
            "Epoch 1/1, Batch 78380/98059, Loss: 2.7927\n",
            "Epoch 1/1, Batch 78400/98059, Loss: 3.3348\n",
            "Epoch 1/1, Batch 78420/98059, Loss: 3.0079\n",
            "Epoch 1/1, Batch 78440/98059, Loss: 2.3939\n",
            "Epoch 1/1, Batch 78460/98059, Loss: 2.9653\n",
            "Epoch 1/1, Batch 78480/98059, Loss: 2.8051\n",
            "Epoch 1/1, Batch 78500/98059, Loss: 3.0590\n",
            "Epoch 1/1, Batch 78520/98059, Loss: 3.2119\n",
            "Epoch 1/1, Batch 78540/98059, Loss: 3.0380\n",
            "Epoch 1/1, Batch 78560/98059, Loss: 2.9362\n",
            "Epoch 1/1, Batch 78580/98059, Loss: 3.0528\n",
            "Epoch 1/1, Batch 78600/98059, Loss: 2.8102\n",
            "Epoch 1/1, Batch 78620/98059, Loss: 2.7546\n",
            "Epoch 1/1, Batch 78640/98059, Loss: 3.0968\n",
            "Epoch 1/1, Batch 78660/98059, Loss: 2.8580\n",
            "Epoch 1/1, Batch 78680/98059, Loss: 2.9877\n",
            "Epoch 1/1, Batch 78700/98059, Loss: 2.6472\n",
            "Epoch 1/1, Batch 78720/98059, Loss: 2.8146\n",
            "Epoch 1/1, Batch 78740/98059, Loss: 3.0213\n",
            "Epoch 1/1, Batch 78760/98059, Loss: 3.0764\n",
            "Epoch 1/1, Batch 78780/98059, Loss: 2.8897\n",
            "Epoch 1/1, Batch 78800/98059, Loss: 2.8990\n",
            "Epoch 1/1, Batch 78820/98059, Loss: 2.8470\n",
            "Epoch 1/1, Batch 78840/98059, Loss: 2.8813\n",
            "Epoch 1/1, Batch 78860/98059, Loss: 3.2228\n",
            "Epoch 1/1, Batch 78880/98059, Loss: 2.9866\n",
            "Epoch 1/1, Batch 78900/98059, Loss: 3.0639\n",
            "Epoch 1/1, Batch 78920/98059, Loss: 3.2597\n",
            "Epoch 1/1, Batch 78940/98059, Loss: 2.9223\n",
            "Epoch 1/1, Batch 78960/98059, Loss: 3.0913\n",
            "Epoch 1/1, Batch 78980/98059, Loss: 2.5850\n",
            "Epoch 1/1, Batch 79000/98059, Loss: 3.0852\n",
            "Epoch 1/1, Batch 79020/98059, Loss: 2.9768\n",
            "Epoch 1/1, Batch 79040/98059, Loss: 3.0892\n",
            "Epoch 1/1, Batch 79060/98059, Loss: 2.9217\n",
            "Epoch 1/1, Batch 79080/98059, Loss: 3.0440\n",
            "Epoch 1/1, Batch 79100/98059, Loss: 3.0208\n",
            "Epoch 1/1, Batch 79120/98059, Loss: 3.0529\n",
            "Epoch 1/1, Batch 79140/98059, Loss: 2.9955\n",
            "Epoch 1/1, Batch 79160/98059, Loss: 2.8280\n",
            "Epoch 1/1, Batch 79180/98059, Loss: 3.2557\n",
            "Epoch 1/1, Batch 79200/98059, Loss: 3.0886\n",
            "Epoch 1/1, Batch 79220/98059, Loss: 2.6932\n",
            "Epoch 1/1, Batch 79240/98059, Loss: 2.7694\n",
            "Epoch 1/1, Batch 79260/98059, Loss: 3.0076\n",
            "Epoch 1/1, Batch 79280/98059, Loss: 3.3030\n",
            "Epoch 1/1, Batch 79300/98059, Loss: 2.7734\n",
            "Epoch 1/1, Batch 79320/98059, Loss: 2.8702\n",
            "Epoch 1/1, Batch 79340/98059, Loss: 2.8083\n",
            "Epoch 1/1, Batch 79360/98059, Loss: 3.1303\n",
            "Epoch 1/1, Batch 79380/98059, Loss: 2.7320\n",
            "Epoch 1/1, Batch 79400/98059, Loss: 3.0475\n",
            "Epoch 1/1, Batch 79420/98059, Loss: 2.9564\n",
            "Epoch 1/1, Batch 79440/98059, Loss: 3.2567\n",
            "Epoch 1/1, Batch 79460/98059, Loss: 2.7812\n",
            "Epoch 1/1, Batch 79480/98059, Loss: 2.8636\n",
            "Epoch 1/1, Batch 79500/98059, Loss: 2.7906\n",
            "Epoch 1/1, Batch 79520/98059, Loss: 3.0794\n",
            "Epoch 1/1, Batch 79540/98059, Loss: 2.6829\n",
            "Epoch 1/1, Batch 79560/98059, Loss: 3.0485\n",
            "Epoch 1/1, Batch 79580/98059, Loss: 2.9740\n",
            "Epoch 1/1, Batch 79600/98059, Loss: 3.0332\n",
            "Epoch 1/1, Batch 79620/98059, Loss: 2.8667\n",
            "Epoch 1/1, Batch 79640/98059, Loss: 3.1003\n",
            "Epoch 1/1, Batch 79660/98059, Loss: 3.0188\n",
            "Epoch 1/1, Batch 79680/98059, Loss: 3.1490\n",
            "Epoch 1/1, Batch 79700/98059, Loss: 3.0001\n",
            "Epoch 1/1, Batch 79720/98059, Loss: 3.1031\n",
            "Epoch 1/1, Batch 79740/98059, Loss: 3.0579\n",
            "Epoch 1/1, Batch 79760/98059, Loss: 3.1116\n",
            "Epoch 1/1, Batch 79780/98059, Loss: 2.9733\n",
            "Epoch 1/1, Batch 79800/98059, Loss: 3.0120\n",
            "Epoch 1/1, Batch 79820/98059, Loss: 3.1786\n",
            "Epoch 1/1, Batch 79840/98059, Loss: 2.9754\n",
            "Epoch 1/1, Batch 79860/98059, Loss: 3.2604\n",
            "Epoch 1/1, Batch 79880/98059, Loss: 2.7630\n",
            "Epoch 1/1, Batch 79900/98059, Loss: 3.0616\n",
            "Epoch 1/1, Batch 79920/98059, Loss: 3.0133\n",
            "Epoch 1/1, Batch 79940/98059, Loss: 3.1353\n",
            "Epoch 1/1, Batch 79960/98059, Loss: 2.8195\n",
            "Epoch 1/1, Batch 79980/98059, Loss: 2.9820\n",
            "Epoch 1/1, Batch 80000/98059, Loss: 2.7929\n",
            "Epoch 1/1, Batch 80020/98059, Loss: 3.1346\n",
            "Epoch 1/1, Batch 80040/98059, Loss: 2.8143\n",
            "Epoch 1/1, Batch 80060/98059, Loss: 3.0466\n",
            "Epoch 1/1, Batch 80080/98059, Loss: 2.8613\n",
            "Epoch 1/1, Batch 80100/98059, Loss: 3.0366\n",
            "Epoch 1/1, Batch 80120/98059, Loss: 2.8166\n",
            "Epoch 1/1, Batch 80140/98059, Loss: 3.0929\n",
            "Epoch 1/1, Batch 80160/98059, Loss: 2.6307\n",
            "Epoch 1/1, Batch 80180/98059, Loss: 2.8489\n",
            "Epoch 1/1, Batch 80200/98059, Loss: 2.8399\n",
            "Epoch 1/1, Batch 80220/98059, Loss: 3.1303\n",
            "Epoch 1/1, Batch 80240/98059, Loss: 3.0134\n",
            "Epoch 1/1, Batch 80260/98059, Loss: 3.2013\n",
            "Epoch 1/1, Batch 80280/98059, Loss: 3.3177\n",
            "Epoch 1/1, Batch 80300/98059, Loss: 3.0036\n",
            "Epoch 1/1, Batch 80320/98059, Loss: 2.9624\n",
            "Epoch 1/1, Batch 80340/98059, Loss: 2.4946\n",
            "Epoch 1/1, Batch 80360/98059, Loss: 2.9393\n",
            "Epoch 1/1, Batch 80380/98059, Loss: 3.1046\n",
            "Epoch 1/1, Batch 80400/98059, Loss: 3.1135\n",
            "Epoch 1/1, Batch 80420/98059, Loss: 2.9325\n",
            "Epoch 1/1, Batch 80440/98059, Loss: 3.2289\n",
            "Epoch 1/1, Batch 80460/98059, Loss: 3.0305\n",
            "Epoch 1/1, Batch 80480/98059, Loss: 2.9261\n",
            "Epoch 1/1, Batch 80500/98059, Loss: 2.9800\n",
            "Epoch 1/1, Batch 80520/98059, Loss: 3.3569\n",
            "Epoch 1/1, Batch 80540/98059, Loss: 2.6964\n",
            "Epoch 1/1, Batch 80560/98059, Loss: 3.1987\n",
            "Epoch 1/1, Batch 80580/98059, Loss: 2.9051\n",
            "Epoch 1/1, Batch 80600/98059, Loss: 3.1200\n",
            "Epoch 1/1, Batch 80620/98059, Loss: 2.9363\n",
            "Epoch 1/1, Batch 80640/98059, Loss: 2.9358\n",
            "Epoch 1/1, Batch 80660/98059, Loss: 2.8910\n",
            "Epoch 1/1, Batch 80680/98059, Loss: 2.8807\n",
            "Epoch 1/1, Batch 80700/98059, Loss: 2.6912\n",
            "Epoch 1/1, Batch 80720/98059, Loss: 2.6853\n",
            "Epoch 1/1, Batch 80740/98059, Loss: 3.0787\n",
            "Epoch 1/1, Batch 80760/98059, Loss: 3.2826\n",
            "Epoch 1/1, Batch 80780/98059, Loss: 3.0495\n",
            "Epoch 1/1, Batch 80800/98059, Loss: 3.0684\n",
            "Epoch 1/1, Batch 80820/98059, Loss: 3.1569\n",
            "Epoch 1/1, Batch 80840/98059, Loss: 2.8205\n",
            "Epoch 1/1, Batch 80860/98059, Loss: 2.8803\n",
            "Epoch 1/1, Batch 80880/98059, Loss: 3.0866\n",
            "Epoch 1/1, Batch 80900/98059, Loss: 2.9122\n",
            "Epoch 1/1, Batch 80920/98059, Loss: 3.1649\n",
            "Epoch 1/1, Batch 80940/98059, Loss: 2.9594\n",
            "Epoch 1/1, Batch 80960/98059, Loss: 2.9613\n",
            "Epoch 1/1, Batch 80980/98059, Loss: 3.1519\n",
            "Epoch 1/1, Batch 81000/98059, Loss: 2.7515\n",
            "Epoch 1/1, Batch 81020/98059, Loss: 2.9786\n",
            "Epoch 1/1, Batch 81040/98059, Loss: 2.4594\n",
            "Epoch 1/1, Batch 81060/98059, Loss: 3.0332\n",
            "Epoch 1/1, Batch 81080/98059, Loss: 2.9845\n",
            "Epoch 1/1, Batch 81100/98059, Loss: 2.9645\n",
            "Epoch 1/1, Batch 81120/98059, Loss: 3.2848\n",
            "Epoch 1/1, Batch 81140/98059, Loss: 3.0262\n",
            "Epoch 1/1, Batch 81160/98059, Loss: 2.8294\n",
            "Epoch 1/1, Batch 81180/98059, Loss: 2.8280\n",
            "Epoch 1/1, Batch 81200/98059, Loss: 2.9080\n",
            "Epoch 1/1, Batch 81220/98059, Loss: 3.1061\n",
            "Epoch 1/1, Batch 81240/98059, Loss: 2.7506\n",
            "Epoch 1/1, Batch 81260/98059, Loss: 2.9323\n",
            "Epoch 1/1, Batch 81280/98059, Loss: 3.2797\n",
            "Epoch 1/1, Batch 81300/98059, Loss: 3.1650\n",
            "Epoch 1/1, Batch 81320/98059, Loss: 2.3600\n",
            "Epoch 1/1, Batch 81340/98059, Loss: 3.2057\n",
            "Epoch 1/1, Batch 81360/98059, Loss: 2.7902\n",
            "Epoch 1/1, Batch 81380/98059, Loss: 2.9343\n",
            "Epoch 1/1, Batch 81400/98059, Loss: 2.7918\n",
            "Epoch 1/1, Batch 81420/98059, Loss: 2.9186\n",
            "Epoch 1/1, Batch 81440/98059, Loss: 2.7336\n",
            "Epoch 1/1, Batch 81460/98059, Loss: 3.3107\n",
            "Epoch 1/1, Batch 81480/98059, Loss: 3.3188\n",
            "Epoch 1/1, Batch 81500/98059, Loss: 2.8553\n",
            "Epoch 1/1, Batch 81520/98059, Loss: 2.2833\n",
            "Epoch 1/1, Batch 81540/98059, Loss: 2.8929\n",
            "Epoch 1/1, Batch 81560/98059, Loss: 2.9320\n",
            "Epoch 1/1, Batch 81580/98059, Loss: 2.6913\n",
            "Epoch 1/1, Batch 81600/98059, Loss: 3.1023\n",
            "Epoch 1/1, Batch 81620/98059, Loss: 2.9663\n",
            "Epoch 1/1, Batch 81640/98059, Loss: 2.9284\n",
            "Epoch 1/1, Batch 81660/98059, Loss: 2.5229\n",
            "Epoch 1/1, Batch 81680/98059, Loss: 2.9157\n",
            "Epoch 1/1, Batch 81700/98059, Loss: 2.8512\n",
            "Epoch 1/1, Batch 81720/98059, Loss: 3.3117\n",
            "Epoch 1/1, Batch 81740/98059, Loss: 2.9119\n",
            "Epoch 1/1, Batch 81760/98059, Loss: 2.9711\n",
            "Epoch 1/1, Batch 81780/98059, Loss: 3.2604\n",
            "Epoch 1/1, Batch 81800/98059, Loss: 2.8193\n",
            "Epoch 1/1, Batch 81820/98059, Loss: 3.1817\n",
            "Epoch 1/1, Batch 81840/98059, Loss: 2.9837\n",
            "Epoch 1/1, Batch 81860/98059, Loss: 3.1918\n",
            "Epoch 1/1, Batch 81880/98059, Loss: 2.5830\n",
            "Epoch 1/1, Batch 81900/98059, Loss: 2.7685\n",
            "Epoch 1/1, Batch 81920/98059, Loss: 2.7146\n",
            "Epoch 1/1, Batch 81940/98059, Loss: 2.9700\n",
            "Epoch 1/1, Batch 81960/98059, Loss: 3.1179\n",
            "Epoch 1/1, Batch 81980/98059, Loss: 2.9923\n",
            "Epoch 1/1, Batch 82000/98059, Loss: 2.9877\n",
            "Epoch 1/1, Batch 82020/98059, Loss: 3.0672\n",
            "Epoch 1/1, Batch 82040/98059, Loss: 2.4396\n",
            "Epoch 1/1, Batch 82060/98059, Loss: 2.8391\n",
            "Epoch 1/1, Batch 82080/98059, Loss: 2.9644\n",
            "Epoch 1/1, Batch 82100/98059, Loss: 3.1506\n",
            "Epoch 1/1, Batch 82120/98059, Loss: 2.5337\n",
            "Epoch 1/1, Batch 82140/98059, Loss: 3.0881\n",
            "Epoch 1/1, Batch 82160/98059, Loss: 2.8352\n",
            "Epoch 1/1, Batch 82180/98059, Loss: 2.7257\n",
            "Epoch 1/1, Batch 82200/98059, Loss: 2.8852\n",
            "Epoch 1/1, Batch 82220/98059, Loss: 3.0817\n",
            "Epoch 1/1, Batch 82240/98059, Loss: 2.8909\n",
            "Epoch 1/1, Batch 82260/98059, Loss: 3.1472\n",
            "Epoch 1/1, Batch 82280/98059, Loss: 2.8553\n",
            "Epoch 1/1, Batch 82300/98059, Loss: 2.9930\n",
            "Epoch 1/1, Batch 82320/98059, Loss: 3.1212\n",
            "Epoch 1/1, Batch 82340/98059, Loss: 2.8272\n",
            "Epoch 1/1, Batch 82360/98059, Loss: 2.9474\n",
            "Epoch 1/1, Batch 82380/98059, Loss: 3.1374\n",
            "Epoch 1/1, Batch 82400/98059, Loss: 3.0915\n",
            "Epoch 1/1, Batch 82420/98059, Loss: 2.8990\n",
            "Epoch 1/1, Batch 82440/98059, Loss: 2.7417\n",
            "Epoch 1/1, Batch 82460/98059, Loss: 2.9915\n",
            "Epoch 1/1, Batch 82480/98059, Loss: 2.6100\n",
            "Epoch 1/1, Batch 82500/98059, Loss: 3.1338\n",
            "Epoch 1/1, Batch 82520/98059, Loss: 2.9969\n",
            "Epoch 1/1, Batch 82540/98059, Loss: 3.0723\n",
            "Epoch 1/1, Batch 82560/98059, Loss: 2.8269\n",
            "Epoch 1/1, Batch 82580/98059, Loss: 3.1629\n",
            "Epoch 1/1, Batch 82600/98059, Loss: 3.0627\n",
            "Epoch 1/1, Batch 82620/98059, Loss: 2.5820\n",
            "Epoch 1/1, Batch 82640/98059, Loss: 2.8744\n",
            "Epoch 1/1, Batch 82660/98059, Loss: 2.9853\n",
            "Epoch 1/1, Batch 82680/98059, Loss: 3.0796\n",
            "Epoch 1/1, Batch 82700/98059, Loss: 2.8459\n",
            "Epoch 1/1, Batch 82720/98059, Loss: 2.7820\n",
            "Epoch 1/1, Batch 82740/98059, Loss: 3.1519\n",
            "Epoch 1/1, Batch 82760/98059, Loss: 2.5378\n",
            "Epoch 1/1, Batch 82780/98059, Loss: 2.8006\n",
            "Epoch 1/1, Batch 82800/98059, Loss: 3.0141\n",
            "Epoch 1/1, Batch 82820/98059, Loss: 2.9494\n",
            "Epoch 1/1, Batch 82840/98059, Loss: 2.8846\n",
            "Epoch 1/1, Batch 82860/98059, Loss: 3.1003\n",
            "Epoch 1/1, Batch 82880/98059, Loss: 3.2240\n",
            "Epoch 1/1, Batch 82900/98059, Loss: 3.1761\n",
            "Epoch 1/1, Batch 82920/98059, Loss: 2.9359\n",
            "Epoch 1/1, Batch 82940/98059, Loss: 2.7553\n",
            "Epoch 1/1, Batch 82960/98059, Loss: 2.7462\n",
            "Epoch 1/1, Batch 82980/98059, Loss: 2.8781\n",
            "Epoch 1/1, Batch 83000/98059, Loss: 2.7732\n",
            "Epoch 1/1, Batch 83020/98059, Loss: 2.8011\n",
            "Epoch 1/1, Batch 83040/98059, Loss: 2.8559\n",
            "Epoch 1/1, Batch 83060/98059, Loss: 3.1612\n",
            "Epoch 1/1, Batch 83080/98059, Loss: 2.9518\n",
            "Epoch 1/1, Batch 83100/98059, Loss: 2.9177\n",
            "Epoch 1/1, Batch 83120/98059, Loss: 2.9093\n",
            "Epoch 1/1, Batch 83140/98059, Loss: 2.9214\n",
            "Epoch 1/1, Batch 83160/98059, Loss: 3.0729\n",
            "Epoch 1/1, Batch 83180/98059, Loss: 3.1238\n",
            "Epoch 1/1, Batch 83200/98059, Loss: 2.9208\n",
            "Epoch 1/1, Batch 83220/98059, Loss: 2.8845\n",
            "Epoch 1/1, Batch 83240/98059, Loss: 2.8560\n",
            "Epoch 1/1, Batch 83260/98059, Loss: 3.0319\n",
            "Epoch 1/1, Batch 83280/98059, Loss: 3.1132\n",
            "Epoch 1/1, Batch 83300/98059, Loss: 2.8053\n",
            "Epoch 1/1, Batch 83320/98059, Loss: 2.8520\n",
            "Epoch 1/1, Batch 83340/98059, Loss: 3.0865\n",
            "Epoch 1/1, Batch 83360/98059, Loss: 3.0916\n",
            "Epoch 1/1, Batch 83380/98059, Loss: 2.7036\n",
            "Epoch 1/1, Batch 83400/98059, Loss: 2.9787\n",
            "Epoch 1/1, Batch 83420/98059, Loss: 3.0054\n",
            "Epoch 1/1, Batch 83440/98059, Loss: 3.0352\n",
            "Epoch 1/1, Batch 83460/98059, Loss: 2.7793\n",
            "Epoch 1/1, Batch 83480/98059, Loss: 2.7877\n",
            "Epoch 1/1, Batch 83500/98059, Loss: 3.1298\n",
            "Epoch 1/1, Batch 83520/98059, Loss: 3.2146\n",
            "Epoch 1/1, Batch 83540/98059, Loss: 2.8381\n",
            "Epoch 1/1, Batch 83560/98059, Loss: 2.8715\n",
            "Epoch 1/1, Batch 83580/98059, Loss: 3.0839\n",
            "Epoch 1/1, Batch 83600/98059, Loss: 3.3687\n",
            "Epoch 1/1, Batch 83620/98059, Loss: 3.2957\n",
            "Epoch 1/1, Batch 83640/98059, Loss: 3.0022\n",
            "Epoch 1/1, Batch 83660/98059, Loss: 3.0141\n",
            "Epoch 1/1, Batch 83680/98059, Loss: 2.9809\n",
            "Epoch 1/1, Batch 83700/98059, Loss: 3.1095\n",
            "Epoch 1/1, Batch 83720/98059, Loss: 3.0901\n",
            "Epoch 1/1, Batch 83740/98059, Loss: 2.6059\n",
            "Epoch 1/1, Batch 83760/98059, Loss: 2.9024\n",
            "Epoch 1/1, Batch 83780/98059, Loss: 3.0380\n",
            "Epoch 1/1, Batch 83800/98059, Loss: 3.0723\n",
            "Epoch 1/1, Batch 83820/98059, Loss: 3.0333\n",
            "Epoch 1/1, Batch 83840/98059, Loss: 2.7727\n",
            "Epoch 1/1, Batch 83860/98059, Loss: 2.8982\n",
            "Epoch 1/1, Batch 83880/98059, Loss: 3.0004\n",
            "Epoch 1/1, Batch 83900/98059, Loss: 2.9790\n",
            "Epoch 1/1, Batch 83920/98059, Loss: 3.0073\n",
            "Epoch 1/1, Batch 83940/98059, Loss: 2.9875\n",
            "Epoch 1/1, Batch 83960/98059, Loss: 2.8251\n",
            "Epoch 1/1, Batch 83980/98059, Loss: 3.0701\n",
            "Epoch 1/1, Batch 84000/98059, Loss: 3.3314\n",
            "Epoch 1/1, Batch 84020/98059, Loss: 3.0977\n",
            "Epoch 1/1, Batch 84040/98059, Loss: 3.1950\n",
            "Epoch 1/1, Batch 84060/98059, Loss: 3.1504\n",
            "Epoch 1/1, Batch 84080/98059, Loss: 2.9608\n",
            "Epoch 1/1, Batch 84100/98059, Loss: 2.5927\n",
            "Epoch 1/1, Batch 84120/98059, Loss: 2.5786\n",
            "Epoch 1/1, Batch 84140/98059, Loss: 3.1221\n",
            "Epoch 1/1, Batch 84160/98059, Loss: 2.9750\n",
            "Epoch 1/1, Batch 84180/98059, Loss: 2.8598\n",
            "Epoch 1/1, Batch 84200/98059, Loss: 2.9716\n",
            "Epoch 1/1, Batch 84220/98059, Loss: 3.0190\n",
            "Epoch 1/1, Batch 84240/98059, Loss: 3.0391\n",
            "Epoch 1/1, Batch 84260/98059, Loss: 2.9817\n",
            "Epoch 1/1, Batch 84280/98059, Loss: 2.9457\n",
            "Epoch 1/1, Batch 84300/98059, Loss: 2.8137\n",
            "Epoch 1/1, Batch 84320/98059, Loss: 2.7024\n",
            "Epoch 1/1, Batch 84340/98059, Loss: 2.9781\n",
            "Epoch 1/1, Batch 84360/98059, Loss: 3.1017\n",
            "Epoch 1/1, Batch 84380/98059, Loss: 2.7871\n",
            "Epoch 1/1, Batch 84400/98059, Loss: 2.9141\n",
            "Epoch 1/1, Batch 84420/98059, Loss: 3.1274\n",
            "Epoch 1/1, Batch 84440/98059, Loss: 2.7360\n",
            "Epoch 1/1, Batch 84460/98059, Loss: 3.1234\n",
            "Epoch 1/1, Batch 84480/98059, Loss: 2.4786\n",
            "Epoch 1/1, Batch 84500/98059, Loss: 2.9933\n",
            "Epoch 1/1, Batch 84520/98059, Loss: 2.9924\n",
            "Epoch 1/1, Batch 84540/98059, Loss: 3.0823\n",
            "Epoch 1/1, Batch 84560/98059, Loss: 3.2879\n",
            "Epoch 1/1, Batch 84580/98059, Loss: 2.3525\n",
            "Epoch 1/1, Batch 84600/98059, Loss: 2.8481\n",
            "Epoch 1/1, Batch 84620/98059, Loss: 2.8971\n",
            "Epoch 1/1, Batch 84640/98059, Loss: 2.8003\n",
            "Epoch 1/1, Batch 84660/98059, Loss: 3.1175\n",
            "Epoch 1/1, Batch 84680/98059, Loss: 2.9038\n",
            "Epoch 1/1, Batch 84700/98059, Loss: 3.1495\n",
            "Epoch 1/1, Batch 84720/98059, Loss: 2.9692\n",
            "Epoch 1/1, Batch 84740/98059, Loss: 3.0835\n",
            "Epoch 1/1, Batch 84760/98059, Loss: 3.0144\n",
            "Epoch 1/1, Batch 84780/98059, Loss: 3.6432\n",
            "Epoch 1/1, Batch 84800/98059, Loss: 2.9315\n",
            "Epoch 1/1, Batch 84820/98059, Loss: 2.8029\n",
            "Epoch 1/1, Batch 84840/98059, Loss: 3.1410\n",
            "Epoch 1/1, Batch 84860/98059, Loss: 2.7094\n",
            "Epoch 1/1, Batch 84880/98059, Loss: 3.4483\n",
            "Epoch 1/1, Batch 84900/98059, Loss: 2.7875\n",
            "Epoch 1/1, Batch 84920/98059, Loss: 3.0554\n",
            "Epoch 1/1, Batch 84940/98059, Loss: 3.5843\n",
            "Epoch 1/1, Batch 84960/98059, Loss: 2.7280\n",
            "Epoch 1/1, Batch 84980/98059, Loss: 3.1238\n",
            "Epoch 1/1, Batch 85000/98059, Loss: 3.1825\n",
            "Epoch 1/1, Batch 85020/98059, Loss: 2.7669\n",
            "Epoch 1/1, Batch 85040/98059, Loss: 2.7811\n",
            "Epoch 1/1, Batch 85060/98059, Loss: 3.1440\n",
            "Epoch 1/1, Batch 85080/98059, Loss: 2.8046\n",
            "Epoch 1/1, Batch 85100/98059, Loss: 2.9282\n",
            "Epoch 1/1, Batch 85120/98059, Loss: 2.9030\n",
            "Epoch 1/1, Batch 85140/98059, Loss: 3.2199\n",
            "Epoch 1/1, Batch 85160/98059, Loss: 2.9540\n",
            "Epoch 1/1, Batch 85180/98059, Loss: 2.9331\n",
            "Epoch 1/1, Batch 85200/98059, Loss: 3.2789\n",
            "Epoch 1/1, Batch 85220/98059, Loss: 2.7393\n",
            "Epoch 1/1, Batch 85240/98059, Loss: 2.9878\n",
            "Epoch 1/1, Batch 85260/98059, Loss: 3.1218\n",
            "Epoch 1/1, Batch 85280/98059, Loss: 3.2159\n",
            "Epoch 1/1, Batch 85300/98059, Loss: 3.0692\n",
            "Epoch 1/1, Batch 85320/98059, Loss: 3.2168\n",
            "Epoch 1/1, Batch 85340/98059, Loss: 3.3531\n",
            "Epoch 1/1, Batch 85360/98059, Loss: 2.9348\n",
            "Epoch 1/1, Batch 85380/98059, Loss: 2.8387\n",
            "Epoch 1/1, Batch 85400/98059, Loss: 2.8799\n",
            "Epoch 1/1, Batch 85420/98059, Loss: 2.5830\n",
            "Epoch 1/1, Batch 85440/98059, Loss: 3.1542\n",
            "Epoch 1/1, Batch 85460/98059, Loss: 2.7078\n",
            "Epoch 1/1, Batch 85480/98059, Loss: 2.7680\n",
            "Epoch 1/1, Batch 85500/98059, Loss: 2.9584\n",
            "Epoch 1/1, Batch 85520/98059, Loss: 3.4035\n",
            "Epoch 1/1, Batch 85540/98059, Loss: 3.0796\n",
            "Epoch 1/1, Batch 85560/98059, Loss: 2.8977\n",
            "Epoch 1/1, Batch 85580/98059, Loss: 2.8495\n",
            "Epoch 1/1, Batch 85600/98059, Loss: 2.6969\n",
            "Epoch 1/1, Batch 85620/98059, Loss: 2.9253\n",
            "Epoch 1/1, Batch 85640/98059, Loss: 3.2706\n",
            "Epoch 1/1, Batch 85660/98059, Loss: 2.5147\n",
            "Epoch 1/1, Batch 85680/98059, Loss: 3.0163\n",
            "Epoch 1/1, Batch 85700/98059, Loss: 3.0171\n",
            "Epoch 1/1, Batch 85720/98059, Loss: 3.1690\n",
            "Epoch 1/1, Batch 85740/98059, Loss: 2.7513\n",
            "Epoch 1/1, Batch 85760/98059, Loss: 2.5730\n",
            "Epoch 1/1, Batch 85780/98059, Loss: 2.8815\n",
            "Epoch 1/1, Batch 85800/98059, Loss: 3.1419\n",
            "Epoch 1/1, Batch 85820/98059, Loss: 2.9514\n",
            "Epoch 1/1, Batch 85840/98059, Loss: 3.0207\n",
            "Epoch 1/1, Batch 85860/98059, Loss: 3.0421\n",
            "Epoch 1/1, Batch 85880/98059, Loss: 2.8026\n",
            "Epoch 1/1, Batch 85900/98059, Loss: 3.0296\n",
            "Epoch 1/1, Batch 85920/98059, Loss: 3.2860\n",
            "Epoch 1/1, Batch 85940/98059, Loss: 3.0617\n",
            "Epoch 1/1, Batch 85960/98059, Loss: 2.8667\n",
            "Epoch 1/1, Batch 85980/98059, Loss: 2.8237\n",
            "Epoch 1/1, Batch 86000/98059, Loss: 2.8226\n",
            "Epoch 1/1, Batch 86020/98059, Loss: 2.9965\n",
            "Epoch 1/1, Batch 86040/98059, Loss: 3.0225\n",
            "Epoch 1/1, Batch 86060/98059, Loss: 3.1258\n",
            "Epoch 1/1, Batch 86080/98059, Loss: 2.8534\n",
            "Epoch 1/1, Batch 86100/98059, Loss: 2.7598\n",
            "Epoch 1/1, Batch 86120/98059, Loss: 2.9713\n",
            "Epoch 1/1, Batch 86140/98059, Loss: 2.7140\n",
            "Epoch 1/1, Batch 86160/98059, Loss: 3.0157\n",
            "Epoch 1/1, Batch 86180/98059, Loss: 2.9170\n",
            "Epoch 1/1, Batch 86200/98059, Loss: 2.8457\n",
            "Epoch 1/1, Batch 86220/98059, Loss: 3.0700\n",
            "Epoch 1/1, Batch 86240/98059, Loss: 2.8135\n",
            "Epoch 1/1, Batch 86260/98059, Loss: 3.1679\n",
            "Epoch 1/1, Batch 86280/98059, Loss: 2.6980\n",
            "Epoch 1/1, Batch 86300/98059, Loss: 3.1080\n",
            "Epoch 1/1, Batch 86320/98059, Loss: 3.1371\n",
            "Epoch 1/1, Batch 86340/98059, Loss: 3.2245\n",
            "Epoch 1/1, Batch 86360/98059, Loss: 2.7724\n",
            "Epoch 1/1, Batch 86380/98059, Loss: 3.1755\n",
            "Epoch 1/1, Batch 86400/98059, Loss: 2.9114\n",
            "Epoch 1/1, Batch 86420/98059, Loss: 2.6775\n",
            "Epoch 1/1, Batch 86440/98059, Loss: 3.1355\n",
            "Epoch 1/1, Batch 86460/98059, Loss: 2.9721\n",
            "Epoch 1/1, Batch 86480/98059, Loss: 2.9688\n",
            "Epoch 1/1, Batch 86500/98059, Loss: 2.8549\n",
            "Epoch 1/1, Batch 86520/98059, Loss: 3.0896\n",
            "Epoch 1/1, Batch 86540/98059, Loss: 3.0666\n",
            "Epoch 1/1, Batch 86560/98059, Loss: 2.8323\n",
            "Epoch 1/1, Batch 86580/98059, Loss: 2.9192\n",
            "Epoch 1/1, Batch 86600/98059, Loss: 2.9444\n",
            "Epoch 1/1, Batch 86620/98059, Loss: 2.8733\n",
            "Epoch 1/1, Batch 86640/98059, Loss: 2.7965\n",
            "Epoch 1/1, Batch 86660/98059, Loss: 2.9064\n",
            "Epoch 1/1, Batch 86680/98059, Loss: 2.6083\n",
            "Epoch 1/1, Batch 86700/98059, Loss: 3.2195\n",
            "Epoch 1/1, Batch 86720/98059, Loss: 3.0195\n",
            "Epoch 1/1, Batch 86740/98059, Loss: 2.8188\n",
            "Epoch 1/1, Batch 86760/98059, Loss: 3.3154\n",
            "Epoch 1/1, Batch 86780/98059, Loss: 3.1172\n",
            "Epoch 1/1, Batch 86800/98059, Loss: 2.9465\n",
            "Epoch 1/1, Batch 86820/98059, Loss: 2.7544\n",
            "Epoch 1/1, Batch 86840/98059, Loss: 2.7639\n",
            "Epoch 1/1, Batch 86860/98059, Loss: 3.0158\n",
            "Epoch 1/1, Batch 86880/98059, Loss: 3.1235\n",
            "Epoch 1/1, Batch 86900/98059, Loss: 2.9720\n",
            "Epoch 1/1, Batch 86920/98059, Loss: 2.9939\n",
            "Epoch 1/1, Batch 86940/98059, Loss: 2.9113\n",
            "Epoch 1/1, Batch 86960/98059, Loss: 3.0378\n",
            "Epoch 1/1, Batch 86980/98059, Loss: 2.8513\n",
            "Epoch 1/1, Batch 87000/98059, Loss: 2.8221\n",
            "Epoch 1/1, Batch 87020/98059, Loss: 3.2534\n",
            "Epoch 1/1, Batch 87040/98059, Loss: 2.5113\n",
            "Epoch 1/1, Batch 87060/98059, Loss: 3.0176\n",
            "Epoch 1/1, Batch 87080/98059, Loss: 3.2412\n",
            "Epoch 1/1, Batch 87100/98059, Loss: 2.8819\n",
            "Epoch 1/1, Batch 87120/98059, Loss: 3.2027\n",
            "Epoch 1/1, Batch 87140/98059, Loss: 3.0076\n",
            "Epoch 1/1, Batch 87160/98059, Loss: 3.1775\n",
            "Epoch 1/1, Batch 87180/98059, Loss: 2.9179\n",
            "Epoch 1/1, Batch 87200/98059, Loss: 3.0140\n",
            "Epoch 1/1, Batch 87220/98059, Loss: 2.8660\n",
            "Epoch 1/1, Batch 87240/98059, Loss: 3.1972\n",
            "Epoch 1/1, Batch 87260/98059, Loss: 3.1524\n",
            "Epoch 1/1, Batch 87280/98059, Loss: 2.7333\n",
            "Epoch 1/1, Batch 87300/98059, Loss: 2.8953\n",
            "Epoch 1/1, Batch 87320/98059, Loss: 3.1844\n",
            "Epoch 1/1, Batch 87340/98059, Loss: 2.8728\n",
            "Epoch 1/1, Batch 87360/98059, Loss: 2.6295\n",
            "Epoch 1/1, Batch 87380/98059, Loss: 2.7785\n",
            "Epoch 1/1, Batch 87400/98059, Loss: 3.0139\n",
            "Epoch 1/1, Batch 87420/98059, Loss: 3.3079\n",
            "Epoch 1/1, Batch 87440/98059, Loss: 3.0717\n",
            "Epoch 1/1, Batch 87460/98059, Loss: 2.9576\n",
            "Epoch 1/1, Batch 87480/98059, Loss: 3.1286\n",
            "Epoch 1/1, Batch 87500/98059, Loss: 3.1523\n",
            "Epoch 1/1, Batch 87520/98059, Loss: 2.8346\n",
            "Epoch 1/1, Batch 87540/98059, Loss: 2.8093\n",
            "Epoch 1/1, Batch 87560/98059, Loss: 2.7567\n",
            "Epoch 1/1, Batch 87580/98059, Loss: 2.6695\n",
            "Epoch 1/1, Batch 87600/98059, Loss: 2.9984\n",
            "Epoch 1/1, Batch 87620/98059, Loss: 2.8827\n",
            "Epoch 1/1, Batch 87640/98059, Loss: 2.7714\n",
            "Epoch 1/1, Batch 87660/98059, Loss: 2.3319\n",
            "Epoch 1/1, Batch 87680/98059, Loss: 2.8105\n",
            "Epoch 1/1, Batch 87700/98059, Loss: 2.5494\n",
            "Epoch 1/1, Batch 87720/98059, Loss: 3.1793\n",
            "Epoch 1/1, Batch 87740/98059, Loss: 2.9717\n",
            "Epoch 1/1, Batch 87760/98059, Loss: 3.2110\n",
            "Epoch 1/1, Batch 87780/98059, Loss: 3.3023\n",
            "Epoch 1/1, Batch 87800/98059, Loss: 3.0846\n",
            "Epoch 1/1, Batch 87820/98059, Loss: 2.8530\n",
            "Epoch 1/1, Batch 87840/98059, Loss: 3.0309\n",
            "Epoch 1/1, Batch 87860/98059, Loss: 2.7200\n",
            "Epoch 1/1, Batch 87880/98059, Loss: 3.0943\n",
            "Epoch 1/1, Batch 87900/98059, Loss: 2.7605\n",
            "Epoch 1/1, Batch 87920/98059, Loss: 2.8535\n",
            "Epoch 1/1, Batch 87940/98059, Loss: 2.8216\n",
            "Epoch 1/1, Batch 87960/98059, Loss: 3.1677\n",
            "Epoch 1/1, Batch 87980/98059, Loss: 3.1232\n",
            "Epoch 1/1, Batch 88000/98059, Loss: 3.3721\n",
            "Epoch 1/1, Batch 88020/98059, Loss: 3.0916\n",
            "Epoch 1/1, Batch 88040/98059, Loss: 2.8099\n",
            "Epoch 1/1, Batch 88060/98059, Loss: 2.7740\n",
            "Epoch 1/1, Batch 88080/98059, Loss: 3.0775\n",
            "Epoch 1/1, Batch 88100/98059, Loss: 3.0331\n",
            "Epoch 1/1, Batch 88120/98059, Loss: 3.0286\n",
            "Epoch 1/1, Batch 88140/98059, Loss: 3.1751\n",
            "Epoch 1/1, Batch 88160/98059, Loss: 3.1204\n",
            "Epoch 1/1, Batch 88180/98059, Loss: 3.1146\n",
            "Epoch 1/1, Batch 88200/98059, Loss: 2.8758\n",
            "Epoch 1/1, Batch 88220/98059, Loss: 2.8403\n",
            "Epoch 1/1, Batch 88240/98059, Loss: 2.9384\n",
            "Epoch 1/1, Batch 88260/98059, Loss: 2.8046\n",
            "Epoch 1/1, Batch 88280/98059, Loss: 2.9280\n",
            "Epoch 1/1, Batch 88300/98059, Loss: 3.2966\n",
            "Epoch 1/1, Batch 88320/98059, Loss: 3.2281\n",
            "Epoch 1/1, Batch 88340/98059, Loss: 2.7411\n",
            "Epoch 1/1, Batch 88360/98059, Loss: 2.4304\n",
            "Epoch 1/1, Batch 88380/98059, Loss: 2.4378\n",
            "Epoch 1/1, Batch 88400/98059, Loss: 3.2925\n",
            "Epoch 1/1, Batch 88420/98059, Loss: 2.9540\n",
            "Epoch 1/1, Batch 88440/98059, Loss: 2.7753\n",
            "Epoch 1/1, Batch 88460/98059, Loss: 3.0221\n",
            "Epoch 1/1, Batch 88480/98059, Loss: 3.2874\n",
            "Epoch 1/1, Batch 88500/98059, Loss: 3.1569\n",
            "Epoch 1/1, Batch 88520/98059, Loss: 2.9768\n",
            "Epoch 1/1, Batch 88540/98059, Loss: 2.8138\n",
            "Epoch 1/1, Batch 88560/98059, Loss: 2.8758\n",
            "Epoch 1/1, Batch 88580/98059, Loss: 2.5348\n",
            "Epoch 1/1, Batch 88600/98059, Loss: 2.9148\n",
            "Epoch 1/1, Batch 88620/98059, Loss: 2.8608\n",
            "Epoch 1/1, Batch 88640/98059, Loss: 3.1864\n",
            "Epoch 1/1, Batch 88660/98059, Loss: 2.9169\n",
            "Epoch 1/1, Batch 88680/98059, Loss: 3.0305\n",
            "Epoch 1/1, Batch 88700/98059, Loss: 2.8655\n",
            "Epoch 1/1, Batch 88720/98059, Loss: 3.0066\n",
            "Epoch 1/1, Batch 88740/98059, Loss: 2.8136\n",
            "Epoch 1/1, Batch 88760/98059, Loss: 2.5910\n",
            "Epoch 1/1, Batch 88780/98059, Loss: 2.8197\n",
            "Epoch 1/1, Batch 88800/98059, Loss: 3.0183\n",
            "Epoch 1/1, Batch 88820/98059, Loss: 3.2530\n",
            "Epoch 1/1, Batch 88840/98059, Loss: 2.9355\n",
            "Epoch 1/1, Batch 88860/98059, Loss: 3.0847\n",
            "Epoch 1/1, Batch 88880/98059, Loss: 3.0884\n",
            "Epoch 1/1, Batch 88900/98059, Loss: 3.2162\n",
            "Epoch 1/1, Batch 88920/98059, Loss: 3.0860\n",
            "Epoch 1/1, Batch 88940/98059, Loss: 2.7927\n",
            "Epoch 1/1, Batch 88960/98059, Loss: 3.0366\n",
            "Epoch 1/1, Batch 88980/98059, Loss: 3.0793\n",
            "Epoch 1/1, Batch 89000/98059, Loss: 2.9857\n",
            "Epoch 1/1, Batch 89020/98059, Loss: 3.1897\n",
            "Epoch 1/1, Batch 89040/98059, Loss: 3.0381\n",
            "Epoch 1/1, Batch 89060/98059, Loss: 2.9149\n",
            "Epoch 1/1, Batch 89080/98059, Loss: 3.1557\n",
            "Epoch 1/1, Batch 89100/98059, Loss: 2.8509\n",
            "Epoch 1/1, Batch 89120/98059, Loss: 3.0301\n",
            "Epoch 1/1, Batch 89140/98059, Loss: 3.0426\n",
            "Epoch 1/1, Batch 89160/98059, Loss: 2.7523\n",
            "Epoch 1/1, Batch 89180/98059, Loss: 3.0753\n",
            "Epoch 1/1, Batch 89200/98059, Loss: 3.0784\n",
            "Epoch 1/1, Batch 89220/98059, Loss: 2.9396\n",
            "Epoch 1/1, Batch 89240/98059, Loss: 2.7974\n",
            "Epoch 1/1, Batch 89260/98059, Loss: 3.0716\n",
            "Epoch 1/1, Batch 89280/98059, Loss: 2.9315\n",
            "Epoch 1/1, Batch 89300/98059, Loss: 2.9564\n",
            "Epoch 1/1, Batch 89320/98059, Loss: 3.1336\n",
            "Epoch 1/1, Batch 89340/98059, Loss: 3.0169\n",
            "Epoch 1/1, Batch 89360/98059, Loss: 3.1131\n",
            "Epoch 1/1, Batch 89380/98059, Loss: 3.1235\n",
            "Epoch 1/1, Batch 89400/98059, Loss: 2.8884\n",
            "Epoch 1/1, Batch 89420/98059, Loss: 3.2178\n",
            "Epoch 1/1, Batch 89440/98059, Loss: 2.9212\n",
            "Epoch 1/1, Batch 89460/98059, Loss: 2.5019\n",
            "Epoch 1/1, Batch 89480/98059, Loss: 2.6922\n",
            "Epoch 1/1, Batch 89500/98059, Loss: 2.9606\n",
            "Epoch 1/1, Batch 89520/98059, Loss: 3.1276\n",
            "Epoch 1/1, Batch 89540/98059, Loss: 2.8924\n",
            "Epoch 1/1, Batch 89560/98059, Loss: 2.7258\n",
            "Epoch 1/1, Batch 89580/98059, Loss: 3.2202\n",
            "Epoch 1/1, Batch 89600/98059, Loss: 3.0171\n",
            "Epoch 1/1, Batch 89620/98059, Loss: 2.8726\n",
            "Epoch 1/1, Batch 89640/98059, Loss: 2.8946\n",
            "Epoch 1/1, Batch 89660/98059, Loss: 2.9114\n",
            "Epoch 1/1, Batch 89680/98059, Loss: 3.1452\n",
            "Epoch 1/1, Batch 89700/98059, Loss: 2.6632\n",
            "Epoch 1/1, Batch 89720/98059, Loss: 3.1107\n",
            "Epoch 1/1, Batch 89740/98059, Loss: 3.0232\n",
            "Epoch 1/1, Batch 89760/98059, Loss: 2.8454\n",
            "Epoch 1/1, Batch 89780/98059, Loss: 2.6968\n",
            "Epoch 1/1, Batch 89800/98059, Loss: 3.0547\n",
            "Epoch 1/1, Batch 89820/98059, Loss: 3.3988\n",
            "Epoch 1/1, Batch 89840/98059, Loss: 2.7764\n",
            "Epoch 1/1, Batch 89860/98059, Loss: 3.0818\n",
            "Epoch 1/1, Batch 89880/98059, Loss: 3.0373\n",
            "Epoch 1/1, Batch 89900/98059, Loss: 2.7819\n",
            "Epoch 1/1, Batch 89920/98059, Loss: 3.0309\n",
            "Epoch 1/1, Batch 89940/98059, Loss: 2.8783\n",
            "Epoch 1/1, Batch 89960/98059, Loss: 3.2040\n",
            "Epoch 1/1, Batch 89980/98059, Loss: 3.3557\n",
            "Epoch 1/1, Batch 90000/98059, Loss: 3.1329\n",
            "Epoch 1/1, Batch 90020/98059, Loss: 2.7461\n",
            "Epoch 1/1, Batch 90040/98059, Loss: 2.7241\n",
            "Epoch 1/1, Batch 90060/98059, Loss: 3.1082\n",
            "Epoch 1/1, Batch 90080/98059, Loss: 2.7530\n",
            "Epoch 1/1, Batch 90100/98059, Loss: 3.0493\n",
            "Epoch 1/1, Batch 90120/98059, Loss: 2.9830\n",
            "Epoch 1/1, Batch 90140/98059, Loss: 2.8880\n",
            "Epoch 1/1, Batch 90160/98059, Loss: 2.9421\n",
            "Epoch 1/1, Batch 90180/98059, Loss: 3.0109\n",
            "Epoch 1/1, Batch 90200/98059, Loss: 3.2812\n",
            "Epoch 1/1, Batch 90220/98059, Loss: 2.8629\n",
            "Epoch 1/1, Batch 90240/98059, Loss: 2.8133\n",
            "Epoch 1/1, Batch 90260/98059, Loss: 2.7211\n",
            "Epoch 1/1, Batch 90280/98059, Loss: 2.9651\n",
            "Epoch 1/1, Batch 90300/98059, Loss: 2.8418\n",
            "Epoch 1/1, Batch 90320/98059, Loss: 3.2147\n",
            "Epoch 1/1, Batch 90340/98059, Loss: 3.1074\n",
            "Epoch 1/1, Batch 90360/98059, Loss: 2.8069\n",
            "Epoch 1/1, Batch 90380/98059, Loss: 3.1849\n",
            "Epoch 1/1, Batch 90400/98059, Loss: 3.0375\n",
            "Epoch 1/1, Batch 90420/98059, Loss: 2.4858\n",
            "Epoch 1/1, Batch 90440/98059, Loss: 3.0018\n",
            "Epoch 1/1, Batch 90460/98059, Loss: 2.7674\n",
            "Epoch 1/1, Batch 90480/98059, Loss: 2.9999\n",
            "Epoch 1/1, Batch 90500/98059, Loss: 2.8313\n",
            "Epoch 1/1, Batch 90520/98059, Loss: 2.7343\n",
            "Epoch 1/1, Batch 90540/98059, Loss: 2.7032\n",
            "Epoch 1/1, Batch 90560/98059, Loss: 2.3905\n",
            "Epoch 1/1, Batch 90580/98059, Loss: 2.7385\n",
            "Epoch 1/1, Batch 90600/98059, Loss: 2.7999\n",
            "Epoch 1/1, Batch 90620/98059, Loss: 2.8130\n",
            "Epoch 1/1, Batch 90640/98059, Loss: 2.8401\n",
            "Epoch 1/1, Batch 90660/98059, Loss: 2.7204\n",
            "Epoch 1/1, Batch 90680/98059, Loss: 2.7810\n",
            "Epoch 1/1, Batch 90700/98059, Loss: 2.6557\n",
            "Epoch 1/1, Batch 90720/98059, Loss: 2.8624\n",
            "Epoch 1/1, Batch 90740/98059, Loss: 2.8612\n",
            "Epoch 1/1, Batch 90760/98059, Loss: 3.1518\n",
            "Epoch 1/1, Batch 90780/98059, Loss: 2.9756\n",
            "Epoch 1/1, Batch 90800/98059, Loss: 2.9000\n",
            "Epoch 1/1, Batch 90820/98059, Loss: 3.0831\n",
            "Epoch 1/1, Batch 90840/98059, Loss: 2.7903\n",
            "Epoch 1/1, Batch 90860/98059, Loss: 2.9546\n",
            "Epoch 1/1, Batch 90880/98059, Loss: 3.2448\n",
            "Epoch 1/1, Batch 90900/98059, Loss: 2.7405\n",
            "Epoch 1/1, Batch 90920/98059, Loss: 3.0969\n",
            "Epoch 1/1, Batch 90940/98059, Loss: 2.9392\n",
            "Epoch 1/1, Batch 90960/98059, Loss: 2.9588\n",
            "Epoch 1/1, Batch 90980/98059, Loss: 3.0746\n",
            "Epoch 1/1, Batch 91000/98059, Loss: 2.7070\n",
            "Epoch 1/1, Batch 91020/98059, Loss: 2.8818\n",
            "Epoch 1/1, Batch 91040/98059, Loss: 2.6714\n",
            "Epoch 1/1, Batch 91060/98059, Loss: 3.4105\n",
            "Epoch 1/1, Batch 91080/98059, Loss: 2.6018\n",
            "Epoch 1/1, Batch 91100/98059, Loss: 3.3681\n",
            "Epoch 1/1, Batch 91120/98059, Loss: 3.0970\n",
            "Epoch 1/1, Batch 91140/98059, Loss: 2.9295\n",
            "Epoch 1/1, Batch 91160/98059, Loss: 2.8997\n",
            "Epoch 1/1, Batch 91180/98059, Loss: 2.8365\n",
            "Epoch 1/1, Batch 91200/98059, Loss: 2.8942\n",
            "Epoch 1/1, Batch 91220/98059, Loss: 3.3986\n",
            "Epoch 1/1, Batch 91240/98059, Loss: 3.0765\n",
            "Epoch 1/1, Batch 91260/98059, Loss: 3.2217\n",
            "Epoch 1/1, Batch 91280/98059, Loss: 3.0128\n",
            "Epoch 1/1, Batch 91300/98059, Loss: 2.7340\n",
            "Epoch 1/1, Batch 91320/98059, Loss: 2.6740\n",
            "Epoch 1/1, Batch 91340/98059, Loss: 2.8719\n",
            "Epoch 1/1, Batch 91360/98059, Loss: 2.7296\n",
            "Epoch 1/1, Batch 91380/98059, Loss: 3.0096\n",
            "Epoch 1/1, Batch 91400/98059, Loss: 3.0269\n",
            "Epoch 1/1, Batch 91420/98059, Loss: 2.7233\n",
            "Epoch 1/1, Batch 91440/98059, Loss: 2.8323\n",
            "Epoch 1/1, Batch 91460/98059, Loss: 2.7992\n",
            "Epoch 1/1, Batch 91480/98059, Loss: 2.7197\n",
            "Epoch 1/1, Batch 91500/98059, Loss: 2.8466\n",
            "Epoch 1/1, Batch 91520/98059, Loss: 2.9344\n",
            "Epoch 1/1, Batch 91540/98059, Loss: 3.1007\n",
            "Epoch 1/1, Batch 91560/98059, Loss: 2.6540\n",
            "Epoch 1/1, Batch 91580/98059, Loss: 3.0552\n",
            "Epoch 1/1, Batch 91600/98059, Loss: 3.1338\n",
            "Epoch 1/1, Batch 91620/98059, Loss: 2.9091\n",
            "Epoch 1/1, Batch 91640/98059, Loss: 3.0131\n",
            "Epoch 1/1, Batch 91660/98059, Loss: 2.9478\n",
            "Epoch 1/1, Batch 91680/98059, Loss: 3.0280\n",
            "Epoch 1/1, Batch 91700/98059, Loss: 3.4045\n",
            "Epoch 1/1, Batch 91720/98059, Loss: 2.9176\n",
            "Epoch 1/1, Batch 91740/98059, Loss: 2.8518\n",
            "Epoch 1/1, Batch 91760/98059, Loss: 3.0974\n",
            "Epoch 1/1, Batch 91780/98059, Loss: 2.5550\n",
            "Epoch 1/1, Batch 91800/98059, Loss: 2.6510\n",
            "Epoch 1/1, Batch 91820/98059, Loss: 3.1714\n",
            "Epoch 1/1, Batch 91840/98059, Loss: 3.0119\n",
            "Epoch 1/1, Batch 91860/98059, Loss: 3.0280\n",
            "Epoch 1/1, Batch 91880/98059, Loss: 3.2838\n",
            "Epoch 1/1, Batch 91900/98059, Loss: 3.2958\n",
            "Epoch 1/1, Batch 91920/98059, Loss: 3.0106\n",
            "Epoch 1/1, Batch 91940/98059, Loss: 2.8386\n",
            "Epoch 1/1, Batch 91960/98059, Loss: 2.7924\n",
            "Epoch 1/1, Batch 91980/98059, Loss: 2.8066\n",
            "Epoch 1/1, Batch 92000/98059, Loss: 2.7855\n",
            "Epoch 1/1, Batch 92020/98059, Loss: 2.8208\n",
            "Epoch 1/1, Batch 92040/98059, Loss: 2.7368\n",
            "Epoch 1/1, Batch 92060/98059, Loss: 2.8452\n",
            "Epoch 1/1, Batch 92080/98059, Loss: 2.8432\n",
            "Epoch 1/1, Batch 92100/98059, Loss: 2.9541\n",
            "Epoch 1/1, Batch 92120/98059, Loss: 2.3606\n",
            "Epoch 1/1, Batch 92140/98059, Loss: 3.0918\n",
            "Epoch 1/1, Batch 92160/98059, Loss: 3.0536\n",
            "Epoch 1/1, Batch 92180/98059, Loss: 3.1075\n",
            "Epoch 1/1, Batch 92200/98059, Loss: 3.0163\n",
            "Epoch 1/1, Batch 92220/98059, Loss: 3.1540\n",
            "Epoch 1/1, Batch 92240/98059, Loss: 2.7548\n",
            "Epoch 1/1, Batch 92260/98059, Loss: 2.6728\n",
            "Epoch 1/1, Batch 92280/98059, Loss: 2.8528\n",
            "Epoch 1/1, Batch 92300/98059, Loss: 2.6387\n",
            "Epoch 1/1, Batch 92320/98059, Loss: 2.6757\n",
            "Epoch 1/1, Batch 92340/98059, Loss: 2.7365\n",
            "Epoch 1/1, Batch 92360/98059, Loss: 2.7505\n",
            "Epoch 1/1, Batch 92380/98059, Loss: 3.0682\n",
            "Epoch 1/1, Batch 92400/98059, Loss: 3.0852\n",
            "Epoch 1/1, Batch 92420/98059, Loss: 2.9603\n",
            "Epoch 1/1, Batch 92440/98059, Loss: 2.8810\n",
            "Epoch 1/1, Batch 92460/98059, Loss: 3.3575\n",
            "Epoch 1/1, Batch 92480/98059, Loss: 2.9031\n",
            "Epoch 1/1, Batch 92500/98059, Loss: 2.5837\n",
            "Epoch 1/1, Batch 92520/98059, Loss: 2.7717\n",
            "Epoch 1/1, Batch 92540/98059, Loss: 3.1624\n",
            "Epoch 1/1, Batch 92560/98059, Loss: 2.9396\n",
            "Epoch 1/1, Batch 92580/98059, Loss: 3.2727\n",
            "Epoch 1/1, Batch 92600/98059, Loss: 2.9901\n",
            "Epoch 1/1, Batch 92620/98059, Loss: 2.9761\n",
            "Epoch 1/1, Batch 92640/98059, Loss: 2.9145\n",
            "Epoch 1/1, Batch 92660/98059, Loss: 3.3947\n",
            "Epoch 1/1, Batch 92680/98059, Loss: 3.4633\n",
            "Epoch 1/1, Batch 92700/98059, Loss: 2.9201\n",
            "Epoch 1/1, Batch 92720/98059, Loss: 3.0977\n",
            "Epoch 1/1, Batch 92740/98059, Loss: 2.8624\n",
            "Epoch 1/1, Batch 92760/98059, Loss: 2.5157\n",
            "Epoch 1/1, Batch 92780/98059, Loss: 2.7784\n",
            "Epoch 1/1, Batch 92800/98059, Loss: 2.9710\n",
            "Epoch 1/1, Batch 92820/98059, Loss: 2.5476\n",
            "Epoch 1/1, Batch 92840/98059, Loss: 2.7854\n",
            "Epoch 1/1, Batch 92860/98059, Loss: 3.1879\n",
            "Epoch 1/1, Batch 92880/98059, Loss: 2.7187\n",
            "Epoch 1/1, Batch 92900/98059, Loss: 2.8731\n",
            "Epoch 1/1, Batch 92920/98059, Loss: 2.7795\n",
            "Epoch 1/1, Batch 92940/98059, Loss: 2.6890\n",
            "Epoch 1/1, Batch 92960/98059, Loss: 3.2025\n",
            "Epoch 1/1, Batch 92980/98059, Loss: 3.0485\n",
            "Epoch 1/1, Batch 93000/98059, Loss: 3.1056\n",
            "Epoch 1/1, Batch 93020/98059, Loss: 2.8864\n",
            "Epoch 1/1, Batch 93040/98059, Loss: 3.0542\n",
            "Epoch 1/1, Batch 93060/98059, Loss: 2.8320\n",
            "Epoch 1/1, Batch 93080/98059, Loss: 3.0022\n",
            "Epoch 1/1, Batch 93100/98059, Loss: 2.8759\n",
            "Epoch 1/1, Batch 93120/98059, Loss: 2.7362\n",
            "Epoch 1/1, Batch 93140/98059, Loss: 2.7014\n",
            "Epoch 1/1, Batch 93160/98059, Loss: 2.6054\n",
            "Epoch 1/1, Batch 93180/98059, Loss: 2.7417\n",
            "Epoch 1/1, Batch 93200/98059, Loss: 2.7339\n",
            "Epoch 1/1, Batch 93220/98059, Loss: 2.9619\n",
            "Epoch 1/1, Batch 93240/98059, Loss: 3.0367\n",
            "Epoch 1/1, Batch 93260/98059, Loss: 3.0018\n",
            "Epoch 1/1, Batch 93280/98059, Loss: 3.0693\n",
            "Epoch 1/1, Batch 93300/98059, Loss: 3.3048\n",
            "Epoch 1/1, Batch 93320/98059, Loss: 3.0867\n",
            "Epoch 1/1, Batch 93340/98059, Loss: 2.8885\n",
            "Epoch 1/1, Batch 93360/98059, Loss: 2.9593\n",
            "Epoch 1/1, Batch 93380/98059, Loss: 2.9012\n",
            "Epoch 1/1, Batch 93400/98059, Loss: 3.1253\n",
            "Epoch 1/1, Batch 93420/98059, Loss: 2.8513\n",
            "Epoch 1/1, Batch 93440/98059, Loss: 2.9818\n",
            "Epoch 1/1, Batch 93460/98059, Loss: 2.6277\n",
            "Epoch 1/1, Batch 93480/98059, Loss: 3.3504\n",
            "Epoch 1/1, Batch 93500/98059, Loss: 3.1000\n",
            "Epoch 1/1, Batch 93520/98059, Loss: 3.1886\n",
            "Epoch 1/1, Batch 93540/98059, Loss: 2.7869\n",
            "Epoch 1/1, Batch 93560/98059, Loss: 2.7926\n",
            "Epoch 1/1, Batch 93580/98059, Loss: 2.6629\n",
            "Epoch 1/1, Batch 93600/98059, Loss: 3.1570\n",
            "Epoch 1/1, Batch 93620/98059, Loss: 2.9055\n",
            "Epoch 1/1, Batch 93640/98059, Loss: 2.8690\n",
            "Epoch 1/1, Batch 93660/98059, Loss: 3.0224\n",
            "Epoch 1/1, Batch 93680/98059, Loss: 3.2909\n",
            "Epoch 1/1, Batch 93700/98059, Loss: 2.7353\n",
            "Epoch 1/1, Batch 93720/98059, Loss: 2.7952\n",
            "Epoch 1/1, Batch 93740/98059, Loss: 2.8876\n",
            "Epoch 1/1, Batch 93760/98059, Loss: 2.8522\n",
            "Epoch 1/1, Batch 93780/98059, Loss: 3.2687\n",
            "Epoch 1/1, Batch 93800/98059, Loss: 3.0146\n",
            "Epoch 1/1, Batch 93820/98059, Loss: 3.0957\n",
            "Epoch 1/1, Batch 93840/98059, Loss: 3.3756\n",
            "Epoch 1/1, Batch 93860/98059, Loss: 2.8917\n",
            "Epoch 1/1, Batch 93880/98059, Loss: 2.9096\n",
            "Epoch 1/1, Batch 93900/98059, Loss: 2.7744\n",
            "Epoch 1/1, Batch 93920/98059, Loss: 3.1006\n",
            "Epoch 1/1, Batch 93940/98059, Loss: 2.9055\n",
            "Epoch 1/1, Batch 93960/98059, Loss: 2.8991\n",
            "Epoch 1/1, Batch 93980/98059, Loss: 3.0542\n",
            "Epoch 1/1, Batch 94000/98059, Loss: 3.3736\n",
            "Epoch 1/1, Batch 94020/98059, Loss: 2.9820\n",
            "Epoch 1/1, Batch 94040/98059, Loss: 2.9516\n",
            "Epoch 1/1, Batch 94060/98059, Loss: 2.6113\n",
            "Epoch 1/1, Batch 94080/98059, Loss: 3.2055\n",
            "Epoch 1/1, Batch 94100/98059, Loss: 3.1711\n",
            "Epoch 1/1, Batch 94120/98059, Loss: 2.5743\n",
            "Epoch 1/1, Batch 94140/98059, Loss: 3.3593\n",
            "Epoch 1/1, Batch 94160/98059, Loss: 3.1818\n",
            "Epoch 1/1, Batch 94180/98059, Loss: 3.0258\n",
            "Epoch 1/1, Batch 94200/98059, Loss: 2.4513\n",
            "Epoch 1/1, Batch 94220/98059, Loss: 2.5419\n",
            "Epoch 1/1, Batch 94240/98059, Loss: 2.8274\n",
            "Epoch 1/1, Batch 94260/98059, Loss: 3.0130\n",
            "Epoch 1/1, Batch 94280/98059, Loss: 2.3203\n",
            "Epoch 1/1, Batch 94300/98059, Loss: 2.9648\n",
            "Epoch 1/1, Batch 94320/98059, Loss: 3.0464\n",
            "Epoch 1/1, Batch 94340/98059, Loss: 3.1127\n",
            "Epoch 1/1, Batch 94360/98059, Loss: 3.0409\n",
            "Epoch 1/1, Batch 94380/98059, Loss: 3.0192\n",
            "Epoch 1/1, Batch 94400/98059, Loss: 2.9800\n",
            "Epoch 1/1, Batch 94420/98059, Loss: 3.1323\n",
            "Epoch 1/1, Batch 94440/98059, Loss: 3.1377\n",
            "Epoch 1/1, Batch 94460/98059, Loss: 3.0801\n",
            "Epoch 1/1, Batch 94480/98059, Loss: 2.6464\n",
            "Epoch 1/1, Batch 94500/98059, Loss: 3.0128\n",
            "Epoch 1/1, Batch 94520/98059, Loss: 2.9821\n",
            "Epoch 1/1, Batch 94540/98059, Loss: 2.8840\n",
            "Epoch 1/1, Batch 94560/98059, Loss: 2.7413\n",
            "Epoch 1/1, Batch 94580/98059, Loss: 2.9748\n",
            "Epoch 1/1, Batch 94600/98059, Loss: 3.0200\n",
            "Epoch 1/1, Batch 94620/98059, Loss: 2.6328\n",
            "Epoch 1/1, Batch 94640/98059, Loss: 2.8444\n",
            "Epoch 1/1, Batch 94660/98059, Loss: 3.1892\n",
            "Epoch 1/1, Batch 94680/98059, Loss: 2.6919\n",
            "Epoch 1/1, Batch 94700/98059, Loss: 3.0650\n",
            "Epoch 1/1, Batch 94720/98059, Loss: 2.8999\n",
            "Epoch 1/1, Batch 94740/98059, Loss: 2.9079\n",
            "Epoch 1/1, Batch 94760/98059, Loss: 2.8924\n",
            "Epoch 1/1, Batch 94780/98059, Loss: 3.2379\n",
            "Epoch 1/1, Batch 94800/98059, Loss: 3.0978\n",
            "Epoch 1/1, Batch 94820/98059, Loss: 2.7839\n",
            "Epoch 1/1, Batch 94840/98059, Loss: 3.0521\n",
            "Epoch 1/1, Batch 94860/98059, Loss: 2.8288\n",
            "Epoch 1/1, Batch 94880/98059, Loss: 2.9156\n",
            "Epoch 1/1, Batch 94900/98059, Loss: 2.5046\n",
            "Epoch 1/1, Batch 94920/98059, Loss: 2.8739\n",
            "Epoch 1/1, Batch 94940/98059, Loss: 3.3156\n",
            "Epoch 1/1, Batch 94960/98059, Loss: 3.0452\n",
            "Epoch 1/1, Batch 94980/98059, Loss: 3.0569\n",
            "Epoch 1/1, Batch 95000/98059, Loss: 2.8700\n",
            "Epoch 1/1, Batch 95020/98059, Loss: 3.0666\n",
            "Epoch 1/1, Batch 95040/98059, Loss: 2.9506\n",
            "Epoch 1/1, Batch 95060/98059, Loss: 2.6790\n",
            "Epoch 1/1, Batch 95080/98059, Loss: 2.8098\n",
            "Epoch 1/1, Batch 95100/98059, Loss: 2.9133\n",
            "Epoch 1/1, Batch 95120/98059, Loss: 2.7626\n",
            "Epoch 1/1, Batch 95140/98059, Loss: 3.0986\n",
            "Epoch 1/1, Batch 95160/98059, Loss: 3.1673\n",
            "Epoch 1/1, Batch 95180/98059, Loss: 3.1798\n",
            "Epoch 1/1, Batch 95200/98059, Loss: 2.9400\n",
            "Epoch 1/1, Batch 95220/98059, Loss: 2.9624\n",
            "Epoch 1/1, Batch 95240/98059, Loss: 3.1159\n",
            "Epoch 1/1, Batch 95260/98059, Loss: 3.0812\n",
            "Epoch 1/1, Batch 95280/98059, Loss: 3.1514\n",
            "Epoch 1/1, Batch 95300/98059, Loss: 2.5605\n",
            "Epoch 1/1, Batch 95320/98059, Loss: 2.8165\n",
            "Epoch 1/1, Batch 95340/98059, Loss: 3.3710\n",
            "Epoch 1/1, Batch 95360/98059, Loss: 2.8927\n",
            "Epoch 1/1, Batch 95380/98059, Loss: 3.1362\n",
            "Epoch 1/1, Batch 95400/98059, Loss: 3.0294\n",
            "Epoch 1/1, Batch 95420/98059, Loss: 2.9932\n",
            "Epoch 1/1, Batch 95440/98059, Loss: 2.8251\n",
            "Epoch 1/1, Batch 95460/98059, Loss: 3.1199\n",
            "Epoch 1/1, Batch 95480/98059, Loss: 3.2831\n",
            "Epoch 1/1, Batch 95500/98059, Loss: 2.6003\n",
            "Epoch 1/1, Batch 95520/98059, Loss: 2.8688\n",
            "Epoch 1/1, Batch 95540/98059, Loss: 2.9853\n",
            "Epoch 1/1, Batch 95560/98059, Loss: 2.8890\n",
            "Epoch 1/1, Batch 95580/98059, Loss: 3.1199\n",
            "Epoch 1/1, Batch 95600/98059, Loss: 2.8262\n",
            "Epoch 1/1, Batch 95620/98059, Loss: 3.3159\n",
            "Epoch 1/1, Batch 95640/98059, Loss: 2.8172\n",
            "Epoch 1/1, Batch 95660/98059, Loss: 3.1314\n",
            "Epoch 1/1, Batch 95680/98059, Loss: 2.5584\n",
            "Epoch 1/1, Batch 95700/98059, Loss: 2.8910\n",
            "Epoch 1/1, Batch 95720/98059, Loss: 3.0529\n",
            "Epoch 1/1, Batch 95740/98059, Loss: 2.9832\n",
            "Epoch 1/1, Batch 95760/98059, Loss: 3.1030\n",
            "Epoch 1/1, Batch 95780/98059, Loss: 2.8494\n",
            "Epoch 1/1, Batch 95800/98059, Loss: 2.6113\n",
            "Epoch 1/1, Batch 95820/98059, Loss: 2.9345\n",
            "Epoch 1/1, Batch 95840/98059, Loss: 2.9284\n",
            "Epoch 1/1, Batch 95860/98059, Loss: 2.7825\n",
            "Epoch 1/1, Batch 95880/98059, Loss: 2.9540\n",
            "Epoch 1/1, Batch 95900/98059, Loss: 3.0589\n",
            "Epoch 1/1, Batch 95920/98059, Loss: 3.1045\n",
            "Epoch 1/1, Batch 95940/98059, Loss: 2.9461\n",
            "Epoch 1/1, Batch 95960/98059, Loss: 3.0708\n",
            "Epoch 1/1, Batch 95980/98059, Loss: 2.9709\n",
            "Epoch 1/1, Batch 96000/98059, Loss: 2.6544\n",
            "Epoch 1/1, Batch 96020/98059, Loss: 3.1624\n",
            "Epoch 1/1, Batch 96040/98059, Loss: 3.0109\n",
            "Epoch 1/1, Batch 96060/98059, Loss: 2.9502\n",
            "Epoch 1/1, Batch 96080/98059, Loss: 2.9589\n",
            "Epoch 1/1, Batch 96100/98059, Loss: 2.9315\n",
            "Epoch 1/1, Batch 96120/98059, Loss: 2.9540\n",
            "Epoch 1/1, Batch 96140/98059, Loss: 2.9913\n",
            "Epoch 1/1, Batch 96160/98059, Loss: 2.8796\n",
            "Epoch 1/1, Batch 96180/98059, Loss: 2.7661\n",
            "Epoch 1/1, Batch 96200/98059, Loss: 2.5719\n",
            "Epoch 1/1, Batch 96220/98059, Loss: 3.0968\n",
            "Epoch 1/1, Batch 96240/98059, Loss: 2.9898\n",
            "Epoch 1/1, Batch 96260/98059, Loss: 2.9244\n",
            "Epoch 1/1, Batch 96280/98059, Loss: 3.1151\n",
            "Epoch 1/1, Batch 96300/98059, Loss: 2.8508\n",
            "Epoch 1/1, Batch 96320/98059, Loss: 2.3082\n",
            "Epoch 1/1, Batch 96340/98059, Loss: 3.2340\n",
            "Epoch 1/1, Batch 96360/98059, Loss: 2.7195\n",
            "Epoch 1/1, Batch 96380/98059, Loss: 2.7548\n",
            "Epoch 1/1, Batch 96400/98059, Loss: 3.1212\n",
            "Epoch 1/1, Batch 96420/98059, Loss: 3.0971\n",
            "Epoch 1/1, Batch 96440/98059, Loss: 2.4347\n",
            "Epoch 1/1, Batch 96460/98059, Loss: 3.0329\n",
            "Epoch 1/1, Batch 96480/98059, Loss: 3.1930\n",
            "Epoch 1/1, Batch 96500/98059, Loss: 2.8914\n",
            "Epoch 1/1, Batch 96520/98059, Loss: 2.7985\n",
            "Epoch 1/1, Batch 96540/98059, Loss: 2.7999\n",
            "Epoch 1/1, Batch 96560/98059, Loss: 3.2143\n",
            "Epoch 1/1, Batch 96580/98059, Loss: 2.9724\n",
            "Epoch 1/1, Batch 96600/98059, Loss: 3.1892\n",
            "Epoch 1/1, Batch 96620/98059, Loss: 2.8854\n",
            "Epoch 1/1, Batch 96640/98059, Loss: 2.9213\n",
            "Epoch 1/1, Batch 96660/98059, Loss: 2.9757\n",
            "Epoch 1/1, Batch 96680/98059, Loss: 2.8072\n",
            "Epoch 1/1, Batch 96700/98059, Loss: 2.6945\n",
            "Epoch 1/1, Batch 96720/98059, Loss: 2.9716\n",
            "Epoch 1/1, Batch 96740/98059, Loss: 2.8317\n",
            "Epoch 1/1, Batch 96760/98059, Loss: 2.9843\n",
            "Epoch 1/1, Batch 96780/98059, Loss: 2.9908\n",
            "Epoch 1/1, Batch 96800/98059, Loss: 2.8299\n",
            "Epoch 1/1, Batch 96820/98059, Loss: 2.7585\n",
            "Epoch 1/1, Batch 96840/98059, Loss: 2.8935\n",
            "Epoch 1/1, Batch 96860/98059, Loss: 3.0258\n",
            "Epoch 1/1, Batch 96880/98059, Loss: 2.8270\n",
            "Epoch 1/1, Batch 96900/98059, Loss: 3.0768\n",
            "Epoch 1/1, Batch 96920/98059, Loss: 2.8018\n",
            "Epoch 1/1, Batch 96940/98059, Loss: 3.0754\n",
            "Epoch 1/1, Batch 96960/98059, Loss: 3.2076\n",
            "Epoch 1/1, Batch 96980/98059, Loss: 2.6428\n",
            "Epoch 1/1, Batch 97000/98059, Loss: 2.8410\n",
            "Epoch 1/1, Batch 97020/98059, Loss: 2.8591\n",
            "Epoch 1/1, Batch 97040/98059, Loss: 3.3145\n",
            "Epoch 1/1, Batch 97060/98059, Loss: 3.4500\n",
            "Epoch 1/1, Batch 97080/98059, Loss: 2.9961\n",
            "Epoch 1/1, Batch 97100/98059, Loss: 3.0309\n",
            "Epoch 1/1, Batch 97120/98059, Loss: 3.2028\n",
            "Epoch 1/1, Batch 97140/98059, Loss: 2.7873\n",
            "Epoch 1/1, Batch 97160/98059, Loss: 2.9703\n",
            "Epoch 1/1, Batch 97180/98059, Loss: 3.1307\n",
            "Epoch 1/1, Batch 97200/98059, Loss: 3.0598\n",
            "Epoch 1/1, Batch 97220/98059, Loss: 2.7454\n",
            "Epoch 1/1, Batch 97240/98059, Loss: 2.5235\n",
            "Epoch 1/1, Batch 97260/98059, Loss: 2.4422\n",
            "Epoch 1/1, Batch 97280/98059, Loss: 3.1101\n",
            "Epoch 1/1, Batch 97300/98059, Loss: 3.3451\n",
            "Epoch 1/1, Batch 97320/98059, Loss: 2.9286\n",
            "Epoch 1/1, Batch 97340/98059, Loss: 3.1739\n",
            "Epoch 1/1, Batch 97360/98059, Loss: 2.8236\n",
            "Epoch 1/1, Batch 97380/98059, Loss: 3.0849\n",
            "Epoch 1/1, Batch 97400/98059, Loss: 3.4778\n",
            "Epoch 1/1, Batch 97420/98059, Loss: 2.6846\n",
            "Epoch 1/1, Batch 97440/98059, Loss: 2.6343\n",
            "Epoch 1/1, Batch 97460/98059, Loss: 3.0249\n",
            "Epoch 1/1, Batch 97480/98059, Loss: 3.0616\n",
            "Epoch 1/1, Batch 97500/98059, Loss: 2.9483\n",
            "Epoch 1/1, Batch 97520/98059, Loss: 3.2426\n",
            "Epoch 1/1, Batch 97540/98059, Loss: 3.1825\n",
            "Epoch 1/1, Batch 97560/98059, Loss: 2.9118\n",
            "Epoch 1/1, Batch 97580/98059, Loss: 2.8065\n",
            "Epoch 1/1, Batch 97600/98059, Loss: 3.1785\n",
            "Epoch 1/1, Batch 97620/98059, Loss: 2.7373\n",
            "Epoch 1/1, Batch 97640/98059, Loss: 2.8473\n",
            "Epoch 1/1, Batch 97660/98059, Loss: 2.9901\n",
            "Epoch 1/1, Batch 97680/98059, Loss: 3.0370\n",
            "Epoch 1/1, Batch 97700/98059, Loss: 2.9103\n",
            "Epoch 1/1, Batch 97720/98059, Loss: 3.5225\n",
            "Epoch 1/1, Batch 97740/98059, Loss: 3.1931\n",
            "Epoch 1/1, Batch 97760/98059, Loss: 2.6278\n",
            "Epoch 1/1, Batch 97780/98059, Loss: 2.8701\n",
            "Epoch 1/1, Batch 97800/98059, Loss: 2.6781\n",
            "Epoch 1/1, Batch 97820/98059, Loss: 2.8020\n",
            "Epoch 1/1, Batch 97840/98059, Loss: 3.1887\n",
            "Epoch 1/1, Batch 97860/98059, Loss: 2.9423\n",
            "Epoch 1/1, Batch 97880/98059, Loss: 2.9185\n",
            "Epoch 1/1, Batch 97900/98059, Loss: 3.0503\n",
            "Epoch 1/1, Batch 97920/98059, Loss: 3.0922\n",
            "Epoch 1/1, Batch 97940/98059, Loss: 3.1739\n",
            "Epoch 1/1, Batch 97960/98059, Loss: 2.9863\n",
            "Epoch 1/1, Batch 97980/98059, Loss: 2.9402\n",
            "Epoch 1/1, Batch 98000/98059, Loss: 2.8858\n",
            "Epoch 1/1, Batch 98020/98059, Loss: 2.6233\n",
            "Epoch 1/1, Batch 98040/98059, Loss: 3.0454\n",
            "Epoch 1/1 Summary:\n",
            "  Train Loss: 3.1238\n",
            "  Val Loss: 3.3057\n",
            "------------------------------\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Visualize Training Progress\n",
        "\n",
        "Plot training and validation loss over time.\n"
      ],
      "metadata": {
        "id": "bUHyGTNPV5rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, 'o-', label='Training')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, 's-', label='Validation')\n",
        "plt.title('Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final results\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "y0YGg-JtUi3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "027a3150-d180-4fe6-c97e-c50a278eaa9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcxxJREFUeJzt3XlcVPX+x/H3AAIii0AgoChuuWPmdmlRU0qta2p0K7PA8maLWrYav8qtuqjZvS3eyDZt0SwtzRYlNbEyd6PcMjPTUhBTWVwAZc7vj65TI6iI8B0YXs/Hg0fMme+c8z1v5jifPnPmjM2yLEsAAAAAAACAQR6ungAAAAAAAABqHppSAAAAAAAAMI6mFAAAAAAAAIyjKQUAAAAAAADjaEoBAAAAAADAOJpSAAAAAAAAMI6mFAAAAAAAAIyjKQUAAAAAAADjaEoBAAAAAADAOJpSAKqcIUOGKCYmplyPHTdunGw2W8VOqJL88ssvstlsmjFjhqunAgAAUONQiwGuR1MKQJnZbLYy/aSnp7t6qi4xZMgQ+fv7n/Z+m82mESNGnPd2XnrpJYonAACqqBkzZshms2ndunWunkqZZGRk6JZbblF0dLR8fHwUEhKi+Ph4TZ8+XcXFxa6eHgA35+XqCQCoPt5++22n22+99ZYWL15cYnmrVq3Oazuvvvqq7HZ7uR77+OOP69FHHz2v7ZvSqFEjHTt2TLVq1Tqnx7300ku64IILNGTIkMqZGAAAqBFee+013XXXXapXr55uvfVWNW/eXPn5+Vq6dKmGDh2qzMxM/d///Z+rp1lpyluLAag4NKUAlNktt9zidHvVqlVavHhxieWnOnr0qPz8/Mq8nfMpDLy8vOTlVT3+abPZbPL19XX1NCRJBQUF8vb2locHJ9ACAFATrFq1SnfddZfi4uL02WefKSAgwHHfqFGjtG7dOm3atMmFM6w8J06ckN1ul7e3d5WpxYCaiv/7AFChevToobZt22r9+vXq1q2b/Pz8HO+wffTRR7rmmmsUFRUlHx8fNW3aVE8++WSJU8NPvabUyc/7T5kyRa+88oqaNm0qHx8fde7cWWvXrnV6bGnXlDr5sbn58+erbdu28vHxUZs2bbRo0aIS809PT1enTp3k6+urpk2batq0aZV2narSrmOQlZWl2267TQ0aNJCPj48iIyPVv39//fLLL5KkmJgYbd68WcuXL3d8XLJHjx6Ox//888/6xz/+oZCQEPn5+elvf/ubPv300xL7aLPZNHv2bD3++OOqX7++/Pz8lJGRIZvNpv/85z8l5vrNN9/IZrPp3XffrfAcAACoib799lv17dtXgYGB8vf3V69evbRq1SqnMcePH9f48ePVvHlz+fr6KjQ0VJdddpkWL17sGHO22uF0xo8fL5vNppkzZzo1pE7q1KmT01nZR44c0YMPPuj4mF+LFi00ZcoUWZbl9LiTddecOXPUunVr1a5dW3Fxcdq4caMkadq0aWrWrJl8fX3Vo0ePEvP8ay15ySWXqHbt2mrcuLFefvllp3FFRUUaM2aMOnbsqKCgINWpU0eXX365li1b5jTur3Xkc88956gjt2zZUq5a7KSXXnpJbdq0kY+Pj6KiojR8+HDl5OSUui9btmzRFVdcIT8/P9WvX1+TJ08+w18GqFmqx+kEAKqVAwcOqG/fvrrpppt0yy23qF69epL+uMaCv7+/HnjgAfn7++uLL77QmDFjlJeXp2eeeeas6501a5by8/N15513ymazafLkybruuuv0888/n/Xsqq+//loffvih7rnnHgUEBOiFF15QQkKCdu/erdDQUEl/FId9+vRRZGSkxo8fr+LiYk2YMEFhYWHntP+///77OY3/q4SEBG3evFkjR45UTEyMsrOztXjxYu3evVsxMTF67rnnNHLkSPn7++uxxx6TJEe++/bt0yWXXKKjR4/q3nvvVWhoqN58801de+21mjt3rgYOHOi0rSeffFLe3t566KGHVFhYqJYtW+rSSy/VzJkzdf/99zuNPVmw9u/fv9z7BgAA/rB582ZdfvnlCgwM1COPPKJatWpp2rRp6tGjh5YvX66uXbtK+uPNtpSUFP3zn/9Uly5dlJeXp3Xr1mnDhg268sorJZ29dijN0aNHtXTpUnXr1k0NGzY863wty9K1116rZcuWaejQobrooouUlpamhx9+WHv27CnxhtZXX32lBQsWaPjw4ZKklJQU/f3vf9cjjzyil156Sffcc48OHTqkyZMn6/bbb9cXX3zh9PhDhw7p6quv1g033KBBgwbp/fff19133y1vb2/dfvvtkqS8vDy99tprGjRokO644w7l5+fr9ddfV+/evbVmzRpddNFFTuucPn26CgoKNGzYMMe1s0q7XERZ8hw3bpzGjx+v+Ph43X333dq2bZtSU1O1du1arVixwqkuPXTokPr06aPrrrtON9xwg+bOnavRo0erXbt26tu371mzB9yeBQDlNHz4cOvUf0a6d+9uSbJefvnlEuOPHj1aYtmdd95p+fn5WQUFBY5lSUlJVqNGjRy3d+7caUmyQkNDrYMHDzqWf/TRR5Yk6+OPP3YsGzt2bIk5SbK8vb2tn376ybHsu+++syRZL774omNZv379LD8/P2vPnj2OZdu3b7e8vLxKrLM0SUlJlqQz/gwfPrzEfk2fPt2yLMs6dOiQJcl65plnzridNm3aWN27dy+xfNSoUZYk66uvvnIsy8/Ptxo3bmzFxMRYxcXFlmVZ1rJlyyxJVpMmTUr8TaZNm2ZJsrZu3epYVlRUZF1wwQVWUlLSWTMAAKCmmz59uiXJWrt27WnHDBgwwPL29rZ27NjhWLZ3714rICDA6tatm2NZ+/btrWuuuea06ylr7XCqk3XQfffdV6bx8+fPtyRZTz31lNPy66+/3rLZbE41liTLx8fH2rlzp2PZyfoiIiLCysvLcyxPTk62JDmNPVlLPvvss45lhYWF1kUXXWSFh4dbRUVFlmVZ1okTJ6zCwkKn+Rw6dMiqV6+edfvttzuWnay3AgMDrezsbKfx5anFsrOzLW9vb+uqq65y1FaWZVlTp061JFlvvPFGiX156623nPYlIiLCSkhIOO02gJqEj+8BqHA+Pj667bbbSiyvXbu24/f8/Hz9/vvvuvzyy3X06FH98MMPZ13vjTfeqODgYMftyy+/XNIfH1k7m/j4eDVt2tRxOzY2VoGBgY7HFhcXa8mSJRowYICioqIc45o1a3ZO72L5+vpq8eLFpf6cTe3ateXt7a309HQdOnSozNs86bPPPlOXLl102WWXOZb5+/tr2LBh+uWXX7Rlyxan8UlJSU5/E0m64YYb5Ovrq5kzZzqWpaWl6ffffz/rtcMAAMDZFRcX6/PPP9eAAQPUpEkTx/LIyEjdfPPN+vrrr5WXlydJqlu3rjZv3qzt27eXuq7y1g4n11/ax/ZK89lnn8nT01P33nuv0/IHH3xQlmVp4cKFTst79erldJbWyTO/EhISnLZ5cvmptZyXl5fuvPNOx21vb2/deeedys7O1vr16yVJnp6e8vb2liTZ7XYdPHhQJ06cUKdOnbRhw4YS+5CQkHDWs9/LkueSJUtUVFSkUaNGOV2L84477lBgYGCJyyb4+/s71VDe3t7q0qVLmepXoCagKQWgwtWvX99RJPzV5s2bNXDgQAUFBSkwMFBhYWGOF+nc3NyzrvfU08tPNqjKUoSVdmp6cHCw47HZ2dk6duyYmjVrVmJcactOx9PTU/Hx8aX+nI2Pj48mTZqkhQsXql69eurWrZsmT56srKysMm17165datGiRYnlJ78NcdeuXU7LGzduXGJs3bp11a9fP82aNcuxbObMmapfv7569uxZpnkAAIDT279/v44ePXra12y73a5ff/1VkjRhwgTl5OTowgsvVLt27fTwww/r+++/d4wvb+0QGBgo6Y83Ccti165dioqKKtHEOl2NcWrdFRQUJEmKjo4udfmptVxUVJTq1KnjtOzCCy+UJKdrO7355puKjY11XG8rLCxMn376aal1ZWl1z6nKkufJfT317+ft7a0mTZqUyKJBgwYlrk361xoUqOloSgGocKeefSNJOTk56t69u7777jtNmDBBH3/8sRYvXqxJkyZJUqmf6T+Vp6dnqcutUy6wWdGPNWnUqFH68ccflZKSIl9fXz3xxBNq1aqVvv322wrfVml/J0lKTEzUzz//rG+++Ub5+flasGCBBg0axDfzAQBgWLdu3bRjxw698cYbatu2rV577TVdfPHFeu211xxjylM7NGvWTF5eXo6Lj1e009VdFVmPvfPOOxoyZIiaNm2q119/XYsWLdLixYvVs2fPUuvK09U9p6roWqy61KCAq/B/GACMSE9P14EDBzRjxgzdd999+vvf/674+Hinj+O5Unh4uHx9ffXTTz+VuK+0ZZWpadOmevDBB/X5559r06ZNKioq0rPPPuu4/3TfBNioUSNt27atxPKTH41s1KhRmbbfp08fhYWFaebMmZo3b56OHj2qW2+9tRx7AgAAThUWFiY/P7/TvmZ7eHg4nVEUEhKi2267Te+++65+/fVXxcbGaty4cU6PO1vtcCo/Pz/17NlTX375peOsrDNp1KiR9u7dW+LMqnOtMcpq7969OnLkiNOyH3/8UZIcHwucO3eumjRpog8//FC33nqrevfurfj4eBUUFJz39s+U58l9PfXvV1RUpJ07d1Z4FoC7oykFwIiT7xL99V2hoqIivfTSS66akpOTH7ubP3++9u7d61j+008/lbhOQmU5evRoiUKqadOmCggIUGFhoWNZnTp1SnzlsCRdffXVWrNmjVauXOlYduTIEb3yyiuKiYlR69atyzQPLy8vxzfdzJgxQ+3atVNsbGz5dgoAADjx9PTUVVddpY8++sjpo2j79u3TrFmzdNlllzk+XnfgwAGnx/r7+6tZs2aOuqCstUNpxo4dK8uydOutt+rw4cMl7l+/fr3efPNNSX/UGMXFxZo6darTmP/85z+y2WwV/i1yJ06c0LRp0xy3i4qKNG3aNIWFhaljx46SSq8tV69e7VQHnauy5BkfHy9vb2+98MILTtt+/fXXlZubq2uuuabc2wdqIi9XTwBAzXDJJZcoODhYSUlJuvfee2Wz2fT2229XqVOXx40bp88//1yXXnqp7r77bkfx1bZtW2VkZFT69n/88Uf16tVLN9xwg1q3bi0vLy/NmzdP+/bt00033eQY17FjR6Wmpuqpp55Ss2bNFB4erp49e+rRRx/Vu+++q759++ree+9VSEiI3nzzTe3cuVMffPDBOX38LjExUS+88IKWLVvm+IglAAAouzfeeEOLFi0qsfy+++7TU089pcWLF+uyyy7TPffcIy8vL02bNk2FhYWaPHmyY2zr1q3Vo0cPdezYUSEhIVq3bp3mzp2rESNGSCp77VCaSy65RP/97391zz33qGXLlrr11lvVvHlz5efnKz09XQsWLNBTTz0lSerXr5+uuOIKPfbYY/rll1/Uvn17ff755/roo480atQopy+TqQhRUVGaNGmSfvnlF1144YV67733lJGRoVdeeUW1atWSJP3973/Xhx9+qIEDB+qaa67Rzp079fLLL6t169alNtnKoix5hoWFKTk5WePHj1efPn107bXXatu2bXrppZfUuXNnvhgGOEc0pQAYERoaqk8++UQPPvigHn/8cQUHB+uWW25Rr1691Lt3b1dPT9IfzZ6FCxfqoYce0hNPPKHo6GhNmDBBW7duLdO3A56v6OhoDRo0SEuXLtXbb78tLy8vtWzZUu+//74SEhIc48aMGaNdu3Zp8uTJys/PV/fu3dWzZ0/Vq1dP33zzjUaPHq0XX3xRBQUFio2N1ccff3zO79p17NhRbdq00datWzV48OCK3lUAANxeampqqcuHDBmiNm3a6KuvvlJycrJSUlJkt9vVtWtXvfPOO45vpJOke++9VwsWLNDnn3+uwsJCNWrUSE899ZQefvhhSWWvHU7nzjvvVOfOnfXss8/qrbfe0v79++Xv76+LL75Y06dPdzRYPDw8tGDBAo0ZM0bvvfeepk+frpiYGD3zzDN68MEHKyAtZ8HBwXrzzTc1cuRIvfrqq6pXr56mTp2qO+64wzFmyJAhysrK0rRp05SWlqbWrVvrnXfe0Zw5c5Senl6u7ZY1z3HjxiksLExTp07V/fffr5CQEA0bNkz/+te/HE0zAGVjs6rSaQoAUAUNGDDgjF/H7K46dOigkJAQLV261NVTAQAANUSPHj30+++/a9OmTa6eCgADuKYUAPzFsWPHnG5v375dn332mXr06OGaCbnIunXrlJGRocTERFdPBQAAAICb4uN7APAXTZo00ZAhQ9SkSRPt2rVLqamp8vb21iOPPOLqqRmxadMmrV+/Xs8++6wiIyN14403unpKAAAAANwUTSkA+Is+ffro3XffVVZWlnx8fBQXF6d//etfat68uaunZsTcuXM1YcIEtWjRQu+++658fX1dPSUAAAAAboprSgEAAAAAAMA4rikFAAAAAAAA42hKAQAAAAAAwDiuKVVOdrtde/fuVUBAgGw2m6unAwAAKollWcrPz1dUVJQ8PHg/73xQPwEAUDOUtX6iKVVOe/fuVXR0tKunAQAADPn111/VoEEDV0+jWqN+AgCgZjlb/URTqpwCAgIk/RFwYGCgi2dTNdjtdu3fv19hYWG8k2wImZtF3uaRuVnkXbq8vDxFR0c7XvtRftRPpePYM4u8zSNzs8jbPDIvqaz1E02pcjp5ynlgYCBF1f/Y7XYVFBQoMDCQA9EQMjeLvM0jc7PI+8z4uNn5o34qHceeWeRtHpmbRd7mkfnpna1+Ii0AAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHFcUwoAUG0UFxfr+PHjrp6G27Lb7Tp+/LgKCgpq1PUQatWqJU9PT1dPAwAAt+eutVxNrKEqqn6iKQUAqPIsy1JmZqZycnJcPRW3ZlmW7Ha78vPza9xFvevWrauIiIgat98AAJhgWZaysrLctparqTVURdRPNKUAAFXevn37lJubq/DwcPn5+dWoF3uTLMvSiRMn5OXlVWMytixLR48eVXZ2tiQpMjLSxTMCAMD9nGxIuWstV9NqqIqsn2hKAQCqNLvdrpycHNWrV0+hoaGuno5bq2kF1Um1a9eWJGVnZys8PJyP8gEAUIGKi4sdDSl3reVqYg1VUfVTzfiwIwCg2iouLpYk+fn5uXgmcGcnn1/ueJ0LAABc6eRrK7Wc+6mI+ommFACgWqgp7zrBNXh+AQBQuXitdT8V8TelKQUAAAAAAADjaEoBAFCNxMTE6Lnnnivz+PT0dNlsNrf9thsAAIDqhFrOGRc6BwCgEpztdOaxY8dq3Lhx57zetWvXqk6dOmUef8kllygzM1NBQUHnvK1zkZ6eriuuuEKHDh1S3bp1K3VbAIBqIOdX6eiBP363LHkdPCgVZ0onXx/9QqW60a6bH6qlYrulNTsPKju/QOEBvurSOESeHpXzscCaVsu5Ck0pAAAqQWZmpuP39957T2PGjNG2bdscy/z9/R2/W5al4uJieXmd/WU5LCzsnObh7e2tiIiIc3oMAADnJedXaWpH6UShpD8+nnPBqWO8fKQR62lMocwWbcrU+I+3KDO3wLEsMshXY/u1Vp+2kRW+vXOt5U5++97ZUMs54+N7AABUgoiICMdPUFCQbDab4/YPP/yggIAALVy4UB07dpSPj4++/vpr7dixQ/3791e9evXk7++vzp07a8mSJU7rPfWUb5vNptdee00DBw6Un5+fmjdvrgULFjjuP/WU7xkzZqhu3bpKS0tTq1at5O/vrz59+jgVXidOnNC9996runXrKjQ0VKNHj1ZSUpIGDBhQ7jwOHTqkxMREBQcHy8/PT3379tX27dsd9+/atUv9+vVTcHCw6tSpozZt2uizzz5zPHbw4MEKCwtT7dq11bx5c02fPr3ccwEAVLKjBxwNqdM6UfjnmVTAWSzalKm739ng1JCSpKzcAt39zgYt2pR5mkeW37nUcr6+vlqxYoVb13KVhaYUAAAu8uijj2rixInaunWrYmNjdfjwYV199dVaunSpvv32W/Xp00f9+vXT7t27z7ie8ePH64YbbtD333+vq6++WoMHD9bBgwdPO/7o0aOaMmWK3n77bX355ZfavXu3HnroIcf9kyZN0syZMzV9+nStWLFCeXl5mj9//nnt65AhQ7Ru3TotWLBAK1eulGVZuvrqqx1fITx8+HAVFhbqyy+/1MaNGzVp0iTHO5BPPPGEtmzZooULF2rr1q1KTU3VBReUeM8dAABUE5Zl6WjRiTL95Bcc19gFm2WVtp7//Xfcgi3KLzhepvVZVmlrKp+TtdyWLVvUrl07t67lKgsf3wMAVEudOklZWWa3GREhrVtXceubMGGCrrzySsftkJAQtW/f3nH7ySef1Lx587RgwQKNGDHitOsZMmSIBg0aJEn617/+pRdeeEFr1qxRnz59Sh1//Phxvfzyy2ratKkkacSIEZowYYLj/qlTpyo5OVkDBw503D551lJ5bN++XQsWLNCKFSt0ySWXSJJmzpyp6OhozZ8/X//4xz+0e/duJSQkqF27dpKkJk2aOB6/e/dudejQQZ06dZL0xzuMAACg+jp2vFitx6RVyLosSVl5BWo37vMyjd8yobf8vCumFXKyljv58b3w8HBddNFFjvtdVcu9+OKLFVrLVSaaUgCAaikrS9qzx9WzOD8nmywnHT58WOPGjdOnn36qzMxMnThxQseOHTvru2uxsbGO3+vUqaPAwEBlZ2efdryfn5+jiJGkyMhIx/jc3Fzt27dPXbp0cdzv6empjh07ym63n9P+nbR161Z5eXmpa9eujmWhoaFq0aKFtm7dKkm69957dffdd+vzzz9XfHy8EhISHPt19913KyEhQRs2bNBVV12lAQMGOJpbAAAArlJaLTd+/Hi3q+UqE00pAEC15IrrPVb0Nk/95pWHHnpIixcv1pQpU9SsWTPVrl1b119/vYqKis64nlq1ajndttlsZyw6Shtfkaeyl8c///lP9e7dW59++qk+//xzpaSk6Nlnn9XIkSPVt29f7dq1S5999pkWL16sXr16afjw4ZoyZYpL5wwAAMqndi1PbZnQu0xj1+w8qCHT15513IzbOqtL45AybbuilFbLLVmypEbWcuVFUwoAUC1V5MfoqooVK1ZoyJAhjlOtDx8+rF9++cXoHIKCglSvXj2tXbtW3bp1kyQVFxdrw4YNTqejn4tWrVrpxIkTWr16teMMpwMHDmjbtm1q3bq1Y1x0dLTuuusu3XXXXUpOTtarr76qkSNHSvrjm2qSkpKUlJSkyy+/XA8//DBNKQAAqimbzVbmj9Bd3jxMkUG+ysotKPW6UjZJEUG+urx5mDw9bBU6z3P1zTffuGUtV5loSgEAUEU0b95cH374ofr16yebzaYnnnjCJadZjxgxQikpKWrWrJlatmypF198UYcOHZLNdvZCb+PGjQoICHDcttlsat++vfr376877rhD06ZNU0BAgB599FHVr19f/fv3lySNGjVKffv21YUXXqhDhw5p2bJlatWqlSRpzJgx6tixo9q0aaPCwkJ98sknjvsAAIB78/SwaWy/1rr7nQ2ySU6NqZOVydh+rV3ekJKqTi03cuTIctdyptGUAgCgivj3v/+t22+/XZdccokuuOACjR49Wnl5ecbnMXr0aO3bt0+JiYny9PTUsGHD1Lt3b3l6nv1095PvyJ3k6empEydOaPr06brvvvv097//XUVFRerWrZs+++wzx+nnxcXFGj58uH777TcFBgaqT58++s9//iNJ8vb2VnJysn755RfVrl1bl19+uWbPnl3xOw4AqBh+oZKXj3Si8PRjvHz+GAeUQZ+2kUq95WKN/3iLMnMLHMsjgnw1tl9r9Wkb6cLZ/enZZ5/V0KFDq0Qtl5WVVa5azjSbVV0/eOhieXl5CgoKUm5urgIDA109nSrBbrcrOztb4eHh8vDwcPV0agQyN4u8zbPb7dqzZ4/y8/PVpEkT+fr6unpKbu3kN8d4eXk5vZNmt9vVqlUr3XDDDXryySddOMPKU1BQoJ07d6px48Ylnme85lccsiwdry9mkbchOb9KRw9IkuyWpYMHDyokJEQeJ19f/EKlutEunKD7qmrP8TO9xp6rYrulNTsPKju/QOEBvurSOKRKnCF1uhqqqqisWq4i6ifOlAIAAE527dqlxYsXq3v37iosLNTUqVO1c+dO3Xzzza6eGgCguqgb/WfTyW7XCc9sKTxcqgJNElRfnh42xTXlDLuz2bVrlz7//PNqUcu59F+E1NRUxcbGKjAwUIGBgYqLi9PChQtPO/7DDz9Up06dVLduXdWpU0cXXXSR3n77bacxlmVpzJgxioyMVO3atRUfH6/t27c7jTl48KAGDx6swMBA1a1bV0OHDtXhw4crZR8BAKhuPDw8NGPGDHXu3FmXXnqpNm7cqCVLlnAdpyqC+gkAAJxJdarlXHqmVIMGDTRx4kQ1b95clmXpzTffVP/+/fXtt9+qTZs2JcaHhIToscceU8uWLeXt7a1PPvlEt912m8LDw9W79x9fJzl58mS98MILevPNN9W4cWM98cQT6t27t7Zs2eI4nWzw4MHKzMzU4sWLdfz4cd12220aNmyYZs2aZXT/AQCoiqKjo7VixQpXTwOnQf0EAADOpFrVclYVExwcbL322mtlHt+hQwfr8ccftyzLsux2uxUREWE988wzjvtzcnIsHx8f691337Usy7K2bNliSbLWrl3rGLNw4ULLZrNZe/bsKfN2c3NzLUlWbm5umR/j7oqLi63MzEyruLjY1VOpMcjcLPI2r7i42Nq9e7e1efNm69ixY66ejtuz2+1WUVGRZbfbXT0V444dO2Zt2bKl1OdZdXjNp36q3nh9MYu8zSNzs6pa3md6jXUXNbWGqoj6qcpcU6q4uFhz5szRkSNHFBcXd9bxlmXpiy++0LZt2zRp0iRJ0s6dO5WVlaX4+HjHuKCgIHXt2lUrV67UTTfdpJUrV6pu3brq1KmTY0x8fLw8PDy0evVqDRw4sNTtFRYWqrDwz2+POHkFfbvd7pKveKyK7Ha7LMsiD4PI3CzyNu9k5tIf/+5bfDdHpftr3jXJyedXaa/rVfmYp35yD7y+mEXe5pG5WVUt75PzcfdaribWUBVRP7m8KbVx40bFxcWpoKBA/v7+mjdvnlq3bn3a8bm5uapfv74KCwvl6empl156SVdeeaUkKSsrS5JUr149p8fUq1fPcV9WVpbCw8Od7vfy8lJISIhjTGlSUlI0fvz4Esv379+vgoKCUh5R89jtduXm5sqyrCrxLQ81AZmbRd7m2e125efnO77R5MSJE66ekluzLEvFxcWSVCW/OaYynThxQna7XQcOHFCtWrWc7svPz3fRrE6P+sm98PpiFnmbR+ZmVbW8jx8/Lrvd7ta1XE2toSqifnJ5U6pFixbKyMhQbm6u5s6dq6SkJC1fvvy0hVVAQIAyMjJ0+PBhLV26VA888ICaNGmiHj16VOo8k5OT9cADDzhu5+XlKTo6WmFhYXyl8f/Y7XbZbDaFhYVViX/8agIyN4u8zTtZwBw+fFheXl7y8nL5y1aNcGpRURN4eXnJw8NDoaGhJb7S+Hy/vroyUD+5F15fzCJv88jcrKqWd0FBgfLz82tELVfTaqiKqJ9c/ozw9vZWs2bNJEkdO3bU2rVr9fzzz2vatGmljvfw8HCMv+iii7R161alpKSoR48eioiIkCTt27dPkZGRjsfs27dPF110kSQpIiJC2dnZTus8ceKEDh486Hh8aXx8fOTj41PqfKrCgV5V2Gw2MjGMzM0ib/NOvttks9lq1DtPrmBZllPeNcnJ51dpx3dVPN6pn9wPry9mkbd5ZG5WVcrbw8PD8TrrrvVFTa2hKqJ+cv0z9BR2u93p2gPnMr5x48aKiIjQ0qVLHffn5eVp9erVjussxMXFKScnR+vXr3eM+eKLL2S329W1a9cK2gsAAABzqJ8AAEB15NKmVHJysr788kv98ssv2rhxo5KTk5Wenq7BgwdLkhITE5WcnOwYn5KSosWLF+vnn3/W1q1b9eyzz+rtt9/WLbfcIumPLt2oUaP01FNPacGCBdq4caMSExMVFRWlAQMGSJJatWqlPn366I477tCaNWu0YsUKjRgxQjfddJOioqKMZwAAwJn06NFDo0aNctyOiYnRc889d8bH2Gw2zZ8//7y3XVHrQcWifgIAoHqgjjs7l358Lzs7W4mJicrMzFRQUJBiY2OVlpbmuPDm7t27nU75OnLkiO655x799ttvql27tlq2bKl33nlHN954o2PMI488oiNHjmjYsGHKycnRZZddpkWLFjl9nnHmzJkaMWKEevXqJQ8PDyUkJOiFF14wt+MAALfXr18/HT9+XIsWLSpx31dffaVu3brpu+++U2xs7Dmtd+3atapTp05FTVOSNG7cOM2fP1/ffvut0/LMzEwFBwdX6LZONWPGDI0aNUo5OTmVuh13Qv0EAKhxcn6Vjh44/f1+oVLd6ArdZGXUcpVZx2VkZDgtN1HHVQSXNqVef/31M96fnp7udPupp57SU089dcbH2Gw2TZgwQRMmTDjtmJCQEM2aNavM8wQA4FwNHTpUCQkJ+u2339SgQQOn+6ZPn65OnTqdc0NKksLCwipqimd1pmsFwXWonwAANUrOr9LUjtKJM3xM3ctHGrG+QhtTlVHLUceVVOWuKQUAgDv4+9//rrCwMM2YMcNp+eHDhzVnzhwNHTpUBw4c0KBBg1S/fn35+fmpXbt2evfdd8+43lNP+96+fbu6desmX19ftW7dWosXLy7xmNGjR+vCCy+Un5+fmjRpoieeeELHjx+X9MeZSuPHj9d3330nDw8PeXt7O+Z86mnfGzduVM+ePVW7dm2FhoZq2LBhOnz4sOP+IUOGaMCAAZoyZYoiIyMVGhqq4cOHO7ZVHrt371b//v3l7++vwMBA3XDDDdq3b5/j/u+++05XXHGFAgICFBgYqI4dO2rdunWSpF27dqlfv34KDg5WnTp11KZNG3322WflngsAAHCBowfO3JCS/rj/TGdSlcPZarkBAwY46rg6deqoQ4cOLq3jTl50vCrVcWXh8m/fAwDAHXl5eSkxMVEzZszQY4895vgmljlz5qi4uFiDBg3S4cOH1bFjR40ePVqBgYH69NNPdeutt6pp06bq0qXLWbdht9t13XXXqV69elq9erVyc3OdrltwUkBAgGbMmKGoqCht3LhRd9xxhwICAvTII4/oxhtv1KZNm7Ro0SItXrxYJ06cUGhoaIl1HDlyRL1791ZcXJzWrl2r7Oxs/fOf/9SIESOcirVly5YpMjJSy5Yt008//aQbb7xRF110ke64445zztButzsaUsuXL9eJEyc0fPhw3XjjjY6zgQYPHqwOHTooNTVVnp6eysjIcHwd8/Dhw1VUVKQvv/xSderU0ZYtW+Tv73/O8wAAABXMsqTjR8s29sSxso8rOnL2cbX8pDJ8Q97ZarlbbrlFc+bM0ejRoxUQEKCPP/5YiYmJatasmUvquCVLlkiSgoKCSqzDFXVcWdGUAgBUT506SVlZZrcZESH97yycsrj99tv1zDPPaPny5erRo4ekP073TkhIUFBQkIKCgvTQQw85xo8cOVJpaWl6//33y1TMLFmyRD/88IPS0tIcF5v+17/+pb59+zqNe/zxxx2/x8TE6KGHHtLs2bP1yCOPqHbt2vL395eXl5ciIiJ04sQJeXmVLA9mzZqlgoICvfXWW45rIUydOlX9+vXTpEmTVK9ePUlScHCwpk6dKk9PT7Vs2VLXXHONli5dWq5iZunSpdq4caN27typ6Og/Tsd/66231KZNG61du1adO3fW7t279fDDD6tly5aSpObNmzsev3v3biUkJKhdu3aSpCZNmpzzHAAAQCU4flT6VwV/UcYbfco27v/2St5lu67TmWq5Ro0aOeo4y7I0fPhwLVmyxKV13Om4oo4rK5pSAIDqKStL2rPH1bM4o5YtW+qSSy7RG2+8oR49euinn37SV1995bhuT3Fxsf71r3/p/fff1549e1RUVKTCwkL5+fmVaf1bt25VdHS007efxcXFlRj33nvv6YUXXtCOHTt0+PBhnThxQoGBgee0L1u3blX79u2dLs556aWXym63a9u2bY5ipk2bNvL09HSMiYyM1MaNG89pW3/dZnR0tKMhJUmtW7dW3bp1tXXrVnXu3FkPPPCA/vnPf+rtt99WfHy8/vGPf6hp06aSpHvvvVd33323Pv/8c8XHxyshIaFc1/ECAAA105lqOeq4ikFTCgBQPbni4o3l2ObQoUM1cuRI/fe//9X06dPVtGlTde/eXZL0zDPP6Pnnn9dzzz2ndu3aqU6dOho1apSKiooqbMorV67U4MGDNX78ePXu3VtBQUGaPXu2nn322Qrbxl+d/OjcSTabTXa7vVK2Jf3xjTM333yzPv30Uy1cuFBjx47V7NmzNXDgQP3zn/9U79699emnn+rzzz9XSkqKnn32WY0cObLS5gMAAMqglt8fZyyVRdb3ZTsL6vZFUkQZ3nyqVbam0Umnq+UmTZrkqOPatm0rHx8fPfzww9Rx54imFACgejqHj9G50g033KD77rtPs2bN0ltvvaW7777bcU2CFStWqH///rrlllsk/XFtgR9//FGtW7cu07pbtWqlX3/9VZmZmYqMjJQkrVq1ymnMN998o0aNGumxxx5zLNu1a5fTGG9vbxUXF591WzNmzNCRI0cc77KtWLFCHh4eatGiRZnme65O7t+vv/7qOFtqy5YtysnJccrowgsv1IUXXqj7779fgwYN0vTp0zVw4EBJUnR0tO666y7dddddSk5O1quvvkpTCgAAV7PZyvwROnnVLvu4sq7zHJyulvtrHWdZloqKiqjjyoFv3wMAoBL5+/vrxhtvVHJysjIzMzVkyBDHfc2bN9fixYv1zTffaOvWrbrzzjudvlnubOLj43XhhRcqKSlJ3333nb766iunouXkNnbv3q3Zs2drx44deuGFFzRv3jynMTExMdq5c6cyMjL0+++/q7Cw5DfcDB48WL6+vkpKStKmTZu0bNkyjRw5UrfeeqvjlO/yKi4uVkZGhtPP1q1bFR8fr3bt2mnw4MHasGGD1qxZo8TERHXv3l2dOnXSsWPHNGLECKWnp2vXrl1asWKF1q5dq1atWkmSRo0apbS0NO3cuVMbNmzQsmXLHPcBAACUxelquVPruHvuuadG1nHni6YUAACVbOjQoTp06JB69+7tdN2Axx9/XBdffLF69+6tHj16KCIiQgMGDCjzej08PDRv3jwdO3ZMXbp00T//+U89/fTTTmOuvfZa3X///RoxYoQuuugiffPNN3riiSecxiQkJKhPnz7q2bOnoqKiSv06Yz8/P6WlpengwYPq3Lmzrr/+evXq1UtTp049tzBKcfjwYXXo0MHpp1+/frLZbProo48UHBysbt26KT4+Xk2aNNF7770nSfL09NSBAweUmJioCy+8UDfccIP69u2r8ePHS/qj2TV8+HC1atVKffr00YUXXqiXXnrpvOcLAAAM8guVvHzOPMbL549xlaS0Wu6vddwVV1yhevXqubSOu+KKKxQWFma8jjtfNsuyLFdPojrKy8tTUFCQcnNzz/kiY+7KbrcrOztb4eHh8vCg32kCmZtF3ubZ7Xbt2bNH+fn5atKkiXx9fV09JbdmWZbj2/dsZfiqZHdSUFCgnTt3qnHjxiWeZ7zmVxyyLB2vL2aRt3lkblZVy/tMr7FllvOrdPTA6e/3C5XqRp/+/kpWU2uoiqifuKYUAAAAAACouupGu7TphMrj+rYpAAAAAAAAahyaUgAAAAAAADCOphQAAAAAAACMoykFAAAAAAAA42hKAQCqtJPfYGK32108E7gznl8AAFQuXmvdT0X8Tfn2PQBAlebp6SkPDw/t3btXYWFh8vb2rlFftWtSTfw6Y8uyVFRUpP3798vDw0Pe3t6unhIAAG7F29vb7Wu5mlZDVWT9RFMKAFCl2Ww2xcTEaN++fdq7d6+rp+PWLMuS3W6Xh4dHjSio/srPz08NGzaUhwcnkQMAUJE8PDzUuHFjZWZmum0tV1NrqIqon2hKAQCqPG9vbzVs2FAnTpxQcXGxq6fjtux2uw4cOKDQ0NAa1Zzx9PSsMe9sAgDgCu5ey9XEGqqi6ieaUgCAasFms6lWrVqqVauWq6fitux2u2rVqiVfX98aU1ABAAAz3LmWo4YqP9ICAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADGubQplZqaqtjYWAUGBiowMFBxcXFauHDhace/+uqruvzyyxUcHKzg4GDFx8drzZo1TmNsNlupP88884xjTExMTIn7J06cWGn7CQAAUFGonwAAgLtwaVOqQYMGmjhxotavX69169apZ8+e6t+/vzZv3lzq+PT0dA0aNEjLli3TypUrFR0drauuukp79uxxjMnMzHT6eeONN2Sz2ZSQkOC0rgkTJjiNGzlyZKXuKwAAQEWgfgIAAO7Cy5Ub79evn9Ptp59+WqmpqVq1apXatGlTYvzMmTOdbr/22mv64IMPtHTpUiUmJkqSIiIinMZ89NFHuuKKK9SkSROn5QEBASXGAgAAVHXUTwAAwF1UmWtKFRcXa/bs2Tpy5Iji4uLK9JijR4/q+PHjCgkJKfX+ffv26dNPP9XQoUNL3Ddx4kSFhoaqQ4cOeuaZZ3TixInzmj8AAIBp1E8AAKA6c+mZUpK0ceNGxcXFqaCgQP7+/po3b55at25dpseOHj1aUVFRio+PL/X+N998UwEBAbruuuuclt977726+OKLFRISom+++UbJycnKzMzUv//979Nuq7CwUIWFhY7beXl5kiS73S673V6m+bo7u90uy7LIwyAyN4u8zSNzs8i7dFUxD+on98KxZxZ5m0fmZpG3eWReUlmzcHlTqkWLFsrIyFBubq7mzp2rpKQkLV++/KyF1cSJEzV79mylp6fL19e31DFvvPGGBg8eXOL+Bx54wPF7bGysvL29deeddyolJUU+Pj6lrislJUXjx48vsXz//v0qKCg4227WCHa7Xbm5ubIsSx4eVeYkPLdG5maRt3lkbhZ5ly4/P9/VUyiB+sm9cOyZRd7mkblZ5G0emZdU1vrJZlmWVclzOSfx8fFq2rSppk2bdtoxU6ZM0VNPPaUlS5aoU6dOpY756quv1K1bN2VkZKh9+/Zn3ObmzZvVtm1b/fDDD2rRokWpY0p7py86OlqHDh1SYGBgGfbM/dntdu3fv19hYWEciIaQuVnkbR6Zm0XepcvLy1NwcLByc3Or7Gs+9VP1xrFnFnmbR+Zmkbd5ZF5SWesnl58pdSq73e5UvJxq8uTJevrpp5WWlnbagkqSXn/9dXXs2PGsBZUkZWRkyMPDQ+Hh4acd4+PjU+q7gB4eHjzp/sJms5GJYWRuFnmbR+ZmkXdJ1SEL6qfqj2PPLPI2j8zNIm/zyNxZWXNwaVMqOTlZffv2VcOGDZWfn69Zs2YpPT1daWlpkqTExETVr19fKSkpkqRJkyZpzJgxmjVrlmJiYpSVlSVJ8vf3l7+/v2O9eXl5mjNnjp599tkS21y5cqVWr16tK664QgEBAVq5cqXuv/9+3XLLLQoODjaw1wAAAOVH/QQAANyFS5tS2dnZSkxMVGZmpoKCghQbG6u0tDRdeeWVkqTdu3c7dddSU1NVVFSk66+/3mk9Y8eO1bhx4xy3Z8+eLcuyNGjQoBLb9PHx0ezZszVu3DgVFhaqcePGuv/++52ukwAAAFBVUT8BAAB3UeWuKVVd5OXlKSgoqEpfX8I0u92u7OxshYeHc8qiIWRuFnmbR+ZmkXfpeM2vOGRZOo49s8jbPDI3i7zNI/OSyvqaT1oAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4lzalUlNTFRsbq8DAQAUGBiouLk4LFy487fhXX31Vl19+uYKDgxUcHKz4+HitWbPGacyQIUNks9mcfvr06eM05uDBgxo8eLACAwNVt25dDR06VIcPH66UfQQAAKhI1E8AAMBduLQp1aBBA02cOFHr16/XunXr1LNnT/Xv31+bN28udXx6eroGDRqkZcuWaeXKlYqOjtZVV12lPXv2OI3r06ePMjMzHT/vvvuu0/2DBw/W5s2btXjxYn3yySf68ssvNWzYsErbTwAAgIpC/QQAANyFzbIsy9WT+KuQkBA988wzGjp06FnHFhcXKzg4WFOnTlViYqKkP97py8nJ0fz580t9zNatW9W6dWutXbtWnTp1kiQtWrRIV199tX777TdFRUWVaZ55eXkKCgpSbm6uAgMDy7Zzbs5utys7O1vh4eHy8OCToSaQuVnkbR6Zm0XepasOr/nUT9Ubx55Z5G0emZtF3uaReUllfc33MjinMyouLtacOXN05MgRxcXFlekxR48e1fHjxxUSEuK0PD09XeHh4QoODlbPnj311FNPKTQ0VJK0cuVK1a1b11FQSVJ8fLw8PDy0evVqDRw4sNRtFRYWqrCw0HE7Ly9P0h9PPrvdfk776q7sdrssyyIPg8jcLPI2j8zNIu/SVeU8qJ/cA8eeWeRtHpmbRd7mkXlJZc3C5U2pjRs3Ki4uTgUFBfL399e8efPUunXrMj129OjRioqKUnx8vGNZnz59dN1116lx48basWOH/u///k99+/bVypUr5enpqaysLIWHhzutx8vLSyEhIcrKyjrttlJSUjR+/PgSy/fv36+CgoIy7q17s9vtys3NlWVZdIcNIXOzyNs8MjeLvEuXn5/v6imUQP3kXjj2zCJv88jcLPI2j8xLKmv95PKmVIsWLZSRkaHc3FzNnTtXSUlJWr58+VkLq4kTJ2r27NlKT0+Xr6+vY/lNN93k+L1du3aKjY1V06ZNlZ6erl69epV7nsnJyXrggQcct/Py8hQdHa2wsDBOP/8fu90um82msLAwDkRDyNws8jaPzM0i79L9tc6oKqif3AvHnlnkbR6Zm0Xe5pF5SWWtn1zelPL29lazZs0kSR07dtTatWv1/PPPa9q0aad9zJQpUzRx4kQtWbJEsbGxZ1x/kyZNdMEFF+inn35Sr169FBERoezsbKcxJ06c0MGDBxUREXHa9fj4+MjHx6fEcg8PD550f2Gz2cjEMDI3i7zNI3OzyLukqpgF9ZP74dgzi7zNI3OzyNs8MndW1hyqXFp2u93p2gOnmjx5sp588kktWrTI6boGp/Pbb7/pwIEDioyMlCTFxcUpJydH69evd4z54osvZLfb1bVr1/PfAQAAAMOonwAAQHXk0jOlkpOT1bdvXzVs2FD5+fmaNWuW0tPTlZaWJklKTExU/fr1lZKSIkmaNGmSxowZo1mzZikmJsZxDQN/f3/5+/vr8OHDGj9+vBISEhQREaEdO3bokUceUbNmzdS7d29JUqtWrdSnTx/dcccdevnll3X8+HGNGDFCN910U5m/OQYAAMBVqJ8AAIC7cOmZUtnZ2UpMTFSLFi3Uq1cvrV27VmlpabryyislSbt371ZmZqZjfGpqqoqKinT99dcrMjLS8TNlyhRJkqenp77//ntde+21uvDCCzV06FB17NhRX331ldOp4zNnzlTLli3Vq1cvXX311brsssv0yiuvmN15AACAcqB+AgAA7sJmWZbl6klUR3l5eQoKClJubi4X6vwfu92u7OxshYeH8zlaQ8jcLPI2j8zNIu/S8ZpfcciydBx7ZpG3eWRuFnmbR+YllfU1n7QAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMbRlAIAAAAAAIBxNKUAAAAAAABgHE0pAAAAAAAAGEdTCgAAAAAAAMa5tCmVmpqq2NhYBQYGKjAwUHFxcVq4cOFpx7/66qu6/PLLFRwcrODgYMXHx2vNmjWO+48fP67Ro0erXbt2qlOnjqKiopSYmKi9e/c6rScmJkY2m83pZ+LEiZW2nwAAABWF+gkAALgLlzalGjRooIkTJ2r9+vVat26devbsqf79+2vz5s2ljk9PT9egQYO0bNkyrVy5UtHR0brqqqu0Z88eSdLRo0e1YcMGPfHEE9qwYYM+/PBDbdu2Tddee22JdU2YMEGZmZmOn5EjR1bqvgIAAFQE6icAAOAuvFy58X79+jndfvrpp5WamqpVq1apTZs2JcbPnDnT6fZrr72mDz74QEuXLlViYqKCgoK0ePFipzFTp05Vly5dtHv3bjVs2NCxPCAgQBERERW4NwAAAJWP+gkAALiLKnNNqeLiYs2ePVtHjhxRXFxcmR5z9OhRHT9+XCEhIacdk5ubK5vNprp16zotnzhxokJDQ9WhQwc988wzOnHixPlMHwAAwDjqJwAAUJ259EwpSdq4caPi4uJUUFAgf39/zZs3T61bty7TY0ePHq2oqCjFx8eXen9BQYFGjx6tQYMGKTAw0LH83nvv1cUXX6yQkBB98803Sk5OVmZmpv7973+fdluFhYUqLCx03M7Ly5Mk2e122e32Ms3X3dntdlmWRR4GkblZ5G0emZtF3qWrinlQP7kXjj2zyNs8MjeLvM0j85LKmoXLm1ItWrRQRkaGcnNzNXfuXCUlJWn58uVnLawmTpyo2bNnKz09Xb6+viXuP378uG644QZZlqXU1FSn+x544AHH77GxsfL29tadd96plJQU+fj4lLq9lJQUjR8/vsTy/fv3q6CgoCy76vbsdrtyc3NlWZY8PKrMSXhujczNIm/zyNws8i5dfn6+q6dQAvWTe+HYM4u8zSNzs8jbPDIvqaz1k82yLKuS53JO4uPj1bRpU02bNu20Y6ZMmaKnnnpKS5YsUadOnUrcf7Kg+vnnn/XFF18oNDT0jNvcvHmz2rZtqx9++EEtWrQodUxp7/RFR0fr0KFDTu8i1mR2u1379+9XWFgYB6IhZG4WeZtH5maRd+ny8vIUHBys3NzcKvuaT/1UvXHsmUXe5pG5WeRtHpmXVNb6yeVnSp3Kbrc7FS+nmjx5sp5++mmlpaWdsaDavn27li1bdtaCSpIyMjLk4eGh8PDw047x8fEp9V1ADw8PnnR/YbPZyMQwMjeLvM0jc7PIu6TqkAX1U/XHsWcWeZtH5maRt3lk7qysObi0KZWcnKy+ffuqYcOGys/P16xZs5Senq60tDRJUmJiourXr6+UlBRJ0qRJkzRmzBjNmjVLMTExysrKkiT5+/vL399fx48f1/XXX68NGzbok08+UXFxsWNMSEiIvL29tXLlSq1evVpXXHGFAgICtHLlSt1///265ZZbFBwc7JogAAAAyoj6CQAAuAuXNqWys7OVmJiozMxMBQUFKTY2VmlpabryyislSbt373bqrqWmpqqoqEjXX3+903rGjh2rcePGac+ePVqwYIEk6aKLLnIas2zZMvXo0UM+Pj6aPXu2xo0bp8LCQjVu3Fj333+/03USAAAAqirqJwAA4C5c2pR6/fXXz3h/enq60+1ffvnljONjYmJ0tktkXXzxxVq1alVZpgcAAFDlUD8BAAB3wYcdAQAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADGlasp9euvv+q3335z3F6zZo1GjRqlV155pcImBgAA4G6ooQAAAP5UrqbUzTffrGXLlkmSsrKydOWVV2rNmjV67LHHNGHChAqdIAAAgLughgIAAPhTuZpSmzZtUpcuXSRJ77//vtq2batvvvlGM2fO1IwZMypyfgAAAG6DGgoAAOBP5WpKHT9+XD4+PpKkJUuW6Nprr5UktWzZUpmZmRU3OwAAADdCDQUAAPCncjWl2rRpo5dffllfffWVFi9erD59+kiS9u7dq9DQ0AqdIAAAgLughgIAAPhTuZpSkyZN0rRp09SjRw8NGjRI7du3lyQtWLDAcUo6AAAAnFFDAQAA/MmrPA/q0aOHfv/9d+Xl5Sk4ONixfNiwYfLz86uwyQEAALgTaigAAIA/letMqWPHjqmwsNBRTO3atUvPPfectm3bpvDw8AqdIAAAgLughgIAAPhTuZpS/fv311tvvSVJysnJUdeuXfXss89qwIABSk1NrdAJAgAAuAtqKAAAgD+Vqym1YcMGXX755ZKkuXPnql69etq1a5feeustvfDCCxU6QQAAAHdBDQUAAPCncjWljh49qoCAAEnS559/ruuuu04eHh7629/+pl27dlXoBAEAANwFNRQAAMCfytWUatasmebPn69ff/1VaWlpuuqqqyRJ2dnZCgwMrNAJAgAAuAtqKAAAgD+Vqyk1ZswYPfTQQ4qJiVGXLl0UFxcn6Y93/Dp06FChEwQAAHAX1FAAAAB/8irPg66//npddtllyszMVPv27R3Le/XqpYEDB1bY5AAAANwJNRQAAMCfytWUkqSIiAhFRETot99+kyQ1aNBAXbp0qbCJAQAAuCNqKAAAgD+U6+N7drtdEyZMUFBQkBo1aqRGjRqpbt26evLJJ2W32yt6jgAAAG6BGgoAAOBP5TpT6rHHHtPrr7+uiRMn6tJLL5Ukff311xo3bpwKCgr09NNPV+gkAQAA3AE1FAAAwJ/K1ZR688039dprr+naa691LIuNjVX9+vV1zz33UFABAACUghoKAADgT+X6+N7BgwfVsmXLEstbtmypgwcPnvekAAAA3BE1FAAAwJ/K1ZRq3769pk6dWmL51KlTFRsbe96TAgAAcEfUUAAAAH8q18f3Jk+erGuuuUZLlixRXFycJGnlypX69ddf9dlnn1XoBAEAANwFNRQAAMCfynWmVPfu3fXjjz9q4MCBysnJUU5Ojq677jpt3rxZb7/9dkXPEQAAwC1QQwEAAPypXGdKSVJUVFSJi3F+9913ev311/XKK6+c98QAAADcETUUAADAH8p1phQAAAAAAABwPmhKAQAAAAAAwDiaUgAAAAAAADDunK4pdd11153x/pycnPOZCwAAgFuihgIAACjpnJpSQUFBZ70/MTHxvCYEAADgbqihAAAASjqnptT06dMrax4AAABuixoKAACgJK4pBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjHNpUyo1NVWxsbEKDAxUYGCg4uLitHDhwtOOf/XVV3X55ZcrODhYwcHBio+P15o1a5zGWJalMWPGKDIyUrVr11Z8fLy2b9/uNObgwYMaPHiwAgMDVbduXQ0dOlSHDx+ulH0EAACoSNRPAADAXbi0KdWgQQNNnDhR69ev17p169SzZ0/1799fmzdvLnV8enq6Bg0apGXLlmnlypWKjo7WVVddpT179jjGTJ48WS+88IJefvllrV69WnXq1FHv3r1VUFDgGDN48GBt3rxZixcv1ieffKIvv/xSw4YNq/T9BQAAOF/UTwAAwG1YVUxwcLD12muvlWnsiRMnrICAAOvNN9+0LMuy7Ha7FRERYT3zzDOOMTk5OZaPj4/17rvvWpZlWVu2bLEkWWvXrnWMWbhwoWWz2aw9e/aUeZ65ubmWJCs3N7fMj3F3xcXFVmZmplVcXOzqqdQYZG4WeZtH5maRd+mqw2s+9VP1xrFnFnmbR+Zmkbd5ZF5SWV/zvVzZEPur4uJizZkzR0eOHFFcXFyZHnP06FEdP35cISEhkqSdO3cqKytL8fHxjjFBQUHq2rWrVq5cqZtuukkrV65U3bp11alTJ8eY+Ph4eXh4aPXq1Ro4cGCp2yosLFRhYaHjdl5eniTJbrfLbref8/66I7vdLsuyyMMgMjeLvM0jc7PIu3RVOQ/qJ/fAsWcWeZtH5maRt3lkXlJZs3B5U2rjxo2Ki4tTQUGB/P39NW/ePLVu3bpMjx09erSioqIcRVRWVpYkqV69ek7j6tWr57gvKytL4eHhTvd7eXkpJCTEMaY0KSkpGj9+fInl+/fvdzq1vSaz2+3Kzc2VZVny8OAa+iaQuVnkbR6Zm0XepcvPz3f1FEqgfnIvHHtmkbd5ZG4WeZtH5iWVtX5yeVOqRYsWysjIUG5urubOnaukpCQtX778rIXVxIkTNXv2bKWnp8vX17fS55mcnKwHHnjAcTsvL0/R0dEKCwtTYGBgpW+/OrDb7bLZbAoLC+NANITMzSJv88jcLPIunYk641xRP7kXjj2zyNs8MjeLvM0j85LKWme4vCnl7e2tZs2aSZI6duyotWvX6vnnn9e0adNO+5gpU6Zo4sSJWrJkiWJjYx3LIyIiJEn79u1TZGSkY/m+fft00UUXOcZkZ2c7re/EiRM6ePCg4/Gl8fHxkY+PT4nlHh4ePOn+wmazkYlhZG4WeZtH5maRd0lVMQvqJ/fDsWcWeZtH5maRt3lk7qysOVS5tOx2u9O1B041efJkPfnkk1q0aJHTdQ0kqXHjxoqIiNDSpUsdy/Ly8rR69WrHdRbi4uKUk5Oj9evXO8Z88cUXstvt6tq1awXvDQAAQOWjfgIAANWRS8+USk5OVt++fdWwYUPl5+dr1qxZSk9PV1pamiQpMTFR9evXV0pKiiRp0qRJGjNmjGbNmqWYmBjHNQz8/f3l7+8vm82mUaNG6amnnlLz5s3VuHFjPfHEE4qKitKAAQMkSa1atVKfPn10xx136OWXX9bx48c1YsQI3XTTTYqKinJJDgAAAGVF/QQAANyFS5tS2dnZSkxMVGZmpoKCghQbG6u0tDRdeeWVkqTdu3c7nfKVmpqqoqIiXX/99U7rGTt2rMaNGydJeuSRR3TkyBENGzZMOTk5uuyyy7Ro0SKnzzPOnDlTI0aMUK9eveTh4aGEhAS98MILlb/DAAAA54n6CQAAuAubZVmWqydRHeXl5SkoKEi5ublcqPN/7Ha7srOzFR4ezudoDSFzs8jbPDI3i7xLx2t+xSHL0nHsmUXe5pG5WeRtHpmXVNbXfNICAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABjn0qZUamqqYmNjFRgYqMDAQMXFxWnhwoWnHb9582YlJCQoJiZGNptNzz33XIkxJ+879Wf48OGOMT169Chx/1133VUZuwgAAFChqJ8AAIC78HLlxhs0aKCJEyeqefPmsixLb775pvr3769vv/1Wbdq0KTH+6NGjatKkif7xj3/o/vvvL3Wda9euVXFxseP2pk2bdOWVV+of//iH07g77rhDEyZMcNz28/OroL0CAACoPNRPAADAXbi0KdWvXz+n208//bRSU1O1atWqUouqzp07q3PnzpKkRx99tNR1hoWFOd2eOHGimjZtqu7duzst9/PzU0RExPlMHwAAwDjqJwAA4C6qzDWliouLNXv2bB05ckRxcXEVss6ioiK98847uv3222Wz2Zzumzlzpi644AK1bdtWycnJOnr0aIVsEwAAwBTqJwAAUJ259EwpSdq4caPi4uJUUFAgf39/zZs3T61bt66Qdc+fP185OTkaMmSI0/Kbb75ZjRo1UlRUlL7//nuNHj1a27Zt04cffnjadRUWFqqwsNBxOy8vT5Jkt9tlt9srZL7Vnd1ul2VZ5GEQmZtF3uaRuVnkXbqqmAf1k3vh2DOLvM0jc7PI2zwyL6msWbi8KdWiRQtlZGQoNzdXc+fOVVJSkpYvX14hhdXrr7+uvn37Kioqymn5sGHDHL+3a9dOkZGR6tWrl3bs2KGmTZuWuq6UlBSNHz++xPL9+/eroKDgvOfqDux2u3Jzc2VZljw8qsxJeG6NzM0ib/PI3CzyLl1+fr6rp1AC9ZN74dgzi7zNI3OzyNs8Mi+prPWTy5tS3t7eatasmSSpY8eOWrt2rZ5//nlNmzbtvNa7a9cuLVmy5Izv3p3UtWtXSdJPP/102qIqOTlZDzzwgON2Xl6eoqOjFRYWpsDAwPOaq7uw2+2y2WwKCwvjQDSEzM0ib/PI3CzyLp2vr6+rp1AC9ZN74dgzi7zNI3OzyNs8Mi+prPWTy5tSp7Lb7U6neZfX9OnTFR4ermuuueasYzMyMiRJkZGRpx3j4+MjHx+fEss9PDx40v2FzWYjE8PI3CzyNo/MzSLvkqpDFtRP1R/HnlnkbR6Zm0Xe5pG5s7Lm4NKmVHJysvr27auGDRsqPz9fs2bNUnp6utLS0iRJiYmJql+/vlJSUiT9ceHNLVu2OH7fs2ePMjIy5O/v73i3UPqjMJs+fbqSkpLk5eW8izt27NCsWbN09dVXKzQ0VN9//73uv/9+devWTbGxsYb2HAAAoHyonwAAgLtwaVMqOztbiYmJyszMVFBQkGJjY5WWlqYrr7xSkrR7926n7trevXvVoUMHx+0pU6ZoypQp6t69u9LT0x3LlyxZot27d+v2228vsU1vb28tWbJEzz33nI4cOaLo6GglJCTo8ccfr7wdBQAAqCDUTwAAwF3YLMuyXD2J6igvL09BQUHKzc3lmgj/Y7fblZ2drfDwcE5ZNITMzSJv88jcLPIuHa/5FYcsS8exZxZ5m0fmZpG3eWReUllf80kLAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAY59KmVGpqqmJjYxUYGKjAwEDFxcVp4cKFpx2/efNmJSQkKCYmRjabTc8991yJMePGjZPNZnP6admypdOYgoICDR8+XKGhofL391dCQoL27dtX0bsHAABQ4aifAACAu3BpU6pBgwaaOHGi1q9fr3Xr1qlnz57q37+/Nm/eXOr4o0ePqkmTJpo4caIiIiJOu942bdooMzPT8fP111873X///ffr448/1pw5c7R8+XLt3btX1113XYXuGwAAQGWgfgIAAO7Cy5Ub79evn9Ptp59+WqmpqVq1apXatGlTYnznzp3VuXNnSdKjjz562vV6eXmdtujKzc3V66+/rlmzZqlnz56SpOnTp6tVq1ZatWqV/va3v5V3dwAAACod9RMAAHAXLm1K/VVxcbHmzJmjI0eOKC4u7rzWtX37dkVFRcnX11dxcXFKSUlRw4YNJUnr16/X8ePHFR8f7xjfsmVLNWzYUCtXrjxtUVVYWKjCwkLH7by8PEmS3W6X3W4/r/m6C7vdLsuyyMMgMjeLvM0jc7PIu3RVOQ/qJ/fAsWcWeZtH5maRt3lkXlJZs3B5U2rjxo2Ki4tTQUGB/P39NW/ePLVu3brc6+vatatmzJihFi1aKDMzU+PHj9fll1+uTZs2KSAgQFlZWfL29lbdunWdHlevXj1lZWWddr0pKSkaP358ieX79+9XQUFBuefrTux2u3Jzc2VZljw8uIa+CWRuFnmbR+ZmkXfp8vPzXT2FEqif3AvHnlnkbR6Zm0Xe5pF5SWWtn1zelGrRooUyMjKUm5uruXPnKikpScuXLy93YdW3b1/H77GxseratasaNWqk999/X0OHDi33PJOTk/XAAw84bufl5Sk6OlphYWEKDAws93rdid1ul81mU1hYGAeiIWRuFnmbR+ZmkXfpfH19XT2FEqif3AvHnlnkbR6Zm0Xe5pF5SWWtn1zelPL29lazZs0kSR07dtTatWv1/PPPa9q0aRWy/rp16+rCCy/UTz/9JEmKiIhQUVGRcnJynN7t27dv3xkv/unj4yMfH58Syz08PHjS/YXNZiMTw8jcLPI2j8zNIu+SqmIW1E/uh2PPLPI2j8zNIm/zyNxZWXOocmnZ7Xanaw+cr8OHD2vHjh2KjIyU9EfhVqtWLS1dutQxZtu2bdq9e/d5X4sBAADAFaifAABAdeTSM6WSk5PVt29fNWzYUPn5+Zo1a5bS09OVlpYmSUpMTFT9+vWVkpIiSSoqKtKWLVscv+/Zs0cZGRny9/d3vFv40EMPqV+/fmrUqJH27t2rsWPHytPTU4MGDZIkBQUFaejQoXrggQcUEhKiwMBAjRw5UnFxcXxzDAAAqPKonwAAgLtwaVMqOztbiYmJyszMVFBQkGJjY5WWlqYrr7xSkrR7926nU7727t2rDh06OG5PmTJFU6ZMUffu3ZWeni5J+u233zRo0CAdOHBAYWFhuuyyy7Rq1SqFhYU5Hvef//xHHh4eSkhIUGFhoXr37q2XXnrJzE4DAACcB+onAADgLmyWZVmunkR1lJeXp6CgIOXm5nKhzv+x2+3Kzs5WeHg4n6M1hMzNIm/zyNws8i4dr/kVhyxLx7FnFnmbR+Zmkbd5ZF5SWV/zSQsAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYJxLm1KpqamKjY1VYGCgAgMDFRcXp4ULF552/ObNm5WQkKCYmBjZbDY999xzJcakpKSoc+fOCggIUHh4uAYMGKBt27Y5jenRo4dsNpvTz1133VXRuwcAAFDhqJ8AAIC7cGlTqkGDBpo4caLWr1+vdevWqWfPnurfv782b95c6vijR4+qSZMmmjhxoiIiIkods3z5cg0fPlyrVq3S4sWLdfz4cV111VU6cuSI07g77rhDmZmZjp/JkydX+P4BAABUNOonAADgLrxcufF+/fo53X766aeVmpqqVatWqU2bNiXGd+7cWZ07d5YkPfroo6Wuc9GiRU63Z8yYofDwcK1fv17dunVzLPfz8zttYQYAAFBVUT8BqE6K7ZZW/3xAP/12UM0Oe6prkwvk6WFz9bQAVBFV5ppSxcXFmj17to4cOaK4uLgKW29ubq4kKSQkxGn5zJkzdcEFF6ht27ZKTk7W0aNHK2ybAAAAJlA/AajKFm3K1GWTvtDNr63RmEU7dfNra3TZpC+0aFOmq6cGoIpw6ZlSkrRx40bFxcWpoKBA/v7+mjdvnlq3bl0h67bb7Ro1apQuvfRStW3b1rH85ptvVqNGjRQVFaXvv/9eo0eP1rZt2/Thhx+edl2FhYUqLCx03M7Ly3Nsw263V8h8qzu73S7LssjDIDI3i7zNI3OzyLt0VTEP6if3wrFnFnmbsWhTlobP+lbWKcuzcgt09zsb9N+bO6hPW868rAw8x80j85LKmoXLm1ItWrRQRkaGcnNzNXfuXCUlJWn58uUVUlgNHz5cmzZt0tdff+20fNiwYY7f27Vrp8jISPXq1Us7duxQ06ZNS11XSkqKxo8fX2L5/v37VVBQcN5zdQd2u125ubmyLEseHlXmJDy3RuZmkbd5ZG4WeZcuPz/f1VMogfrJvXDsmUXela/Ybmncgk0lGlKSHMvGL9ik9hfY+ChfJeA5bh6Zl1TW+snlTSlvb281a9ZMktSxY0etXbtWzz//vKZNm3Ze6x0xYoQ++eQTffnll2rQoMEZx3bt2lWS9NNPP522qEpOTtYDDzzguJ2Xl6fo6GiFhYUpMDDwvObqLux2u2w2m8LCwjgQDSFzs8jbPDI3i7xL5+vr6+oplED95F449swi78q36ucDyj58/Ixj9h0+rl1HvfS3JqGGZlVz8Bw3j8xLKmv95PKm1KnsdrvTad7nyrIsjRw5UvPmzVN6eroaN2581sdkZGRIkiIjI087xsfHRz4+PiWWe3h48KT7C5vNRiaGkblZ5G0emZtF3iVVhyyon6o/jj2zyLty7T9cVOZx/A0qB89x88jcWVlzcGlTKjk5WX379lXDhg2Vn5+vWbNmKT09XWlpaZKkxMRE1a9fXykpKZKkoqIibdmyxfH7nj17lJGRIX9/f8e7hcOHD9esWbP00UcfKSAgQFlZWZKkoKAg1a5dWzt27NCsWbN09dVXKzQ0VN9//73uv/9+devWTbGxsS5IAQAAoOyonwBUdeEBZTtDoqzjALgvlzalsrOzlZiYqMzMTAUFBSk2NlZpaWm68sorJUm7d+926q7t3btXHTp0cNyeMmWKpkyZou7duys9PV2SlJqaKknq0aOH07amT5+uIUOGyNvbW0uWLNFzzz2nI0eOKDo6WgkJCXr88ccrd2cBAAAqAPUTgKquS+MQRQb5Kiu3oNTrStkkRQT5qkvjkFLuBVCTuLQp9frrr5/x/pOF0kkxMTGyrNL+WfvT2e6Pjo7W8uXLyzQ/AACAqob6CUBV5+lh09h+rXX3Oxtkk5waUycvaz62X2sucg5AfNgRAAAAAFCh+rSNVOotFysiyPkjehFBvkq95WL1aXv669EBqDmq3IXOAQAAAADVX5+2kbqydYRW//y7fvptv5o1CFPXJhdwhhQAB5pSAAAAAIBK4elh09+ahKqJf7HCw0PlQUMKwF/w8T0AAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHE0pQAAAAAAAGAcTSkAAAAAAAAYR1MKAAAAAAAAxtGUAgAAAAAAgHFerp5AdWVZliQpLy/PxTOpOux2u/Lz8+Xr6ysPD/qdJpC5WeRtHpmbRd6lO/laf/K1H+VH/VQ6jj2zyNs8MjeLvM0j85LKWj/RlCqn/Px8SVJ0dLSLZwIAAEzIz89XUFCQq6dRrVE/AQBQs5ytfrJZvO1XLna7XXv37lVAQIBsNpurp1Ml5OXlKTo6Wr/++qsCAwNdPZ0agczNIm/zyNws8i6dZVnKz89XVFQU736eJ+qn0nHsmUXe5pG5WeRtHpmXVNb6iTOlysnDw0MNGjRw9TSqpMDAQA5Ew8jcLPI2j8zNIu+SOEOqYlA/nRnHnlnkbR6Zm0Xe5pG5s7LUT7zdBwAAAAAAAONoSgEAAAAAAMA4mlKoMD4+Pho7dqx8fHxcPZUag8zNIm/zyNws8gZcg2PPLPI2j8zNIm/zyLz8uNA5AAAAAAAAjONMKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlUGYHDx7U4MGDFRgYqLp162ro0KE6fPjwGR9TUFCg4cOHKzQ0VP7+/kpISNC+fftKHXvgwAE1aNBANptNOTk5lbAH1U9lZP7dd99p0KBBio6OVu3atdWqVSs9//zzlb0rVdZ///tfxcTEyNfXV127dtWaNWvOOH7OnDlq2bKlfH191a5dO3322WdO91uWpTFjxigyMlK1a9dWfHy8tm/fXpm7UK1UZN7Hjx/X6NGj1a5dO9WpU0dRUVFKTEzU3r17K3s3qpWKfo7/1V133SWbzabnnnuugmcNVG/nctwdP35cEyZMUNOmTeXr66v27dtr0aJFJcbt2bNHt9xyi0JDQ1W7dm21a9dO69atq8zdqFYqOvPi4mI98cQTaty4sWrXrq2mTZvqySefFJfjlb788kv169dPUVFRstlsmj9//lkfk56erosvvlg+Pj5q1qyZZsyYUWLMub5e1RSVkXdKSoo6d+6sgIAAhYeHa8CAAdq2bVvl7EA1VFnP8ZMmTpwom82mUaNGVdicqzULKKM+ffpY7du3t1atWmV99dVXVrNmzaxBgwad8TF33XWXFR0dbS1dutRat26d9be//c265JJLSh3bv39/q2/fvpYk69ChQ5WwB9VPZWT++uuvW/fee6+Vnp5u7dixw3r77bet2rVrWy+++GJl706VM3v2bMvb29t64403rM2bN1t33HGHVbduXWvfvn2ljl+xYoXl6elpTZ482dqyZYv1+OOPW7Vq1bI2btzoGDNx4kQrKCjImj9/vvXdd99Z1157rdW4cWPr2LFjpnaryqrovHNycqz4+Hjrvffes3744Qdr5cqVVpcuXayOHTua3K0qrTKe4yd9+OGHVvv27a2oqCjrP//5TyXvCVB9nOtx98gjj1hRUVHWp59+au3YscN66aWXLF9fX2vDhg2OMQcPHrQaNWpkDRkyxFq9erX1888/W2lpadZPP/1kareqtMrI/Omnn7ZCQ0OtTz75xNq5c6c1Z84cy9/f33r++edN7VaV9dlnn1mPPfaY9eGHH1qSrHnz5p1x/M8//2z5+flZDzzwgLVlyxbrxRdftDw9Pa1FixY5xpzr37AmqYy8e/fubU2fPt3atGmTlZGRYV199dVWw4YNrcOHD1fy3lQPlZH5SWvWrLFiYmKs2NhY67777qucHahmaEqhTLZs2WJJstauXetYtnDhQstms1l79uwp9TE5OTlWrVq1rDlz5jiWbd261ZJkrVy50mnsSy+9ZHXv3t1aunQpTan/qezM/+qee+6xrrjiioqbfDXRpUsXa/jw4Y7bxcXFVlRUlJWSklLq+BtuuMG65pprnJZ17drVuvPOOy3Lsiy73W5FRERYzzzzjOP+nJwcy8fHx3r33XcrYQ+ql4rOuzRr1qyxJFm7du2qmElXc5WV+W+//WbVr1/f2rRpk9WoUSOaUsBfnOtxFxkZaU2dOtVp2XXXXWcNHjzYcXv06NHWZZddVjkTdgOVkfk111xj3X777WccA6tM/8P+yCOPWG3atHFaduONN1q9e/d23D7Xv2FNVVF5nyo7O9uSZC1fvrwipulWKjLz/Px8q3nz5tbixYut7t2705T6Hz6+hzJZuXKl6tatq06dOjmWxcfHy8PDQ6tXry71MevXr9fx48cVHx/vWNayZUs1bNhQK1eudCzbsmWLJkyYoLfeekseHjwlT6rMzE+Vm5urkJCQipt8NVBUVKT169c7ZeXh4aH4+PjTZrVy5Uqn8ZLUu3dvx/idO3cqKyvLaUxQUJC6du16xvxrgsrIuzS5ubmy2WyqW7duhcy7OquszO12u2699VY9/PDDatOmTeVMHqimynPcFRYWytfX12lZ7dq19fXXXztuL1iwQJ06ddI//vEPhYeHq0OHDnr11VcrZyeqmcrK/JJLLtHSpUv1448/Svrj8gdff/21+vbtWwl74d7O9tpSnr8hTq+89ZOkGvf/AxWlrJkPHz5c11xzTYmxNR0dAJRJVlaWwsPDnZZ5eXkpJCREWVlZp32Mt7d3if85rFevnuMxhYWFGjRokJ555hk1bNiwUuZeXVVW5qf65ptv9N5772nYsGEVMu/q4vfff1dxcbHq1avntPxMWWVlZZ1x/Mn/nss6a4rKyPtUBQUFGj16tAYNGqTAwMCKmXg1VlmZT5o0SV5eXrr33nsrftJANVee4653797697//re3bt8tut2vx4sX68MMPlZmZ6Rjz888/KzU1Vc2bN1daWpruvvtu3XvvvXrzzTcrdX+qg8rK/NFHH9VNN92kli1bqlatWurQoYNGjRqlwYMHV+r+uKPTvbbk5eXp2LFj5fob4vTOlvep7Ha7Ro0apUsvvVRt27Y1NU23UpbMZ8+erQ0bNiglJcUVU6zSaErVcI8++qhsNtsZf3744YdK235ycrJatWqlW265pdK2UdW4OvO/2rRpk/r376+xY8fqqquuMrJNoDIcP35cN9xwgyzLUmpqqqun47bWr1+v559/XjNmzJDNZnP1dAC38Pzzz6t58+Zq2bKlvL29NWLECN12221OZ4/b7XZdfPHF+te//qUOHTpo2LBhuuOOO/Tyyy+7cObVV1kyf//99zVz5kzNmjVLGzZs0JtvvqkpU6bQCITbGT58uDZt2qTZs2e7eipu69dff9V9992nmTNnljhLE5KXqycA13rwwQc1ZMiQM45p0qSJIiIilJ2d7bT8xIkTOnjwoCIiIkp9XEREhIqKipSTk+N05s6+ffscj/niiy+0ceNGzZ07V5Ic32hywQUX6LHHHtP48ePLuWdVl6szP2nLli3q1auXhg0bpscff7xc+1KdXXDBBfL09CzxbZClZXVSRETEGcef/O++ffsUGRnpNOaiiy6qwNlXP5WR90knG1K7du3SF198wVlS/1MZmX/11VfKzs52OrO1uLhYDz74oJ577jn98ssvFbsTQDVTnuMuLCxM8+fPV0FBgQ4cOKCoqCg9+uijatKkiWNMZGSkWrdu7fS4Vq1a6YMPPqj4nahmKivzhx9+2HG2lCS1a9dOu3btUkpKipKSkipvh9zQ6V5bAgMDVbt2bXl6ep7z3xCnd7a8/2rEiBH65JNP9OWXX6pBgwYmp+lWzpb5+vXrlZ2drYsvvthxf3Fxsb788ktNnTpVhYWF8vT0ND3tKoMzpWq4sLAwtWzZ8ow/3t7eiouLU05OjtavX+947BdffCG73a6uXbuWuu6OHTuqVq1aWrp0qWPZtm3btHv3bsXFxUmSPvjgA3333XfKyMhQRkaGXnvtNUl//I/P8OHDK3HPXcfVmUvS5s2bdcUVVygpKUlPP/105e1sFebt7a2OHTs6ZWW327V06VKnrP4qLi7OabwkLV682DG+cePGioiIcBqTl5en1atXn3adNUVl5C392ZDavn27lixZotDQ0MrZgWqoMjK/9dZb9f333zv+zc7IyFBUVJQefvhhpaWlVd7OANVEeY67k3x9fVW/fn2dOHFCH3zwgfr37++479JLLy3xde0//vijGjVqVLE7UA1VVuZHjx4tca1TT09P2e32it2BGuBsry3n8zdESWWpnyzL0ogRIzRv3jx98cUXaty4selpupWzZd6rVy9t3LjRqX7q1KmTBg8erIyMjBrdkJIkvn0PZdanTx+rQ4cO1urVq62vv/7aat68uTVo0CDH/b/99pvVokULa/Xq1Y5ld911l9WwYUPriy++sNatW2fFxcVZcXFxp93GsmXL+Pa9v6iMzDdu3GiFhYVZt9xyi5WZmen4yc7ONrpvVcHs2bMtHx8fa8aMGdaWLVusYcOGWXXr1rWysrIsy7KsW2+91Xr00Ucd41esWGF5eXlZU6ZMsbZu3WqNHTvWqlWrlrVx40bHmIkTJ1p169a1PvroI+v777+3+vfvbzVu3Ng6duyY8f2raio676KiIuvaa6+1GjRoYGVkZDg9nwsLC12yj1VNZTzHT8W37wHOzvW4W7VqlfXBBx9YO3bssL788kurZ8+eVuPGjZ1qoTVr1lheXl7W008/bW3fvt2aOXOm5efnZ73zzjumd69KqozMk5KSrPr161uffPKJtXPnTuvDDz+0LrjgAuuRRx4xvXtVTn5+vvXtt99a3377rSXJ+ve//219++23jm++ffTRR61bb73VMf7nn3+2/Pz8rIcfftjaunWr9d///tfy9PS0Fi1a5Bhztr9hTVYZed99991WUFCQlZ6e7lQ/HT161Pj+VUWVkfmp+Pa9P9GUQpkdOHDAGjRokOXv728FBgZat912m5Wfn++4f+fOnZYka9myZY5lx44ds+655x4rODjY8vPzswYOHGhlZmaedhs0pZxVRuZjx461JJX4adSokcE9qzpefPFFq2HDhpa3t7fVpUsXa9WqVY77unfvbiUlJTmNf//9960LL7zQ8vb2ttq0aWN9+umnTvfb7XbriSeesOrVq2f5+PhYvXr1srZt22ZiV6qFisz75PO/tJ+/HhM1XUU/x09FUwoo6VyOu/T0dKtVq1aWj4+PFRoaat16663Wnj17Sqzz448/ttq2bWv5+PhYLVu2tF555RUTu1JtVHTmeXl51n333Wc1bNjQ8vX1tZo0aWI99thjvOlh/Vmvn/pzMuOkpCSre/fuJR5z0UUXWd7e3laTJk2s6dOnl1jvmf6GNVll5H26+qm0v0tNVFnP8b+iKfUnm2X97yI+AAAAAAAAgCFcUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAAAAAAADG0ZQCAAAAAACAcTSlAAAAAAAAYBxNKQAAAAAAABhHUwoAKpnNZtP8+fNdPQ0AAIBqhRoKcH80pQC4tSFDhshms5X46dOnj6unBgAAUGVRQwEwwcvVEwCAytanTx9Nnz7daZmPj4+LZgMAAFA9UEMBqGycKQXA7fn4+CgiIsLpJzg4WNIfp4Wnpqaqb9++ql27tpo0aaK5c+c6PX7jxo3q2bOnateurdDQUA0bNkyHDx92GvPGG2+oTZs28vHxUWRkpEaMGOF0/++//66BAwfKz89PzZs314IFCyp3pwEAAM4TNRSAykZTCkCN98QTTyghIUHfffedBg8erJtuuklbt26VJB05ckS9e/dWcHCw1q5dqzlz5mjJkiVOBVNqaqqGDx+uYcOGaePGjVqwYIGaNWvmtI3x48frhhtu0Pfff6+rr75agwcP1sGDB43uJwAAQEWihgJw3iwAcGNJSUmWp6enVadOHaefp59+2rIsy5Jk3XXXXU6P6dq1q3X33XdblmVZr7zyihUcHGwdPnzYcf+nn35qeXh4WFlZWZZlWVZUVJT12GOPnXYOkqzHH3/ccfvw4cOWJGvhwoUVtp8AAAAViRoKgAlcUwqA27viiiuUmprqtCwkJMTxe1xcnNN9cXFxysjIkCRt3bpV7du3V506dRz3X3rppbLb7dq2bZtsNpv27t2rXr16nXEOsbGxjt/r1KmjwMBAZWdnl3eXAAAAKh01FIDKRlMKgNurU6dOiVPBK0rt2rXLNK5WrVpOt202m+x2e2VMCQAAoEJQQwGobFxTCkCNt2rVqhK3W7VqJUlq1aqVvvvuOx05csRx/4oVK+Th4aEWLVooICBAMTExWrp0qdE5AwAAuBo1FIDzxZlSANxeYWGhsrKynJZ5eXnpggsukCTNmTNHnTp10mWXXaaZM2dqzZo1ev311yVJgwcP1tixY5WUlKRx48Zp//79GjlypG699VbVq1dPkjRu3DjdddddCg8PV9++fZWfn68VK1Zo5MiRZncUAACgAlFDAahsNKUAuL1FixYpMjLSaVmLFi30ww8/SPrjW11mz56te+65R5GRkXr33XfVunVrSZKfn5/S0tJ03333qXPnzvLz81NCQoL+/e9/O9aVlJSkgoIC/ec//9FDDz2kCy64QNdff725HQQAAKgE1FAAKpvNsizL1ZMAAFex2WyaN2+eBgwY4OqpAAAAVBvUUAAqAteUAgAAAAAAgHE0pQAAAAAAAGAcH98DAAAAAACAcZwpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAONoSgEAAAAAAMA4mlIAAAAAAAAwjqYUAAAAAAAAjKMpBQAAAAAAAOP+H8a1BmlCFWxsAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 3.1238\n",
            "Final Validation Loss: 3.3057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Next Token Prediction\n",
        "\n",
        "Test the model's ability to predict the next token given some input text."
      ],
      "metadata": {
        "id": "_PRx6SpSV8JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(model, tokenizer, text: str, top_k: int = 5):\n",
        "    \"\"\"Predict next token for given text\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode input\n",
        "    encoded = tokenizer.encoder(text)\n",
        "    if not encoded:\n",
        "        return []\n",
        "\n",
        "    input_seq = torch.tensor([encoded], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(1)\n",
        "        outputs, hidden = model(input_seq, hidden)\n",
        "\n",
        "        # Get predictions for last position\n",
        "        last_predictions = outputs[0, -1, :]\n",
        "        probabilities = torch.softmax(last_predictions, dim=0)\n",
        "\n",
        "        # Get top-k predictions\n",
        "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "        results = []\n",
        "        for i in range(top_k):\n",
        "            token_idx = top_k_indices[i].item()\n",
        "            prob = top_k_probs[i].item()\n",
        "            token = tokenizer.idx_to_word[token_idx]\n",
        "\n",
        "            results.append({\n",
        "                'token': token,\n",
        "                'probability': prob,\n",
        "                'rank': i + 1\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test next token prediction\n",
        "test_texts = [\n",
        "    \"ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\",\n",
        "    \"ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä\",\n",
        "    \"ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä\",\n",
        "    \"ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß ÿßŸÑÿ≠ÿØŸäÿ´ÿ©\"\n",
        "]\n",
        "\n",
        "print(\"=== Next Token Prediction Results ===\")\n",
        "print()\n",
        "\n",
        "for test_text in test_texts:\n",
        "    print(f\"Input text: '{test_text}'\")\n",
        "    predictions = predict_next_token(model, tokenizer, test_text, top_k=5)\n",
        "\n",
        "    print(\"Top 5 predictions:\")\n",
        "    for pred in predictions:\n",
        "        print(f\"  {pred['rank']}. '{pred['token']}' (probability: {pred['probability']:.3f})\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "l32kidHcUm4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404a08ce-f8bb-453a-88e8-60e8d2a854f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Next Token Prediction Results ===\n",
            "\n",
            "Input text: 'ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©'\n",
            "Top 5 predictions:\n",
            "  1. '<UNK>' (probability: 0.270)\n",
            "  2. 'ÿßŸÑŸÑÿ∫ÿ©' (probability: 0.173)\n",
            "  3. 'ŸÑŸÑÿ≥ŸÜÿ©' (probability: 0.158)\n",
            "  4. 'ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™' (probability: 0.076)\n",
            "  5. 'Ÿà' (probability: 0.061)\n",
            "--------------------------------------------------\n",
            "Input text: 'ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä'\n",
            "Top 5 predictions:\n",
            "  1. '<UNK>' (probability: 0.560)\n",
            "  2. 'ŸÅŸä' (probability: 0.031)\n",
            "  3. 'ŸÖŸÜ' (probability: 0.023)\n",
            "  4. 'ÿπŸÑŸâ' (probability: 0.016)\n",
            "  5. 'Ÿà' (probability: 0.009)\n",
            "--------------------------------------------------\n",
            "Input text: 'ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä'\n",
            "Top 5 predictions:\n",
            "  1. '<UNK>' (probability: 0.560)\n",
            "  2. 'ŸÅŸä' (probability: 0.031)\n",
            "  3. 'ŸÖŸÜ' (probability: 0.023)\n",
            "  4. 'ÿπŸÑŸâ' (probability: 0.016)\n",
            "  5. 'Ÿà' (probability: 0.009)\n",
            "--------------------------------------------------\n",
            "Input text: 'ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß ÿßŸÑÿ≠ÿØŸäÿ´ÿ©'\n",
            "Top 5 predictions:\n",
            "  1. '<UNK>' (probability: 0.499)\n",
            "  2. 'ŸÖŸÜÿ™ÿØŸâ' (probability: 0.069)\n",
            "  3. 'ŸÇÿ≥ŸÖ' (probability: 0.049)\n",
            "  4. 'Ÿà' (probability: 0.022)\n",
            "  5. 'ŸÅŸä' (probability: 0.020)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Text Generation\n",
        "\n",
        "Generate new Arabic text starting from a seed phrase."
      ],
      "metadata": {
        "id": "t9CMImc0V_mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text: str, max_length: int = 20, temperature: float = 0.8):\n",
        "    \"\"\"Generate text starting from seed\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode seed\n",
        "    encoded = tokenizer.encoder(seed_text)\n",
        "    if not encoded:\n",
        "        encoded = [tokenizer.word_to_idx[tokenizer.START_TOKEN]]\n",
        "\n",
        "    generated = encoded.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Use last sequence_length tokens\n",
        "            input_seq = torch.tensor([generated[-sequence_length:]], dtype=torch.long).to(device)\n",
        "\n",
        "            # Get prediction\n",
        "            outputs, hidden = model(input_seq, hidden)\n",
        "            last_output = outputs[0, -1, :] / temperature\n",
        "\n",
        "            # Sample next token\n",
        "            probabilities = torch.softmax(last_output, dim=0)\n",
        "            next_token = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            # Stop conditions\n",
        "            if next_token == tokenizer.word_to_idx.get(tokenizer.END_TOKEN, -1):\n",
        "                break\n",
        "            if next_token == tokenizer.word_to_idx[tokenizer.PAD_TOKEN]:\n",
        "                continue\n",
        "\n",
        "            generated.append(next_token)\n",
        "\n",
        "    return tokenizer.decoder(generated)\n",
        "\n",
        "# Test text generation\n",
        "seed_texts = [\n",
        "    \"ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\",\n",
        "    \"ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä\",\n",
        "    \"ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß\",\n",
        "    \"ÿßŸÑŸÖÿ≥ÿ™ŸÇÿ®ŸÑ\"\n",
        "]\n",
        "\n",
        "print(\"=== Text Generation Results ===\")\n",
        "print()\n",
        "\n",
        "for seed in seed_texts:\n",
        "    generated = generate_text(model, tokenizer, seed, max_length=15, temperature=0.8)\n",
        "    print(f\"Seed: '{seed}'\")\n",
        "    print(f\"Generated: '{generated}'\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "D3Anw0E4UyFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7ece47-52cc-4ba3-87e9-48ef5e533f91"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Text Generation Results ===\n",
            "\n",
            "Seed: 'ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©'\n",
            "Generated: 'ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÖŸÜÿ™ÿØŸâ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿßŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ÿßŸÑŸÑÿ∫ÿ© <UNK> ŸÖŸÜÿ™ÿØŸâ <UNK> ŸÖŸÜÿ™ÿØŸâ <UNK> ŸÖŸÜÿ™ÿØŸâ <UNK> <UNK> ŸÖŸÜÿ™ÿØŸâ <UNK> <UNK>'\n",
            "------------------------------------------------------------\n",
            "Seed: 'ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä'\n",
            "Generated: '<UNK> <UNK> ŸÅŸä <UNK> <UNK> <UNK> <UNK> <UNK> ŸÖŸÜ <UNK> <UNK> <UNK> <UNK> <UNK> ÿ¨ÿØÿß ÿ™ŸÇÿØŸÖ <UNK>'\n",
            "------------------------------------------------------------\n",
            "Seed: 'ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß'\n",
            "Generated: 'ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß ÿ®ÿ±ÿßŸÖÿ¨ <UNK> <UNK> ÿ®ÿ±ÿßŸÖÿ¨ <UNK> <UNK> <UNK> ÿ®ÿ±ÿßŸÖÿ¨ <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'\n",
            "------------------------------------------------------------\n",
            "Seed: 'ÿßŸÑŸÖÿ≥ÿ™ŸÇÿ®ŸÑ'\n",
            "Generated: 'ÿßŸÑŸÖÿ≥ÿ™ŸÇÿ®ŸÑ <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Model Evaluation and Analysis\n",
        "\n",
        "Analyze the model's performance and vocabulary usage."
      ],
      "metadata": {
        "id": "80qlfQJ_WErR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_vocabulary(tokenizer):\n",
        "    \"\"\"Analyze vocabulary statistics\"\"\"\n",
        "    vocab_size = len(tokenizer.word_to_idx)\n",
        "\n",
        "    print(f\"Vocabulary Analysis:\")\n",
        "    print(f\"  Total vocabulary size: {vocab_size}\")\n",
        "    print(f\"  Special tokens: {[tokenizer.PAD_TOKEN, tokenizer.UNK_TOKEN, tokenizer.START_TOKEN, tokenizer.END_TOKEN]}\")\n",
        "\n",
        "    # Show some random vocabulary words\n",
        "    import random\n",
        "    vocab_words = list(tokenizer.word_to_idx.keys())\n",
        "    random_words = random.sample([w for w in vocab_words if not w.startswith('<')], min(20, len(vocab_words)))\n",
        "    print(f\"  Sample vocabulary words: {random_words[:10]}\")\n",
        "\n",
        "analyze_vocabulary(tokenizer)\n",
        "\n",
        "# Model performance summary\n",
        "print(f\"\\nModel Performance Summary:\")\n",
        "print(f\"  Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"  Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Model size: ~{sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.1f} MB\")"
      ],
      "metadata": {
        "id": "8HuUgZOiVKDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3dbee4-b38a-43d6-c456-267c9006812d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Analysis:\n",
            "  Total vocabulary size: 2000\n",
            "  Special tokens: ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
            "  Sample vocabulary words: ['ŸÅŸáŸä', 'ŸÜÿ™Ÿäÿ¨ÿ©', 'ŸÖÿÆÿßŸÑŸÅ', 'ŸàŸáŸà', 'ÿ≠ŸàÿßŸÑŸä', 'ÿ¥ÿπÿ±', 'ŸÉŸÅÿßÿ±', 'ŸÖÿ¨ÿßŸÜŸäÿ©', 'ŸàŸÑÿßŸäÿ©', 'ŸÑÿ≠ŸÇŸàŸÇ']\n",
            "\n",
            "Model Performance Summary:\n",
            "  Final training loss: 3.1238\n",
            "  Final validation loss: 3.3057\n",
            "  Total parameters: 11,519,952\n",
            "  Model size: ~43.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Save and Load Model\n",
        "\n",
        "Save the trained model for future use."
      ],
      "metadata": {
        "id": "dUiNX5TBWIW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, tokenizer, filepath):\n",
        "    \"\"\"Save model and tokenizer\"\"\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': model_config,\n",
        "        'vocab_size': len(tokenizer.word_to_idx)\n",
        "    }, filepath)\n",
        "\n",
        "    # Save tokenizer\n",
        "    tokenizer_path = filepath.replace('.pth', '_tokenizer.pkl')\n",
        "    with open(tokenizer_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'word_to_idx': tokenizer.word_to_idx,\n",
        "            'idx_to_word': tokenizer.idx_to_word,\n",
        "            'vocab_size': tokenizer.vocab_size,\n",
        "            'vocab_built': tokenizer.vocab_built\n",
        "        }, f)\n",
        "\n",
        "    print(f\"Model saved to: {filepath}\")\n",
        "    print(f\"Tokenizer saved to: {tokenizer_path}\")\n",
        "\n",
        "def load_model(filepath, device):\n",
        "    \"\"\"Load model and tokenizer\"\"\"\n",
        "    # Load model\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "\n",
        "    # Recreate model\n",
        "    loaded_model = ArabicLSTM(**checkpoint['model_config']).to(device)\n",
        "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer_path = filepath.replace('.pth', '_tokenizer.pkl')\n",
        "    with open(tokenizer_path, 'rb') as f:\n",
        "        tokenizer_data = pickle.load(f)\n",
        "\n",
        "    loaded_tokenizer = ArabicTokenizer()\n",
        "    loaded_tokenizer.word_to_idx = tokenizer_data['word_to_idx']\n",
        "    loaded_tokenizer.idx_to_word = tokenizer_data['idx_to_word']\n",
        "    loaded_tokenizer.vocab_size = tokenizer_data['vocab_size']\n",
        "    loaded_tokenizer.vocab_built = tokenizer_data['vocab_built']\n",
        "\n",
        "    return loaded_model, loaded_tokenizer"
      ],
      "metadata": {
        "id": "bUvFcWEjU-ba"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, tokenizer, 'arabic_lstm_model.pth')"
      ],
      "metadata": {
        "id": "jb96jXhILnS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4df2721-ebdf-4110-a678-9c62696c0f77"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: arabic_lstm_model.pth\n",
            "Tokenizer saved to: arabic_lstm_model_tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Push the Model to Hugging face\n",
        "\n",
        "Push the model to hugging face space."
      ],
      "metadata": {
        "id": "KpeySYINohnv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d3aa63",
        "outputId": "5c35768f-2d67-4041-c06a-dd7ff2580e4c"
      },
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2023.9.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import HfApi, notebook_login\n",
        "import os\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "4311f718b56c42db90db595642f5d3d6",
            "909ad79b29844469a1fcb7339d084228",
            "3e270f3b52b0413fa7564ed44d664d53",
            "8e942e320b9b461a87f706fdef7e2b7c",
            "83a6270fa5234e149e6a9aec925e39bf",
            "15b06350267c46c5b1de4d91e14ffe75",
            "97fa739ec0eb4bd6b2e750694e62f7a7",
            "adfda549e79942758cc893d48c34b5e5",
            "744b31d7cba049db93df5737a8e70a2d",
            "839cbc4086f5464a842d3dbb2586bce4",
            "bb11143c69ef46088c321cfc510d8a83",
            "b5515f6dfe9b417cbdfc3946f66efb4e",
            "3a094b2bdc5d4421b9e3ad61b3e4b6fd",
            "568e93b27af145db8f02911c5ec6edb0",
            "1a10352c40904612a34adfde317831f6",
            "f0ef66f2b0f9429ab85afa0f4f04448e",
            "2d7a7a80320347c993fa7933efaad557",
            "8926e39034af4aaa82612c7cfc817438",
            "420fd728c23e4b8ab48b0c17522161df",
            "bb210ceaba8749088ab3ef9a18cdca0c"
          ]
        },
        "id": "6UbwYfWEiX5G",
        "outputId": "7d9458dc-fc3e-493c-9f8f-e35d7f8de249"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4311f718b56c42db90db595642f5d3d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"Yousif22/arabic-lstm-demo\"\n",
        "print(f\"Target repository: {repo_id}\")\n",
        "\n",
        "# Define the files to push\n",
        "local_files = ['arabic_lstm_model.pth', 'arabic_lstm_model_tokenizer.pkl']\n",
        "print(f\"Files to push: {local_files}\")\n",
        "\n",
        "# Define the target directory in the repository\n",
        "target_repo_dir = \"models\"\n",
        "print(f\"Target directory in repository: ./{target_repo_dir}\")\n",
        "\n",
        "# Push the files to the repository\n",
        "# The create_pr=True option creates a pull request instead of pushing directly\n",
        "try:\n",
        "    api = HfApi()\n",
        "    for file_path in local_files:\n",
        "        # Construct the path within the repository\n",
        "        path_in_repo = os.path.join(target_repo_dir, os.path.basename(file_path))\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=path_in_repo,\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"space\",\n",
        "            commit_message=f\"Add {os.path.basename(file_path)} to {target_repo_dir}\",\n",
        "            create_pr=False,\n",
        "        )\n",
        "        print(f\"Successfully pushed {os.path.basename(file_path)} to Hugging Face Space '{repo_id}/{path_in_repo}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing to Hugging Face: {e}\")\n",
        "    print(\"Please ensure the repository exists and you have write access.\")\n",
        "    print(\"Also, verify your authentication token.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOtXIvkCvxDS",
        "outputId": "3c03f3d6-5b5b-401f-c476-7c0803c005c8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target repository: Yousif22/arabic-lstm-demo\n",
            "Files to push: ['arabic_lstm_model.pth', 'arabic_lstm_model_tokenizer.pkl']\n",
            "Target directory in repository: ./models\n",
            "Successfully pushed arabic_lstm_model.pth to Hugging Face Space 'Yousif22/arabic-lstm-demo/models/arabic_lstm_model.pth'\n",
            "Successfully pushed arabic_lstm_model_tokenizer.pkl to Hugging Face Space 'Yousif22/arabic-lstm-demo/models/arabic_lstm_model_tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16: Interactive Testing\n",
        "\n",
        "Create functions for easy testing of the trained model."
      ],
      "metadata": {
        "id": "I216EW5VWLru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_prediction(model, tokenizer):\n",
        "    \"\"\"Interactive function for testing predictions\"\"\"\n",
        "    print(\"=== Interactive Next Token Prediction ===\")\n",
        "    print(\"Enter Arabic text to get next token predictions (type 'quit' to exit):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter Arabic text: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            predictions = predict_next_token(model, tokenizer, user_input, top_k=3)\n",
        "            print(f\"\\nNext token predictions for '{user_input}':\")\n",
        "            for pred in predictions:\n",
        "                print(f\"  {pred['rank']}. '{pred['token']}' ({pred['probability']:.3f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "def interactive_generation(model, tokenizer):\n",
        "    \"\"\"Interactive function for text generation\"\"\"\n",
        "    print(\"=== Interactive Text Generation ===\")\n",
        "    print(\"Enter seed text to generate Arabic text (type 'quit' to exit):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter seed text: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            generated = generate_text(model, tokenizer, user_input, max_length=10)\n",
        "            print(f\"\\nGenerated text: '{generated}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Example usage\n",
        "interactive_prediction(model, tokenizer)\n",
        "interactive_generation(model, tokenizer)\n",
        "\n",
        "print(\"Notebook completed successfully!\")\n",
        "print(\"\\nTo test the model interactively, uncomment and run:\")\n",
        "print(\"interactive_prediction(model, tokenizer)\")\n",
        "print(\"interactive_generation(model, tokenizer)\")"
      ],
      "metadata": {
        "id": "FXZzcVqWVP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated how to build an LSTM language model for Arabic text that can:\n",
        "\n",
        "1. **Process Arabic Text**: Clean, tokenize, and encode Arabic text properly\n",
        "2. **Build Vocabulary**: Create efficient word-to-index mappings\n",
        "3. **Train LSTM Model**: Use PyTorch to train a neural language model\n",
        "4. **Predict Next Tokens**: Given input text, predict most likely next words\n",
        "5. **Generate Text**: Create new Arabic text starting from seed phrases\n",
        "\n",
        "### Key Components:\n",
        "- **ArabicTokenizer**: Handles Arabic text preprocessing and vocabulary\n",
        "- **ArabicLSTMDataset**: Prepares training sequences for LSTM\n",
        "- **ArabicLSTM**: Neural network architecture for language modeling\n",
        "- **Training Loop**: Optimizes model parameters using gradient descent\n",
        "- **Evaluation Functions**: Test model performance on various tasks\n",
        "\n",
        "### Next Steps:\n",
        "- Train on larger Arabic datasets for better performance\n",
        "- Experiment with different model architectures (GRU, Transformer)\n",
        "- Fine-tune hyperparameters for specific use cases"
      ],
      "metadata": {
        "id": "LE5s42jBWQHI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbv0upZCpKZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}